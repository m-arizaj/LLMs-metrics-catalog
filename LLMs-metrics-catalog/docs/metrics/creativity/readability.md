---
id: readability
title: Readability
sidebar_label: Readability
---

## Definition

**Readability** is a criterion used to evaluate the **Linguistic Quality** of text generated by Large Language Models (LLMs).

In the context of the source paper, Readability is not treated as a standalone automatic metric but as a key component of **Fluency** , which is a fundamental criterion for the **Human Evaluation** of LLMs. Fluency assesses the model's ability to produce content that flows smoothly. A text with high readability is grammatically correct, maintains a consistent tone and style, and ensures a seamless user experience.

***

## Formula (General Idea)

In the context of the source paper (Chang et al., 2024), Readability is treated as a component of **Fluency**, a criterion used in **Human Evaluation** rather than a specific automatic formula.

It is a qualitative assessment made by human evaluators who judge the quality of the generated text. This contrasts with automatic metrics like ROUGE or F1-Score which are calculated based on token overlap.

***

## Purpose

The primary purpose of evaluating Readability (as part of Fluency) is to assess the quality of the generated text from a human perspective. This evaluation aims to:
* Ensure the text "flows smoothly".
* Verify that the text is "grammatically correct".
* Confirm the model maintains a "consistent tone and style".
* Measure if the text provides a "seamless user experience".
* Determine if the model "avoids awkward expressions and abrupt shifts in language or topic".

***

## Domains

* General LLM Evaluation
* Human Evaluation of LLMs 
* Open-ended generation tasks 
* Open-domain conversations 

***

## Advantages

* **More Accurate Feedback:** As a human evaluation metric, it is "closer to the actual application scenario" and provides "more comprehensive and accurate feedback" than automatic metrics.
* **More Reliable for Generation:** It is considered "more reliable" for evaluating open-generation tasks where standard reference-based metrics (like ROUGE or BLEU) are not suitable.
* **Captures Nuance:** It assesses subtle linguistic qualities like smooth flow, tone, and style, which automatic formulas often fail to capture.

***

## Limitations

* **High Variance:** As a human-judged metric, it can suffer from "high variance and instability".
* **Subjectivity:** Evaluation can be influenced by "cultural and individual differences" among the human evaluators.
* **Requires Rigor:** To be reliable, the evaluation process requires "thoughtful attention" to crucial factors, such as the number of evaluators, their expertise level, and clear evaluation rubrics.

***

## Key References

* **Primary Source (Application):**
    * Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2024). *A Survey on Evaluation of Large Language Models*. https://doi.org/10.48550/arXiv.2307.03109
* **Cited for Fluency/Readability Definition:**
    * Van Der Lee, C., Gatt, A., Van Miltenburg, E., Wubben, S., & Krahmer, E. (2019). *Best practices for the human evaluation of automatically generated text*. Proceedings of the 12th International Conference on Natural Language Generation. https://doi.org/10.18653/v1/W19-8643