---
id: similarity
title: Similarity Metrics
sidebar_label: Similarity
---

## Introduction

In code generation and software engineering, "Similarity" is not a single metric but a broad category of evaluation techniques. Its purpose is to quantify how "close" a generated output is to a reference, especially when a simple **Exact Match** is too strict and fails to capture partially correct or semantically equivalent solutions.

Different tasks require different notions of similarity. For example:
* In **code completion**, a suggestion should be *syntactically* close to the desired code, minimizing the typing or editing effort required by the developer.
* In **docstring generation** or robustness checks, the output should be *semantically* close, even if the exact wording differs.
* In **data visualization**, the generated *image* must be visually similar to the target, a property the code itself cannot guarantee.

This framework covers several key variants of similarity metrics used in evaluation.

## 1. Levenshtein Edit Similarity
### Definition
Also known as **Edit Distance**, this metric measures the "edit effort" between two strings. It is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change the candidate string into the reference string. A lower distance means higher similarity.

$$
\text{Similarity} = 1 - \frac{\text{LevenshteinDistance}(\text{str}_1, \text{str}_2)}{\max(\text{len}(\text{str}_1), \text{len}(\text{str}_2))}
$$

*(Note: The exact formula for similarity score based on edit distance can vary, but the core is the distance calculation.)*

### Purpose
In code evaluation, it is used as a "critical evaluation metric"  because it directly measures how much effort a developer would need to manually correct a model's generated code to match the ground truth.

### Applications
* **Code Completion:** Evaluating line-level  or token-level code suggestions.
* **Structural Distance:** Assessing syntactic closeness of code snippets.

### Benchmarks
* PY150 
* GitHub Java Corpus 

## 2. Sentence Cosine Similarity
### Definition
This is a **semantic similarity** metric that measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. In NLP and code analysis, these vectors are typically "sentence embeddings" of docstrings or code, generated by models like Sentence Transformers (SBERT).
A score of $1.0$ means the vectors point in the same direction (semantically identical), while $0.0$ means they are orthogonal (unrelated).

### Purpose
To verify that a generated piece of text (like a docstring) or a code element (like a function name) preserves its original *meaning*, even if the surface-level text (words, syntax) is different.
### Applications
* **Semantic Similarity:** Used in robustness benchmarks (like ReCode) to confirm that a "perturbed" docstring or function name is still semantically equivalent to the original.
* **Code Generation Evaluation:** Assessing the quality of generated docstrings or comments.

### Benchmarks
* HumanEval (in ReCode) 
* MBPP (in ReCode) 

## 3. Visualization Similarity Scores
### Definition
This metric evaluates the **output quality** of data science code by comparing the *rendered image* of a generated plot against an expected reference plot. The specific calculation method is often based on image processing techniques (e.g., Structural Similarity Index - SSIM, or Mean Squared Error - MSE), which produce a score quantifying the visual likeness.

### Purpose
To evaluate the correctness of visualization tasks, where the code itself is a means to an end. Different code can produce identical plots, and text-based metrics (like BLEU or Edit Similarity) on the code are poor indicators of the final visual correctness.

### Applications
* **Data Science Code Generation:** Specifically for tasks requiring `matplotlib`, `seaborn`, or `plotly` outputs.

## 4. Comparative Summary

| Metric | Based on | Measures | Typical Domain |
| --- | --- | --- | --- |
| **Levenshtein Edit Similarity** | Character Edits | Syntactic closeness / Edit effort | Code Completion  |
| **Sentence Cosine Similarity** | Vector Embeddings | Semantic meaning | Code/Docstring Robustness  |
| **Visualization Similarity** | Image Comparison | Visual output quality | Data Science Visualization  |

## References
* Lu, S., Guo, D., Ren, S., Huang, J., et al. (2021). *CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation*. 
https://doi.org/10.48550/arXiv.2102.04664
* Li, Z., Qian, H., Yang, C., Wang, Z., et al. (2022). *ReCode: Robustness Evaluation of Code Generation Models*. https://doi.org/10.48550/arXiv.2212.10264
* Nascimento, N., Guimaraes, E., Chintakunta, S., & Boominathan, A. (2024). *LLM4DS: Evaluating Large Language Models for Data Science Code Generation*. https://doi.org/10.48550/arXiv.2411.11908
