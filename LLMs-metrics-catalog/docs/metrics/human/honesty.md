---
id: honesty
title: Honesty
sidebar_label: Honesty
---

## Definition
**Honesty** is a core principle used in the **Human Evaluation** of Large Language Models (LLMs). It is part of the "3H rule" — **Helpfulness, Honesty, and Harmlessness** — which serves as a foundational concept for developing detailed human assessment criteria.

Human evaluation itself is defined as a method to assess the quality and accuracy of a model's generated results through human participation. This approach is considered more comprehensive and accurate than automated evaluation because it is closer to real-world application scenarios.

***

## Formula (General Idea)
Honesty is not a quantitative metric with a mathematical formula. It is a qualitative principle assessed by human evaluators (such as experts, researchers, or ordinary users).

In the survey (Paper 9), the principle of "Honesty" is elaborated into more specific, measurable criteria that evaluators use, such as **Accuracy**. Accuracy is defined as scrutinizing "the extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies".

***

## Purpose
The purpose of evaluating for Honesty is to assess an LLM's adherence to truthfulness and factual correctness. It is a fundamental component of the "3H rule" for human alignment and trustworthiness. This principle helps ensure that models are reliable and avoid generating factually inaccurate information or "hallucinations".

***

## Domains
* Human Evaluation
* Human-centered Evaluation
* LLM Evaluation (Trustworthiness, Ethics)

***

## Benchmarks
Honesty, as a human evaluation criterion, is applied within broader evaluation frameworks and benchmarks. The benchmarks explicitly mentioned in connection with this evaluation domain include:

* **HELM (Holistic Evaluation of Language Models)** 
* **Chatbot Arena**

***

## Advantages
* **More Reliable:** Human evaluation based on principles like Honesty is considered "more reliable" than automated metrics, especially for open-ended generation tasks.
* **Real-World Scenarios:** This evaluation method is "closer to the actual application scenario" and "can provide more comprehensive and accurate feedback".
* **Fundamental Assessment:** It assesses a core pillar of model trustworthiness that automated metrics (like F1 or ROUGE) may not capture.

***

## Limitations
* **Subjectivity and Variance:** Human evaluation can have "high variance and instability," which may be "due to cultural and individual differences" among the human evaluators.
* **Requires Human Labor:** By definition, it is not an automated process and requires inviting human evaluators (experts, researchers, or users) to assess the model's outputs.
* **Requires Clear Rubrics:** The effectiveness of the evaluation depends heavily on the "evaluation criteria" and the "evaluator's expertise level".

***

## Key References
* Chang, Y., Wang, X., Wang, J., et al. (2023). *A Survey on Evaluation of Large Language Models*. [https://doi.org/10.48550/arXiv.2307.03109](https://doi.org/10.48550/arXiv.2307.03109)
* Askell, A., Bai, Y., Chen, A., et al. (2021). *A general language assistant as a laboratory for alignment*. arXiv preprint arXiv:2112.00861. (Cited in Paper 9 as the source of the 3H rule) 
* (Excel Data: Paper 9)
