---
id: toxicity
title: Toxicity
sidebar_label: Toxicity
---

## Overview
Toxicity measures the extent to which text generated by a model contains harmful, offensive, or ethically inappropriate content.  
In the context of Large Language Models (LLMs) and Software Engineering (SE), toxicity metrics are vital for assessing the safety and ethical reliability of generated code comments, commit messages, documentation, or natural language outputs in development workflows.

Traditional approaches (such as RealToxicityPrompts or ToxiGen) rely on reference datasets with labeled toxic and non-toxic examples, while recent frameworks like LATTE (Koh et al., 2024) integrate LLM-based evaluators that contextualize toxicity across categories like demeaning, biased, and harmful content.  
This evolution reflects the increasing need for context-aware and value-aligned evaluation in AI-assisted software engineering tools.

## Formula and Structure

Toxicity metrics are commonly derived as probability-based or classifier-based scores:

1. *Toxicity Probability*
   $$
   T(x) = P_{model}(toxic|x)
   $$
   where $x$ is the generated text and the model estimates the likelihood of it being toxic.

2. *Toxicity Rate*
   $$
   Toxicity\ Rate = \frac{\text{Number of toxic outputs}}{\text{Total outputs}}
   $$

3. *Expected Toxicity*
   $$
   E[T] = \frac{1}{N} \sum_{i=1}^{N} T(x_i)
   $$

These formulations quantify how often and how severely toxic behaviors appear in model outputs.  
Modern approaches like LATTE also incorporate multi-dimensional scoring, weighting attributes such as offensiveness, stereotyping, and ethical preference.

## Variants
- *Toxicity:* General classification of text as toxic or non-toxic, used in safety evaluations.  
- *Toxicity Rate:* Ratio of toxic responses generated across all outputs.  
- *Contextual Toxicity (LLM-based):* Incorporates semantic context and moral reasoning through model-based evaluators (e.g., LATTE).  
Each variant can be calibrated to reflect cultural norms, bias sensitivity, or ethical compliance standards.

## Applications in Software Engineering
Toxicity metrics are increasingly used in AI-assisted code generation, documentation synthesis, and developer communication tools to detect unintended harmful language.  
Benchmark datasets such as:
- *HELM (2024)* apply toxicity scoring for safety evaluations.  
- *RealToxicityPrompts (2023)* and *ToxiGen (2023)* evaluate ethical safety and bias robustness.    
Together, these applications demonstrate the importance of toxicity evaluation in promoting inclusive, ethical, and trustworthy model behavior within SE contexts.

## Interpretation
A high toxicity score indicates a strong presence of offensive or discriminatory content, while a low score suggests ethical alignment and safety.  
However, toxicity detection remains subjective and context-dependent:
- Classifiers may reflect cultural or dataset biases.  
- LLM-based judges (e.g., LATTE) mitigate these issues by incorporating nuanced ethical criteria.  
In SE environments, monitoring toxicity ensures professional communication quality and helps prevent bias propagation in training data or generated documentation.

## References
1. Koh, Y., Wang, R., Xie, Y., & Zhou, W. (2024). Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric. arXiv preprint. [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)  
2. Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models.  
3. Hartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., & Ettinger, A. (2022). ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. 

### Additional References in Dataset
- 4, 9,Â 16