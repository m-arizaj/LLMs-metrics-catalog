---
id: toxicity
title: Toxicity
sidebar_label: Toxicity
---
import { ReferencesIndex } from '@site/src/components/References';

## Introduction
Toxicity measures the extent to which text generated by a model contains harmful, offensive, abusive, or otherwise inappropriate content. In Large Language Models (LLMs) and Software Engineering (SE), toxicity metrics help evaluate the safety and ethical reliability of generated code comments, commit messages, documentation, and natural-language responses. They are essential for ensuring professional communication quality and preventing the propagation of harmful language in development workflows.
Traditional toxicity evaluations, such as *RealToxicityPrompts* (Gehman et al., 2020) and *ToxiGen* (Hartvigsen et al., 2022), use curated datasets of toxic and non-toxic text to score model outputs using classifier-based toxicity probabilities.  
More recent frameworks like *LATTE* (Koh et al., 2024) introduce LLM-based toxicity evaluators that contextualize and score toxicity along multiple dimensions, reflecting a shift toward more nuanced, value-aligned evaluation.


## Formula and Structure

Toxicity metrics are commonly defined using classifier probabilities or aggregated toxicity rates.

### 1. Toxicity Probability
A classifier or evaluator assigns a toxicity score:
$$
T(x) = P(\text{toxic} \mid x)
$$
where $x$ is the generated text.  
This matches classifier outputs such as the Perspective API score used in RealToxicityPrompts.

### 2. Toxicity Rate
The proportion of generated outputs classified as toxic:
$$
Toxicity\ Rate = \frac{N_{\text{toxic}}}{N_{\text{total}}}
$$

### 3. Expected Toxicity
The average toxicity across $N$ generations:
$$
E[T] = \frac{1}{N} \sum_{i=1}^{N} T(x_i)
$$

These formulations quantify how often toxicity occurs (rate) and how severe it is (probability). LLM-based evaluators like LATTE extend this with multi-dimensional scores that account for contextual meaning, intent, and ethical alignment.

## Variants

### Toxicity (Binary)
Classifier-based categorization of text as toxic or non-toxic, common in early safety benchmarks.

### Toxicity Rate
A frequency-based measure of how often a model generates harmful content.

### Contextual Toxicity (LLM-Based)
Introduced in LATTE, where LLM evaluators score toxicity across *evaluation dimensions* (e.g., harmful intent, demeaning phrasing, stereotyping). These scores incorporate semantic context rather than relying solely on surface-level lexical cues.

Variants can be calibrated to different cultural norms, bias sensitivities, or ethical guidelines.


## Applications in Software Engineering
In SE contexts, toxicity metrics are used to evaluate:

- generated *code comments*,  
- automated *documentation*,  
- *commit messages*,  
- SE-related natural-language explanations.

Frameworks such as *HELM* apply toxicity scoring for safety analysis, while benchmarks like *RealToxicityPrompts* and *ToxiGen* quantify toxicity robustness under varied prompts. These evaluations help ensure that AI-assisted development tools produce safe, respectful, and workplace-appropriate language.


## Interpretation
A high toxicity score indicates stronger presence of harmful, abusive, or discriminatory content, while a low score signals safer and more ethically aligned outputs.

However, toxicity detection is context-dependent:
- Classifier-based systems may encode dataset or cultural biases.  
- Scoring can over-penalize minority dialects or reclaimed terms (documented in RealToxicityPrompts and ToxiGen).  
- LLM-based evaluators (LATTE) address these limitations by applying contextual reasoning and multi-dimension judgment.

In SE environments, monitoring toxicity supports ethical AI adoption and helps prevent harmful language from propagating into technical ecosystems.


## References
1. Koh, Y., Wang, R., Xie, Y., & Zhou, W. (2024). Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric. [https://doi.org/10.48550/arXiv.2402.06900](https://doi.org/10.48550/arXiv.2402.06900)  
2. Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. [https://doi.org/10.48550/arXiv.2009.11462](https://doi.org/10.48550/arXiv.2009.11462)
3. Hartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., & Ettinger, A. (2022). ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. [https://doi.org/10.48550/arXiv.2203.09509](https://doi.org/10.48550/arXiv.2203.09509)

### Additional References in Dataset
- <ReferencesIndex ids={['4','9','16']}Â />