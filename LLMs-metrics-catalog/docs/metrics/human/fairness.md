---
id: fairness
title: Fairness
sidebar_label: Fairness
---

## Definition
**Fairness** is a critical category of ethical evaluation for Large Language Models (LLMs). It is concerned with assessing whether a model's outputs and behaviors result in **disparate treatment or outcomes between different social groups**.

This evaluation is essential as LLMs, which are typically trained on massive, uncurated Internet-scale data, can learn, perpetuate, and even amplify harmful social biases. These biases include stereotypes, misrepresentations, and derogatory language that disproportionately affect marginalized communities. The goal of fairness evaluation is to measure these harms to understand and prevent the propagation of bias.

In the context of LLMs, fairness is often operationalized into two main concepts:
* **Group Fairness:** Requires that a statistical outcome measure (like accuracy or toxicity score) has approximate parity across different social groups.
* **Individual Fairness:** Requires that individuals who are similar with respect to a specific task are treated similarly by the model.

***

## How Fairness is Measured

Fairness is not a single metric but a broad category. The metrics used to measure it are typically organized by the part of the model they operate on:

1.  **Embedding-Based Metrics:**

    * **What they do:** Measure bias by computing distances in the model's vector space (embeddings). They often check the association between neutral words (e.g., "doctor") and identity-related words (e.g., "man," "woman").
    * **Examples:** WEAT (Word Embedding Association Test), SEAT (Sentence Encoder Association Test).

2.  **Probability-Based Metrics:**

    * **What they do:** Use the model-assigned token probabilities to estimate bias. This is often done by comparing the likelihood of "counterfactual" sentences that are identical except for a perturbed social group attribute  (e.g., "The *man* was a nurse" vs. "The *woman* was a nurse").
    * **Examples:** CrowS-Pairs Score, DisCo, Log-Probability Bias Score.

3.  **Generated Text-Based Metrics:**

    * **What they do:** Analyze the final free-text output generated by the model in response to a prompt.
    * **This includes:**
        * **Distribution:** Comparing co-occurrence counts of group terms with neutral or stereotypical words. Metrics like **Demographic Representation** and **Stereotypical Associations** fall here.
        * **Classifiers:** Using an auxiliary classifier (e.g., for toxicity or sentiment) to score outputs generated from prompts mentioning different social groups.
        * **Lexicons:** Comparing output words against pre-compiled lists of harmful, derogatory, or biased terms.

***

## Purpose

The primary purpose of fairness evaluation is to **identify, measure, and ultimately mitigate harmful social biases** that LLMs may learn and amplify. This allows researchers and practitioners to understand and prevent the propagation of bias, such as stereotypes, misrepresentations, and derogatory language, which disproportionately harm marginalized communities.

***

## Domains

* General LLM Evaluation 
* Fairness / Bias Evaluation
* Ethical Evaluation
* Text Generation
* Question-Answering
* Classification (e.g., Toxicity Detection)
* Machine Translation
* Information Retrieval

***

## Benchmarks

* **HELM** 
* **Chatbot Arena**
* **HolisticBias**
* **BBQ (Bias Benchmark for QA)**
* **CrowS-Pairs**
* **StereoSet**
* **BOLD (Bias in Open-Ended Language Generation)**
* **RealToxicityPrompts**
* **Winogender**
* **WinoBias**
* **GAP**

***

## Advantages

* **Identifies Harms:** Allows researchers to precisely define and measure specific social harms, such as stereotyping, misrepresentation, and derogatory language.
* **Enables Mitigation:** Provides the necessary feedback to develop and assess bias mitigation techniques.
* **Promotes Equity:** Empowers developers to understand and prevent the amplification of historical and structural power asymmetries.
* **Task-Specific:** Allows for targeted evaluation of how bias manifests differently across various NLP tasks.

***

## Limitations

* **Weak Downstream Correlation:** Embedding-based and probability-based metrics often show a **weak and unreliable relationship** with actual bias observed in downstream applications.
* **Dataset Validity:** Many widely-used datasets (like StereoSet and CrowS-Pairs) suffer from **severe reliability and validity issues**, containing ambiguities that raise questions about whether they measure real-world stereotypes at all.
* **Biased Classifiers:** Metrics that rely on an auxiliary classifier (e.g., for toxicity) can be unreliable, as the classifier itself may be biased (e.g., disproportionately flagging African-American English as toxic).
* **Oversimplification:** Many metrics and datasets assume binary social groups or treat "social groups as interchangeable," which "ignores the underlying forces of injustice."
* **Subjectivity:** Fairness is "highly subjective, value-dependent, and non-static." There is **no universal fairness specification**, making a single, objective benchmark difficult to establish.

***

## Key References

* Gallegos, I. O., Rossi, R. A., Barrow, J., et al. (2024). *Bias and Fairness in Large Language Models: A Survey*. [https://doi.org/10.1162/coli_a_00524](https://doi.org/10.1162/coli_a_00524)
* Chang, Y., Wang, X., Wang, J., et al. (2023). *A Survey on Evaluation of Large Language Models*. [https://doi.org/10.48550/arXiv.2307.03109](https://doi.org/10.48550/arXiv.2307.03109)
* Blodgett, S. L., Barocas, S., Daum√© III, H., & Wallach, H. (2020). *Language (technology) is power: A critical survey of "bias" in NLP*.
* Parrish, A., Chen, A., Nangia, N., et al. (2022). *BBQ: A hand-built bias benchmark for question answering*.  
* Nangia, N., Vania, C., Bhalerao, R., & Bowman, S. R. (2020). *CrowS-Pairs: A challenge dataset for measuring social biases in masked language models*.
* Nadeem, M., Bethke, A., & Reddy, S. (2021). *StereoSet: Measuring stereotypical bias in pretrained language models*.
* (Excel Data: Papers 9, 16)