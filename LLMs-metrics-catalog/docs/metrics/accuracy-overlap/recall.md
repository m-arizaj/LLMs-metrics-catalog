---
id: recall
title: Recall
sidebar_label: Recall
---
import { ReferencesIndex } from '@site/src/components/References';

## Introduction

Recall (also known as Sensitivity or True Positive Rate) measures the proportion of relevant instances that are successfully retrieved by a model. It is particularly important in contexts where false negatives are costly, such as defect detection, security vulnerability identification, or code correctness prediction in software engineering.

The general formula is:

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

where:
- $TP$ = True Positives (correctly identified positive instances)  
- $FN$ = False Negatives (missed positive instances)

Recall emphasizes the completeness of a model’s predictions, how well it captures all relevant items, rather than just the precision of those retrieved.

## Theoretical Context

In software engineering, recall has become a fundamental measure for evaluating model performance in bug detection, defect prediction, classification robustness, and code understanding.  
It helps determine whether a model can identify as many true issues or relevant entities as possible, which is critical when omissions (false negatives) are more harmful than false alarms (false positives).

- **Tantithamthavorn et al. (2018)** emphasize that recall is highly sensitive to class imbalance, a common issue in defect prediction datasets. When the majority of cases are non-defective, optimizing recall ensures that minority (defective) cases are not ignored — an insight that translates directly to LLM evaluations where rare but critical cases (e.g., security vulnerabilities or logical failures) exist.  

- **Wattanakriengkrai et al. (2020)** extend this perspective by applying recall to line-level code defect prediction, demonstrating that a higher recall reflects a model’s capacity to detect all potentially defective lines. This parallels LLM-based code generation or repair tasks, where missing a single error can compromise functional correctness.  

- **Díaz, Ekstrand & Mitra (2023)** discuss recall from a broader evaluation theory perspective, connecting it to robustness and lexicographic evaluation in large-scale model assessment. They propose that recall is not only a detection measure but also a lens to evaluate models’ stability and reliability across edge cases, which aligns with LLM robustness testing in software engineering and NLP.

## Variants of Recall

Across software engineering benchmarks and LLM evaluations, Recall appears in multiple specialized forms to adapt to context-specific needs:

### 1. **Standard Recall**
Measures the proportion of correctly identified positive instances.  
Common in bug detection, classification, and security evaluation tasks.
- Examples:  
  - `Defects4J`, `QuixBugs`, `CYBERSECEVAL`, `AMBER`, `PII Benchmark`  
  - Domains: Bug Repair, Classification, Security, Hallucination Detection  

### 2. **Recall@k**
Evaluates recall when considering the *top-k* predictions generated by the model — useful in ranking or multi-output settings such as code generation** or retrieval tasks.

- Examples:  
  - `DevEval`, `EvoCodeBench`  
  - Measures model’s ability to include the correct solution among its k-best outputs.  
  - Often expressed as:
    $$
    \text{Recall@k} = \frac{|\text{Relevant items in top-k}|}{|\text{All relevant items}|}
    $$

### 3. **Dependency Recall (DEP)**
Used in structural code evaluation tasks such as `ClassEval`, focusing on how well a model preserves field** or method dependencies in generated code.

- **DEP(F)** — *Field Dependency Recall*: Evaluates if the generated code retains the correct field-level relationships between variables.  
- **DEP(M)** — *Method Dependency Recall*: Measures preservation of logical or functional dependencies between methods.  

These variants extend the idea of recall beyond classification to structural and contextual understanding, assessing LLMs’ ability to maintain internal code consistency.

## Applications in Software Engineering

- **Bug Detection / Repair**: Ensures that the model detects as many defective components or code lines as possible.  
- **Classification Tasks**: Measures the completeness of predictions in multi-class or discriminative models.  
- **Security Evaluation**: Assesses the detection rate of vulnerabilities.  
- **Dependency Understanding**: Evaluates structural correctness and code context maintenance.  
- **Hallucination and Robustness Testing**: Monitors how consistently models recall true facts or relevant entities.

## References

1. Tantithamthavorn, C., McIntosh, S., Hassan, A. E., & Matsumoto, K. (2018). *The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models.* [https://arxiv.org/abs/1801.10269](https://arxiv.org/abs/1801.10269)

2. Wattanakriengkrai, S., Tantithamthavorn, C., Matsumoto, K., & Hassan, A. E. (2020). *Predicting Defective Lines Using a Model-Agnostic Technique.* [https://arxiv.org/abs/2009.03612](https://arxiv.org/abs/2009.03612)

3. Díaz, F., Ekstrand, M. D., & Mitra, B. (2023). *Recall, Robustness, and Lexicographic Evaluation.* [https://doi.org/10.48550/arXiv.2302.11370](https://doi.org/10.48550/arXiv.2302.11370)

### Additional References in Dataset
- <ReferencesIndex ids={['2','3','6','7','8','18','22','26','36','45','48','50','63','67']} />
