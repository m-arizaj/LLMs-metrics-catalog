---
id: ice-score
title: ICE-Score
sidebar_label: ICE-Score
---
import { ReferencesIndex } from '@site/src/components/References';

## Introduction
ICE-Score (Instructed Code Evaluation Score) is a reference-free evaluation metric designed to assess the quality of code generated by Large Language Models (LLMs). Unlike traditional metrics such as BLEU or CodeBLEU, which rely on lexical or syntactic similarity, ICE-Score uses LLMs themselves as evaluators, guided by structured natural language instructions to emulate human judgment.

The metric focuses on two primary dimensions:
- *Functional Correctness* — whether the generated code behaves as intended.
- *Code Usefulness / Alignment* — how well the code aligns with task goals and developer expectations.

Developed by Zhuo (2023), ICE-Score belongs to the emerging family of LLM-as-a-Judge metrics, following the philosophy of *G-EVAL* but adapted to code understanding and reasoning tasks. It allows consistent and scalable evaluations across programming datasets and code generation benchmarks such as CoNaLa, HumanEval, and APPS.

## Formula and Structure
Unlike numeric formulas, ICE-Score is computed through prompt-based structured evaluation using LLMs. The model is prompted to provide ratings (typically on a 1–5 or 0–1 scale) for key criteria and then averages them into a composite score.

General structure:
> *Prompt:*  
> “Evaluate the following generated code based on correctness, clarity, and usefulness.  
> Rate from 1 (poor) to 5 (excellent). Justify your decision.”

Formally, if $s_i$ represents the model’s normalized score for criterion i, and there are n evaluation aspects, then:

$$
ICE = \frac{1}{n} \sum_{i=1}^{n} s_i
$$

### Key Evaluation Dimensions
1. *Functional Correctness:* Logical and execution consistency.  
2. *Usefulness:* Relevance and practical applicability of the code.   
These aspects may vary depending on task setup and dataset.

## Variants
ICE-Score has evolved into several enhanced forms to address evaluation depth and context dependence:
- *ICE-Score:* The base form, uses LLMs to judge correctness and usefulness directly.  
- *CoT-ICE-Score:* Adds Chain-of-Thought prompting, encouraging evaluators to reason step-by-step before scoring.  
- *Ref-ICE-Score:* Includes reference code snippets to anchor judgments, combining LLM reasoning with ground-truth examples.  
- *CoT-Ref-ICE-Score:* Merges both enhancements, using CoT reasoning and reference context for higher alignment with human evaluations.  
These variants improve reliability, reduce bias, and enhance alignment between automated and expert human scoring.

## Applications in Software Engineering
ICE-Score and its variants are now central to evaluating code generation, repair, and summarization tasks where functional correctness is more important than surface similarity. From the database, these are the main contexts of use:

- LLM-as-Judge Evaluation: ICE-Score assesses generated outputs directly through model reasoning.  
- Human Alignment Tasks: Measures how well model judgments align with human evaluators (CoNaLa, HumanEval).  
- Functional Correctness Evaluation: Judges whether the generated code meets expected test behavior without executing it (HumanEval).  
- Chain-of-Thought Evaluation: CoT-ICE-Score uses intermediate reasoning steps for deeper functional assessment.  
- Reference-enhanced Evaluation: Ref-ICE-Score and CoT-Ref-ICE-Score leverage gold-standard implementations as additional grounding.
Together, these methods form a scalable alternative to test-based metrics, enabling performance assessment even when ground truth or runnable environments are missing.

## Interpretation
ICE-Score represents a paradigm shift in how we evaluate AI-generated code:
- It blends human interpretability with automation, capturing nuanced correctness judgments that token-based metrics cannot.  
- Higher ICE-Scores imply stronger functional, semantic, and stylistic alignment with intended solutions.  
- The metric shows high correlation with human expert ratings, outperforming traditional metrics like BLEU, METEOR, and CodeBLEU.  

However:
- It depends on the quality and bias of the judging LLM.  
- Results may vary depending on prompt structure and model temperature, requiring standardized configurations for reproducibility.
In summary, ICE-Score bridges the gap between automatic evaluation and human reasoning for software engineering code generation.

## References
1. *Zhuo, T. Y. (2023).* ICE-Score: Instructing Large Language Models to Evaluate Code. 
   [https://doi.org/10.48550/arXiv.2304.14317](https://doi.org/10.48550/arXiv.2304.14317)

### Additional References in Dataset
- <ReferencesIndex ids={['12','24','40']} />