---
id: energy-metrics
title: Energy & Resource Metrics
sidebar_label: Energy & Resource
---

## Introduction

This group of metrics evaluates the **Efficiency** and **Sustainability** of Large Language Models (LLMs). As LLMs grow in scale, their "large parameter scale... necessitates substantial computational resources," leading to significant overhead. These metrics are not focused on task correctness, but rather on quantifying the computational, time, and energy costs required for training and inference.

This category includes broad dimensions like **Resource Consumption** , specific benchmark metrics like **Energy Consumption** , and holistic categories like **Efficiency** from the HELM benchmark.

## 1. Resource Consumption (General Category)

### Definition

**Resource Consumption** (or "Resource overhead") is a comprehensive evaluation dimension that assesses the high demands of LLMs. This category is broken down into three key components:
- **Computational Power Consumption:** The "high demands for computing power and storage resources" required for training and inference.
- **Time Consumption:** The "extensive training time" (weeks or months) and "multiple rounds of parameter tuning" required.
- **Energy Consumption:** The "significant overhead" from "electricity cost," as high-performance devices "consume substantial amounts of electricity" and require cooling systems.

### Purpose
The purpose of evaluating this dimension is to "optimize model efficiency, reduce resource consumption, and improve application efficiency".

### Applications
- Used as a high-level dimension for the "comprehensive evaluation of large language models".
- Assesses the overall training and inference "resource overheads".

## 2. Efficiency (HELM Benchmark Category)

### Definition

**Efficiency** is one of the seven top-level metrics in the **HELM (Holistic Evaluation of Language Models)** benchmark .

### Purpose
It is used to provide a "holistic evaluation" of model efficiency by measuring several practical proxies for computational cost .

### Applications
HELM specifically measures this category using the following sub-metrics:
- **Training Time:** The time required to train the model on a proxy task .
- **Inference Time:** The time required to process prompts and generate outputs .
- **Number of Parameters:** The size of the model .

## 3. Energy Consumption (Mercury Benchmark)
### Definition
**Energy Consumption** is a specific metric used within the **Mercury** benchmark , which is "designed to assess the efficiency of code generated by Large Language Models" .

### Purpose
This metric provides a direct, quantitative measure of the sustainability and efficiency of an LLM's *generated code*, rather than the LLM itself.

### Applications
- The metric is measured by running the LLM-generated code using "preset test cases... in a fixed hardware environment" and "recording execution time, memory usage, and **energy consumption**" .

## 4. Comparative Summary

| Metric | Domain | Scope | Measured Components |
| :--- | :--- | :--- | :--- |
| **Resource Consumption** | General LLM Evaluation | Model (Training & Inference) | Power, Time, and Energy (as a general dimension)  |
| **Efficiency (HELM)** | General LLM Evaluation | Model (Training & Inference) | Training Time, Inference Time, # of Parameters |
| **Energy Consumption (Mercury)** | Code Generation | Generated Code (Runtime) | Energy consumed by the output code during execution  |

## References
- L. Lin, D. Zhu and J. Shang, "Overview of the Comprehensive Evaluation of Large Language Models," 2024 IEEE Smart World Congress (SWC), Nadi, Fiji, 2024, pp. 1504-1512, doi: 10.1109/SWC62898.2024.00231.

- Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., ... & RÃ©, C. (2022). *Holistic Evaluation of Language Models*.[https://doi.org/10.48550/arXiv.2211.09110](https://doi.org/10.48550/arXiv.2211.09110)

- Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., ... & Zhang, S. (2024). *A Survey on Evaluating Large Language Models in Code Generation Tasks*. doi: [https://doi.org/10.48550/arXiv.2408.16498](https://doi.org/10.48550/arXiv.2408.16498).

Chang, Y., et al. (2023). A survey on evaluation of large language models (arXiv:2307.03109). arXiv. https://doi.org/10.48550/arXiv.2307.03109
