---
id: llm-evaluation-metrics
title: LLM-as-Judge and Sample Killings
sidebar_label: LLM-as-Judge / Killings
---

## Introduction

This document covers two distinct but related evaluation techniques that use Large Language Models (LLMs) as part of the evaluation process itself.

1.  **LLM Sample Killings** is a strategy to measure and improve **test effectiveness**. It identifies which test cases are most valuable by measuring their ability to "kill" (i.e., detect and falsify) incorrect code samples generated by a set of LLMs.
2.  **LLM-as-Judge Ratings** is a **human-aligned evaluation** method. It uses a powerful LLM to act as a "judge" and assign a qualitative score to another model's output (such as a textual description), assessing its quality against human-defined criteria.

These methods represent a shift from traditional evaluation, leveraging LLMs' own outputs or reasoning capabilities to create more robust benchmarks and assessments.

---

## 1. LLM Sample Killings

### Definition
**LLM Sample Killings** is a testing requirement used for test-suite reduction, particularly in the **HumanEval+** benchmark. It is defined as the set of incorrect LLM-generated code samples that a specific test case can successfully detect and falsify.

This empirical metric is used to find the most effective test cases: a test is considered valuable if it "kills" many incorrect LLM solutions that other, simpler tests might miss.

### Purpose
The primary purpose of this metric is **test-suite reduction** and ensuring **test effectiveness**.

By identifying the tests that "kill" the most diverse set of empirically wrong LLM samples, a large test suite (like HumanEval+) can be "distilled" into a much smaller one (like HumanEval+-MINI). This reduced suite remains highly effective and can "achieve almost the same pass@1* drop" as the full suite, making rigorous evaluation much more efficient.

### Applications
* **Software Engineering / Code Generation**
* **Test Effectiveness / Model Robustness**
* Benchmark creation and test-suite minimization

### Advantages
* **Highly Effective:** This strategy was found to be the "most effective" for test-suite reduction, preserving the benchmark's ability to find errors.
* **Practical:** It is an empirical, practical measure of a test case's value, based on real-world LLM failures.

### Limitations
* **Requires Cross-Validation:** To evaluate a new LLM, one must use the "sample kills" data gathered from *other* LLMs (a leave-one-out cross-validation approach).

---

## 2. LLM-as-Judge Ratings (1-3 Scale)

### Definition
**LLM-as-Judge Ratings** is an evaluation method used in the **HumanEval-V** benchmark where a capable LLM (e.g., GPT-4o) acts as a "judge" to rate the quality of problem specifications (PS) generated by other LMMs.

The judge rates the generated text on a **1-3 scale** (1 = severe error, 3 = near perfect) across three distinct dimensions:
1.  **Basic-Level Perception:** Identifying basic visual elements (shapes, text, colors, etc.).
2.  **High-Level Comprehension:** Understanding the relationships, patterns, constraints, and operations depicted in the visual context.
3.  **Contextual Interpretation:** The clarity of the description, ensuring it is free of vagueness or hallucinations.

### Purpose
The goal is to provide a scalable, **human-aligned evaluation** of an LMM's *visual understanding* capabilities, separate from its *coding* proficiency. It assesses the quality of the intermediate textual description that the model generates from a visual diagram.

### Applications
* **Multimodal Code Generation**
* **Visual Reasoning**
* Evaluating the quality of intermediate textual representations (e.g., problem specifications).

### Advantages
* **Decouples Skills:** It allows for the isolated evaluation of visual comprehension without penalizing a model for poor coding ability.
* **Scalable & Qualitative:** It offers a way to get nuanced, human-like qualitative feedback at scale without requiring expensive human annotation for every output.

### Limitations
* **Limited Robustness:** The study found "limitations of using LLM-as-judge as an evaluation tool".
* **Poor Correlation:** The ratings difference between problem specifications that led to *passed* tasks versus *failed* tasks was minimal, suggesting the judge's ratings did not strongly correlate with downstream functional correctness.
* **Rigid Comparisons:** The method may suffer from "rigid comparisons" to the human-annotated ground truth, which is why the benchmark emphasizes functional pass rates as the primary metric.

---

## 3. Comparative Summary

| Metric | Core Concept | Evaluation Target | Primary Goal | Domain |
| :--- | :--- | :--- | :--- | :--- |
| **LLM Sample Killings** | Using failed LLM outputs | Test cases | To build an efficient, robust test suite | Code Generation |
| **LLM-as-Judge Ratings** | Using an LLM's evaluative ability | LMM-generated text | To provide qualitative, human-aligned ratings | Visual Reasoning |

---

## References
* Liu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. arXiv. https://doi.org/10.48550/arXiv.2305.01210
* Zhang, F., Wu, L., Bai, H., Lin, G., Li, X., Yu, X., Wang, Y., Chen, B., & Keung, J. (2025). HumanEval-V: Benchmarking high-level visual reasoning with complex diagrams in coding tasks. arXiv. https://doi.org/10.48550/arXiv.2410.12381