---
id: reward-score
title: Reward Score
sidebar_label: Reward Score
---

## Definition

The **Reward Score** (also referred to as reward/score) is an objective, quantitative metric used in the evaluation of LLM-based autonomous agents. It is a primary metric in the category of **Task Success**, which measures how well an agent can successfully complete its goals.

This metric is typically defined by the environment or benchmark itself, where the agent receives a score based on its performance.

***

## Formula (General Idea)

The specific formula for the Reward Score is task-dependent and defined by the benchmark being used. It is a quantitative value where **higher values indicate greater task completion ability**.

For example, in a benchmark like WebShop, the reward score could be based on a combination of task completion and other factors.

***

## Purpose

The purpose of the Reward Score is to provide a "concrete, measurable insight"  into an agent's performance and its ability to achieve its defined objectives. It is one of the key metrics used to quantify an agent's task completion ability.

***

## Domains

* LLM Agents / Autonomous Agents
* Objective Evaluation of Agents

***

## Advantages

* **Objective:** It is an objective metric that provides "concrete, measurable insights" into an agent's performance, as opposed to subjective human judgments.
* **Direct Measurement:** It directly measures the agent's ability to complete a task successfully.
* **Comparable:** As a quantitative score, it allows for direct comparison and tracking of performance over time between different agents.

***

## Limitations

* While the metric itself is objective, it may not capture all aspects of an agent's performance, such as "intelligence or user-friendliness," which are better assessed via subjective evaluation.

***

## Key References
* Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., & Wen, J.-R. (2024). A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), 186345. https://doi.org/10.1007/s11704-024-40231-1

* Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J. B., Shu, T., & Gan, C. (2024). Building cooperative embodied agents modularly with large language models. arXiv. https://doi.org/10.48550/arXiv.2307.02485

* Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023). ReAct: Synergizing reasoning and acting in language models. arXiv. https://doi.org/10.48550/arXiv.2210.03629

* Mehta, N., Teruel, M., Sanz, P. F., Deng, X., Awadallah, A. H., & Kiseleva, J. (2024). *Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback*.https://doi.org/10.48550/arXiv.2304.10750