---
id: mutant-killings
title: Mutant Killings (Mutation Score)
sidebar_label: Mutant Killings
---

## Definition
**Mutant Killings**, also known as **Mutation Score**, is a metric used to precisely evaluate the effectiveness of a test suite. It is derived from **Mutation Testing (or Mutation Analysis)**, a technique that involves:
1.  Creating a large number of artificial buggy versions of a program (the "ground-truth solution"). Each version is called a **mutant** and contains one subtle, seeded bug (e.g., changing a `<` to a `>`).
2.  Running the test suite against these mutants.
3.  If a test case causes a mutant to fail (i.e., it detects the bug), that mutant is considered **"killed"**.

The "Mutant Killings" metric is the resulting score that measures what percentage of mutants the test suite successfully detected.

***

## Formula (General Idea)
The metric is formally known as the **Mutation Score**, which is the ratio of killed mutants to the total number of non-equivalent mutants.

$$
\text{Mutation Score} = \frac{\text{Number of Killed Mutants}}{\text{Total Number of Mutants}}
$$

A higher score indicates a more effective test suite, as it can detect a larger percentage of the seeded bugs.

***

## Purpose
The primary purpose of measuring mutant killings is to **quantify test effectiveness more precisely than code coverage**.

A test suite might achieve high code coverage (executing most of the code) but still be ineffective at finding critical defects. Mutation testing provides a better assessment by checking if the tests can actually detect faults (the mutants).

In the context of evaluating LLM-generated code, it is used as a "testing requirement" for **test-suite reduction**. The goal is to create a minimal set of tests (like in the HumanEval+-Mini benchmark) that preserves the full test suite's effectiveness, meaning the reduced suite can still kill the same set of mutants.
***

## Domains
* Software Engineering
* Test Effectiveness Evaluation
* Mutation Analysis
* Test-Suite Reduction
* Code Generation (for evaluating test suites in benchmarks)

***

## Advantages
* **Precise Effectiveness Measure:** It is considered a more precise and rigorous way to evaluate test suite quality compared to code coverage.
* **Focuses on Defect Detection:** Instead of just measuring which lines are executed, it measures the test suite's actual ability to find faults.

***

## Limitations
* **Theoretical Basis:** It is a "theoretical" metric of test adequacy, based on artificial bugs (mutants). This may not perfectly align with "empirical" effectiveness, such as the ability to detect real, incorrect samples generated by LLMs.
* **Cost:** Generating and running tests against a large number of mutants can be computationally expensive (though this is a limitation of the technique, not the metric itself).

***

## Key References
* Liu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. arXiv. https://doi.org/10.48550/arXiv.2305.01210
