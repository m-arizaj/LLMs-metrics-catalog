---
id: performance
title: Performance (Functional & Efficiency)
sidebar_label: Performance
---

## Introduction

In the evaluation of Large Language Models, particularly in code generation and software engineering domains, **Performance** is a broad category of metrics. It is used to quantify the model's success in two key areas:

1.  **Functional Performance / Correctness:** Does the code generated by the model work as intended? This is typically measured by executing the code against a set of test cases.
2.  **Efficiency Evaluation:** How efficient is the model in its code generation process, especially when measured against a baseline?

This category includes several specific metrics, such as scores based on unit and integration test pass rates, as well as differential scores that compare a model's performance to a standard.

## 1. Functional Performance (General)

### Definition

**Functional Performance** is a general metric used to evaluate the **Functional Correctness** of generated code. It assesses whether the model's output (e.g., a function, a class, or a patch) behaves according to the requirements, which is often validated by executing it.

### Applications

* Used in benchmarks like **DevQualityEval** to assess the quality and correctness of code generated by LLMs.
* Domain: Software Engineering / Code Generation.

## 2. Unit Test Performance

### Definition

**Unit Test Performance** is a specific measure of **Functional Correctness** that evaluates whether the code generated by an LLM passes a predefined set of unit tests. Unit tests are fine-grained tests that check the correctness of individual components (e.g., a single function or method) in isolation.

### Applications

* Used in benchmarks like **LoCoBench** to evaluate the functional correctness of models in long-context software engineering tasks.

## 3. Integration Test Performance

### Definition

**Integration Test Performance** is another measure of **Functional Correctness**. Unlike unit tests, integration tests evaluate the model's ability to generate code that functions correctly when combined with other parts of a larger system. It checks the interactions between different modules or components.

### Applications

* Used in benchmarks like **LoCoBench** to assess if a model's generated code integrates properly within a broader software context.

## 4. Differential Performance Score

### Definition

The **Differential Performance Score** is a metric used for **Efficiency Evaluation** in code generation. It is designed to measure the *difference* in performance (such as execution speed or resource usage) between the code generated by an LLM and a baseline or reference implementation.

### Applications

* Used in the **EVALPERF** benchmark to evaluate the efficiency of LLM-generated code.

## 5. Normalized Differential Performance Score

### Definition

The **Normalized Differential Performance Score** is a variant of the Differential Performance Score, also used for **Efficiency Evaluation**. By normalizing the score, it allows for a more standardized comparison of performance differences across different tasks or models, which might have different scales of performance.

### Applications

* Also used within the **EVALPERF** benchmark to provide a standardized metric for code generation efficiency.

## 6. Comparative Summary

| Metric | Category | Benchmark | Purpose |
| :--- | :--- | :--- | :--- |
| **Functional Performance** | Functional Correctness | DevQualityEval | Assesses if generated code works as intended. |
| **Unit Test Performance** | Functional Correctness | LoCoBench | Assesses correctness at the individual function/method level. |
| **Integration Test Perf.** | Functional Correctness | LoCoBench | Assesses correctness of interactions between code components. |
| **Diff. Performance Score** | Efficiency Evaluation | EVALPERF | Measures the performance *difference* against a baseline. |
| **Normalized Diff. Perf. Score** | Efficiency Evaluation | EVALPERF | Provides a *standardized* score for performance differences. |

## References

* Bistarelli, S., Fiore, M., Mercanti, I. et al. Usage of Large Language Model for Code Generation Tasks: A Review. SN COMPUT. SCI. 6, 673 (2025). https://doi-org.ezproxy.uniandes.edu.co/10.1007/s42979-025-04241-5
* Liu, J., Xie, S., Wang, J., Wei, Y., Ding, Y., & Zhang, L. (2024). Evaluating language models for efficient code generation (arXiv:2408.06450). arXiv. https://doi.org/10.48550/arXiv.2408.06450
* Qiu, J., Liu, Z., Liu, Z., Murthy, R., Zhang, J., Chen, H., Wang, S., Zhu, M., Yang, L., Tan, J., Cen, Z., Qian, C., Heinecke, S., Yao, W., Savarese, S., Xiong, C., & Wang, H. (2025). LoCoBench: A benchmark for long-context large language models in complex software engineering (arXiv:2509.09614). arXiv. https://doi.org/10.48550/arXiv.2509.09614