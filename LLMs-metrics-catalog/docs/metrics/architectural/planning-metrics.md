---
id: planning-metrics
title: Planning Metrics
sidebar_label: Planning Metrics
---

## Introduction

In the context of LLM-based autonomous agents, **planning** is a critical capability that allows an agent to deconstruct a complex task into simpler, solvable subtasks. This module is essential for making agents "behave more reasonably, powerfully, and reliably".

For tool-learning agents, this is especially important, as "inter-connected tools require holistic and efficient planning" to be used correctly.

To evaluate how well an agent formulates these plans, specific metrics are used. These metrics generally fall into two categories: **efficiency metrics**, which measure the cost or length of a plan, and **output quality metrics**, which measure the correctness or similarity of the plan compared to an ideal solution. This document covers two key metrics used in agent evaluation: **Length of Planning** and **Plan Score**.

## 1. Length of Planning

### Definition

**Length of Planning** is an objective evaluation metric used to assess the **efficiency** of an autonomous agent. It is defined as the number of steps, operations, or reasoning steps an agent includes in its generated plan to accomplish a task.

### Purpose

The primary purpose of this metric is to quantify the efficiency of an agent's reasoning process. In general, a plan with fewer steps is considered more efficient, assuming it successfully completes the task. It is often measured alongside other efficiency metrics like development cost and inference speed.

### Applications

* **LLM Agents / Autonomous Agents:** Used in objective evaluation protocols to measure the efficiency of agent performance.
* **Task Planning:** It helps compare the conciseness of plans generated by different agents for the same complex task.

### Limitations

* This metric only measures efficiency (length) and does not, by itself, measure the **correctness** or **quality** of the plan. A short plan may be inefficient if it fails to solve the task.

## 2. Plan Score

### Definition

**Plan Score** is an objective metric introduced in the **T-Eval** benchmark  to evaluate the **output quality** of an agent's plan. It measures the **similarity between the agent's predicted plan and a "gold answer"** (a correct plan, often from human annotators).

The score is calculated based on the F1-score (harmonic mean) of precision and recall, which are derived from the longest common subsequence of actions between the predicted and gold plans.

### Formula (General Idea)

The calculation involves several steps:
1.  **Similarity Matrix ($S$):** First, a similarity matrix is computed for all action pairs ($a_i, a_j$) between the predicted plan and the gold plan. This similarity considers both the tool chosen and its arguments.
    $$
    S_{i,j}=\beta\sigma(\text{tool}_{i},\text{tool}_{j})+(1-\beta)\sigma(\text{args}_{i},\text{args}_{j})
    $$
2.  **Longest Subsequence ($l$):** The Longest Increasing Subsequence (LIS) algorithm is used on the similarity matrix $S$ to find the length ($l$) of the longest-ordered, matching action sequence.
3.  **Precision ($p$) and Recall ($r$):** Precision and recall are calculated based on $l$ and the lengths of the predicted ($n^{\text{pred}}$) and gold ($n^{\text{gt}}$) plans.
    $$
    p = l / n^{\text{pred}}
    $$
    $$
    r = l / n^{\text{gt}}
    $$
4.  **Plan Score:** The final score is the F1-score (harmonic mean) of precision and recall.
    $$
    \text{planscore} = \frac{2pr}{p+r}
    $$

### Purpose

The purpose of the Plan Score is to quantify the quality of a generated plan, rewarding agents that produce plans structurally and functionally similar to a human-annotated correct solution.

### Applications

* **LLM Agents / Tool Learning:** Used to evaluate the quality of plans generated for multi-step, multi-tool tasks.
* **T-Eval Benchmark:** The metric is a core component of the T-Eval benchmark for evaluating tool utilization.

### Limitations

* This metric requires a "gold answer" or human-annotated plan for comparison. This makes it difficult to use in open-ended scenarios where multiple, distinct plans could all be correct.
* The calculation is complex, relying on similarity matching and subsequence algorithms.

## 3. Comparative Summary

| Metric | Primary Goal | How it Works | Category | Requires Gold Standard? |
| :--- | :--- | :--- | :--- | :--- |
| **Length of Planning** | Measure plan **efficiency**  | Counts the number of steps in the generated plan. | Efficiency | No |
| **Plan Score** | Measure plan **quality** and **correctness**  | Calculates an F1-score based on similarity (tool & args) to a gold-standard plan. | Output Quality | **Yes**  |

## References

* Chen, Z., Du, W., Zhang, W. et al. (2023). T-eval: Evaluating the tool utilization capability step by step. *ArXiv preprint arXiv:2312.14033*. 
* Liu, B., Jiang, Y., Zhang, X. et al. (2023a). Llm+ p: empowering large language models with optimal planning proficiency. *ArXiv preprint arXiv:2304.11477*.

### Additional References

* Wang, L., Ma, C., Feng, X. et al. A survey on large language model based autonomous agents. Front. Comput. Sci. 18, 186345 (2024). https://doi-org.ezproxy.uniandes.edu.co/10.1007/s11704-024-40231-1 (Paper 49)
* Xu, W., Huang, C., Gao, S. et al. LLM-Based Agents for Tool Learning: A Survey. Data Sci. Eng. (2025). https://doi-org.ezproxy.uniandes.edu.co/10.1007/s41019-025-00296-9 (Paper 48)
