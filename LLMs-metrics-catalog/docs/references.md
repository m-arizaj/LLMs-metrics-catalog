---
id: references
title: References
---

This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link.

- [**1**](https://doi.org/10.1109/SWC62898.2024.00231) — L. Lin, D. Zhu and J. Shang, "Overview of the Comprehensive Evaluation of Large Language Models," 2024 IEEE Smart World Congress (SWC), Nadi, Fiji, 2024, pp. 1504-1512
- [**2**](https://doi.org/10.48550/arXiv.2404.09135) — Hu, T., & Zhou, X.-H. (2024). Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions (Version 1). arXiv.
- [**3**](https://elib.dlr.de/217570/1/Bektas_Ali_MA.pdf) — A. Bektas (2025). Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies. Freie Universität Berlin
- [**4**](https://doi.org/10.48550/arXiv.2211.09110) — Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta-Navas, D., Hudson, D. A., … Koreeda, Y. (2022). Holistic Evaluation of Language Models. arXiv. 
- [**5**](https://doi.org/10.48550/arXiv.2310.19736) — Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B., & Xiong, D. (2023). Evaluating Large Language Models: A Comprehensive Survey (Version 3). arXiv.
- [**6**](https://doi.org/10.48550/arXiv.2308.10620) — Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., & Wang, H. (2023). Large Language Models for Software Engineering: A Systematic Literature Review (Version 6). arXiv.
- [**7**](https://doi.org/10.48550/arXiv.2505.08903) — Hu, X., Niu, F., Chen, J., Zhou, X., Zhang, J., He, J., Xia, X., & Lo, D. (2025). Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks (Version 4). arXiv.
- [**8**](https://doi.org/10.48550/arXiv.2311.07397) — Wang, J., Wang, Y., Xu, G., Zhang, J., Gu, Y., Jia, H., Wang, J., Xu, H., Yan, M., Zhang, J., & Sang, J. (2023). AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation (Version 2). arXiv.
- [**9**](https://doi.org/10.48550/arXiv.2307.03109) — Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2023). A Survey on Evaluation of Large Language Models (Version 9). arXiv.
- [**10**](https://doi.org/10.48550/arXiv.2408.16498) — Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., Wu, J., Wang, Y., Gao, Q., Wang, J., Ye, W., & Zhang, S. (2024). A Survey on Evaluating Large Language Models in Code Generation Tasks (Version 2). arXiv.
- [**11**](https://doi.org/10.48550/arXiv.2505.20854) — Zhou, X., Kim, K., Zhang, T., Weyssow, M., Gomes, L. F., Yang, G., Liu, K., Xia, X., & Lo, D. (2025). An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks (Version 2). arXiv.
- [**12**](https://doi.org/10.48550/arXiv.2304.14317) — Zhuo, T. Y. (2023). ICE-Score: Instructing Large Language Models to Evaluate Code (Version 2). arXiv.
- [**13**](https://doi.org/10.48550/arXiv.2403.08604) — Li, B., Wu, W., Tang, Z., Shi, L., Yang, J., Li, J., Yao, S., Qian, C., Hui, B., Zhang, Q., Yu, Z., Du, H., Yang, P., Lin, D., Peng, C., & Chen, K. (2024). Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study (Version 3). arXiv.
- [**14**](https://doi.org/10.48550/arXiv.2310.06770) — Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. (2023). SWE-bench: Can Language Models Resolve Real-World GitHub Issues? (Version 3). arXiv.
- [**15**](https://doi.org/10.48550/arXiv.2206.04615) — Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., … Wu, Z. (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. arXiv.
- [**16**](https://doi.org/10.1162/coli_a_00524) — Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., & Ahmed, N. K. (2024). Bias and Fairness in Large Language Models: A Survey. Computational Linguistics, 50(3), 1097–1179. 
- [**17**](https://doi.org/10.56038/oprd.v4i1.444) — Ersoy, P., & Erşahin, M. (2024). Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation. Orclever Proceedings of Research and Development, 4(1), 52–58.
- [**18**](https://doi.org/10.56155/978-81-955020-9-7-24) — Anand, A., Chopra, S., & Arora, M. (2024). Analysis of LLM Code Synthesis in Software Productivity. In Applied Intelligence and Computing (pp. 247–259). Soft Computing Research Society.
- [**19**](https://doi.org/10.48550/arXiv.2406.12655) — Paul, D. G., Zhu, H., & Bayley, I. (2024). Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review (Version 1). arXiv.
- [**20**](https://doi.org/10.48550/arXiv.2305.01210) — Liu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation (Version 3). arXiv.
- [**21**](https://doi.org/10.4218/etrij.2023-0357) — Yeo, S., Ma, Y., Kim, S. C., Jun, H., & Kim, T. (2024). Framework for evaluating code generation ability of large language models. ETRI Journal, 46(1), 106–117.
- [**22**](https://doi.org/10.48550/arXiv.2401.06401) — Li, J., Li, G., Zhao, Y., Li, Y., Jin, Z., Zhu, H., Liu, H., Liu, K., Wang, L., Fang, Z., Wang, L., Ding, J., Zhang, X., Dong, Y., Zhu, Y., Gu, B., & Yang, M. (2024). DevEval: Evaluating Code Generation in Practical Software Projects (Version 4). arXiv.
- [**23**](https://doi.org/10.48550/arXiv.2212.10264) — Wang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., Nallapati, R., Ramanathan, M. K., Roth, D., & Xiang, B. (2022). ReCode: Robustness Evaluation of Code Generation Models (Version 1). arXiv.
- [**24**](https://doi.org/10.48550/arXiv.2302.05527) — Zhou, S., Alon, U., Agarwal, S., & Neubig, G. (2023). CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code (Version 2). arXiv.
- [**25**](https://doi.org/10.1016/j.jss.2023.111741) — Evtikhiev, M., Bogomolov, E., Sokolov, Y., & Bryksin, T. (2023). Out of the BLEU: How should we assess quality of the Code Generation models? Journal of Systems and Software, 203, 111741.
- [**26**](https://doi.org/10.48550/arXiv.2406.06902) — Yang, G., Zhou, Y., Chen, X., & Zhang, X. (2024). CodeScore-R: An Automated Robustness Metric for Assessing the FunctionalCorrectness of Code Synthesis (Version 1). arXiv.
- [**27**](https://doi.org/10.18653/v1/2024.findings-emnlp.303) — Zhang, Y., Wang, S., Qian, H., Wang, Z., Shang, M., Liu, L., Gouda, S. K., Ray, B., Ramanathan, M. K., Ma, X., & Deoras, A. (2024). CodeFort: Robust Training for Code Generation Models. In Findings of the Association for Computational Linguistics: EMNLP 2024 (pp. 5262–5277). Findings of the Association for Computational Linguistics: EMNLP 2024. Association for Computational Linguistics.
- [**28**](https://doi.org/10.1007/s42979-025-04241-5) — Bistarelli, S., Fiore, M., Mercanti, I., & Mongiello, M. (2025). Usage of Large Language Model for Code Generation Tasks: A Review. SN Computer Science, 6(6).
- [**29**](https://doi.org/10.1007/s10009-025-00798-x) — Busch, D., Bainczyk, A., Smyth, S., & Steffen, B. (2025). LLM-based code generation and system migration in language-driven engineering. International Journal on Software Tools for Technology Transfer, 27(1), 137–147.
- [**30**](https://doi.org/10.1007/s10710-024-09494-2) — Hemberg, E., Moskal, S., & O’Reilly, U.-M. (2024). Evolving code with a large language model. Genetic Programming and Evolvable Machines, 25(2).
- [**31**](https://doi.org/10.48550/arXiv.2107.03374) — Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). Evaluating Large Language Models Trained on Code (Version 2). arXiv.
- [**32**](https://doi.org/10.48550/arXiv.2102.04664) — Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., … Liu, S. (2021). CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation (Version 2). arXiv.
- [**33**](https://doi.org/10.48550/arXiv.2410.12381) — Zhang, F., Wu, L., Bai, H., Lin, G., Li, X., Yu, X., Wang, Y., Chen, B., & Keung, J. (2024). HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks (Version 3). arXiv.
- [**34**](https://doi.org/10.48550/arXiv.2108.07732) — Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., & Sutton, C. (2021). Program Synthesis with Large Language Models (Version 1). arXiv.
- [**35**](https://doi.org/10.48550/arXiv.2208.08227) — Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., Guha, A., Greenberg, M., & Jangda, A. (2022). MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation (Version 4). arXiv.
- [**36**](https://doi.org/10.48550/arXiv.2404.00599) — Li, J., Li, G., Zhang, X., Dong, Y., & Jin, Z. (2024). EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories (Version 1). arXiv. 
- [**37**](https://doi.org/10.48550/arXiv.2301.09043) — Dong, Y., Ding, J., Jiang, X., Li, G., Li, Z., & Jin, Z. (2023). CodeScore: Evaluating Code Generation by Learning Code Execution (Version 4). arXiv.
- [**38**](https://doi.org/10.1145/3650105.3652295) — Niu, C., Zhang, T., Li, C., Luo, B., & Ng, V. (2024). On Evaluating the Efficiency of Source Code Generated by LLMs. In Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (pp. 103–107). FORGE ’24: 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering. ACM.
- [**39**](https://doi.org/10.3390/digital4010005) — Coello, C. E. A., Alimam, M. N., & Kouatly, R. (2024). Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models. Digital, 4(1), 114–125.
- [**40**](https://doi.org/10.18653/v1/2024.emnlp-main.1118) — Tong, W., & Zhang, T. (2024). CodeJudge: Evaluating Code Generation with Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 20032–20051). Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
- [**41**](https://doi.org/10.48550/arXiv.2408.06450) — Liu, J., Xie, S., Wang, J., Wei, Y., Ding, Y., & Zhang, L. (2024). Evaluating Language Models for Efficient Code Generation (Version 1). arXiv.
- [**42**](https://doi.org/10.48550/arXiv.2411.11908) — Nascimento, N., Guimaraes, E., Chintakunta, S. S., & Boominathan, S. A. (2024). LLM4DS: Evaluating Large Language Models for Data Science Code Generation (Version 1). arXiv. 
- [**43**](https://doi.org/10.18653/v1/2021.emnlp-main.685) — Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 8696–8708). Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
- [**44**](https://doi.org/10.48550/arXiv.2207.11280) — Christopoulou, F., Lampouras, G., Gritta, M., Zhang, G., Guo, Y., Li, Z., Zhang, Q., Xiao, M., Shen, B., Li, L., Yu, H., Yan, L., Zhou, P., Wang, X., Ma, Y., Iacobacci, I., Wang, Y., Liang, G., Wei, J., … Liu, Q. (2022). PanGu-Coder: Program Synthesis with Function-Level Language Modeling (Version 1). arXiv.
- [**45**](https://doi.org/10.48550/arXiv.2301.03988) — Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L. K., Anderson, C. J., Zi, Y., Poirier, J. L., Schoelkopf, H., Troshin, S., Abulkhanov, D., Romero, M., Lappert, M., … von Werra, L. (2023). SantaCoder: don’t reach for the stars! (Version 2). arXiv.
- [**46**](https://doi.org/10.48550/arXiv.2303.17568) — Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y., Su, T., Yang, Z., & Tang, J. (2023). CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X (Version 2). arXiv.
- [**47**](https://doi.org/10.48550/arXiv.2206.08474) — Zhu, M., Jain, A., Suresh, K., Ravindran, R., Tipirneni, S., & Reddy, C. K. (2022). XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence (Version 1). arXiv.
- [**48**](https://doi.org/10.1007/s41019-025-00296-9) — Xu, W., Huang, C., Gao, S., & Shang, S. (2025). LLM-Based Agents for Tool Learning: A Survey. Data Science and Engineering.
- [**49**](https://doi.org/10.1007/s11704-024-40231-1) — Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., & Wen, J. (2024). A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6).
- [**50**](https://doi.org/10.1007/s11432-023-4127-5) — Chen, X., Hu, X., Huang, Y., Jiang, H., Ji, W., Jiang, Y., Jiang, Y., Liu, B., Liu, H., Li, X., Lian, X., Meng, G., Peng, X., Sun, H., Shi, L., Wang, B., Wang, C., Wang, J., Wang, T., … Zhang, L. (2024). Deep learning-based software engineering: progress, challenges, and opportunities. Science China Information Sciences, 68(1).
- [**51**](https://doi.org/10.1007/s44443-025-00074-7) — Rong, Y., Du, T., Li, R., & Bao, W. (2025). Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education. Journal of King Saud University Computer and Information Sciences, 37(5).
- [**52**](https://doi.org/10.1007/s10515-024-00451-y) — Le, K. T., & Andrzejak, A. (2024). Rethinking AI code generation: a one-shot correction approach based on user feedback. Automated Software Engineering, 31(2).
- [**53**](https://doi.org/10.1007/s10462-024-10888-y) — Kumar, P. (2024). Large language models (LLMs): survey, technical frameworks, and future challenges. Artificial Intelligence Review, 57(10).
- [**54**](https://doi.org/10.48550/arXiv.2509.09614) — Qiu, J., Liu, Z., Liu, Z., Murthy, R., Zhang, J., Chen, H., Wang, S., Zhu, M., Yang, L., Tan, J., Cen, Z., Qian, C., Heinecke, S., Yao, W., Savarese, S., Xiong, C., & Wang, H. (2025). LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering (Version 1). arXiv.
- [**55**](https://doi.org/10.48550/arXiv.2506.10833) — Peña, F. C., & Herbold, S. (2025). Evaluating Large Language Models on Non-Code Software Engineering Tasks (Version 1). arXiv.
- [**56**](https://doi.org/10.1007/s43681-025-00721-9) — Afreen, J., Mohaghegh, M., & Doborjeh, M. (2025). Systematic literature review on bias mitigation in generative AI. AI and Ethics, 5(5), 4789–4841.
- [**57**](https://doi.org/10.1109/ACCESS.2024.3482107) — Shao, M., Basit, A., Karri, R., & Shafique, M. (2024). Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges. IEEE Access, 12, 188664–188706.
- [**58**](https://doi.org/10.1109/ACCESS.2024.3403858) — Ságodi, Z., Siket, I., & Ferenc, R. (2024). Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot. IEEE Access, 12, 72303–72316.
- [**59**](https://doi.org/10.1109/ACCESS.2024.3484947) — Black, G., Mathew Vaidyan, V., & Comert, G. (2024). Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation. IEEE Access, 12, 156065–156081.
- [**60**](https://doi.org/10.1109/ACCESS.2025.3553870) — Ko, E., & Kang, P. (2025). Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems. IEEE Access, 13, 52925–52938.
- [**61**](https://doi.org/10.1109/ACCESS.2025.3601206) — Woesle, C., Fischer-Brandies, L., & Buettner, R. (2025). A Systematic Literature Review of Hallucinations in Large Language Models. IEEE Access, 13, 148231–148253.
- [**62**](https://doi.org/10.1016/j.csi.2024.103942) — Li, Y., Liu, P., Wang, H., Chu, J., & Wong, W. E. (2025). Evaluating large language models for software testing. Computer Standards & amp; Interfaces, 93, 103942.
- [**63**](https://doi.org/10.1145/3597503.3639219) — Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., & Lou, Y. (2024). Evaluating Large Language Models in Class-Level Code Generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (pp. 1–13). ICSE ’24: IEEE/ACM 46th International Conference on Software Engineering. ACM. 
- [**64**](https://doi.org/10.1109/TSE.2023.3334955) — Schäfer, M., Nadi, S., Eghbali, A., & Tip, F. (2024). An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. IEEE Transactions on Software Engineering, 50(1), 85–105.
- [**65**](https://doi.org/10.1007/s10664-025-10687-1) — Alhanahnah, M., Rashedul Hasan, M., Xu, L., & Bagheri, H. (2025). An empirical evaluation of pre-trained large language models for repairing declarative formal specifications. Empirical Software Engineering, 30(5).
- [**66**](https://doi.org/10.48550/arXiv.2506.01793) — Guo, Y., Ji, K., Zhu, X., Wang, J., Wen, F., Li, C., Zhang, Z., & Zhai, G. (2025). Human-Centric Evaluation for Foundation Models (Version 1). arXiv.
- [**67**](https://doi.org/10.48550/arXiv.2306.04675) — Stein, G., Cresswell, J. C., Hosseinzadeh, R., Sui, Y., Ross, B. L., Villecroze, V., Liu, Z., Caterini, A. L., Taylor, J. E. T., & Loaiza-Ganem, G. (2023). Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. arXiv. 
- [**68**](https://doi.org/10.48550/arXiv.2509.12395) — Mundhra, Y., Valk, M., & Izadi, M. (2025). Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML (Version 1). arXiv.