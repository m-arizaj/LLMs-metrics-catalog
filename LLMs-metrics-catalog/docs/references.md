---
id: references
title: References
---

This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link.

- [**1**](https://doi.org/10.1109/SWC62898.2024.00231) — Overview of the Comprehensive Evaluation of Large Language Models
- [**2**](https://doi.org/10.48550/arXiv.2404.09135) — Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions
- [**3**](https://www.mi.fu-berlin.de/w/SE/MArbeitReviewLLMs4SE) — Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies
- [**4**](https://doi.org/10.48550/arXiv.2211.09110) — Holistic Evaluation of Language Models
- [**5**](https://doi.org/10.48550/arXiv.2310.19736) — Evaluating Large Language Models: A Comprehensive Survey
- [**6**](https://doi.org/10.48550/arXiv.2308.10620) — Large Language Models for Software Engineering: A Systematic Literature Review
- [**7**](https://doi.org/10.48550/arXiv.2505.08903) — Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks
- [**8**](https://doi.org/10.48550/arXiv.2311.07397) — AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
- [**9**](https://doi.org/10.48550/arXiv.2307.03109) — A Survey on Evaluation of Large Language Models
- [**10**](https://doi.org/10.48550/arXiv.2408.16498) — A Survey on Evaluating Large Language Models in Code Generation Tasks
- [**11**](https://doi.org/10.48550/arXiv.2505.20854) — An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks
- [**12**](https://doi.org/10.48550/arXiv.2304.14317) — ICE-Score: Instructing Large Language Models to Evaluate Code
- [**13**](https://doi.org/10.48550/arXiv.2403.08604) — Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study
- [**14**](https://doi.org/10.48550/arXiv.2310.06770) — SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
- [**15**](https://doi.org/10.48550/arXiv.2206.04615) — Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
- [**16**](https://doi.org/10.1162/coli_a_00524) — Bias and Fairness in Large Language Models: A Survey
- [**17**](https://doi.org/10.56038/oprd.v4i1.444) — Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation
- [**18**](https://doi.org/10.56155/978-81-955020-9-7-24) — Analysis of LLM Code Synthesis in Software Productivity
- [**19**](https://doi.org/10.48550/arXiv.2406.12655) — Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review
- [**20**](https://doi.org/10.48550/arXiv.2305.01210) — Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language 
Models for Code Generation
- [**21**](https://doi.org/10.4218/etrij.2023-0357) — Framework for evaluating code generation ability of large language models
- [**22**](https://doi.org/10.48550/arXiv.2401.0640) — DevEval: Evaluating Code Generation in Practical Software Projects
- [**23**](https://doi.org/10.48550/arXiv.2212.10264) — ReCode: Robustness Evaluation of Code Generation Models
- [**24**](https://doi.org/10.48550/arXiv.2302.05527) — CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
- [**25**](https://doi.org/10.1016/j.jss.2023.111741) — Out of the BLEU: how should we assess quality of the Code Generation models?
- [**26**](https://doi.org/10.7544/issn1000-1239.202330715) — CodeScore-R: An Automated Robustness Metric for Assessing the Functional Correctness of Code
 Synthesis
- [**27**](https://doi.org/10.18653/v1/2024.findings-emnlp.303) — CodeFort: Robust Training for Code Generation Models
- [**28**](https://doi.org/10.1007/s42979-025-04241-5) — Usage of Large Language Model for Code Generation Tasks: A Review
- [**29**](https://doi.org/10.1007/s10009-025-00798-x) — LLM-based code generation and system migration in language-driven engineering
- [**30**](https://doi.org/10.1007/s10710-024-09494-2) — Evolving code with a large language model
- [**31**](https://doi.org/10.48550/arXiv.2107.03374) — Evaluating Large Language Models Trained on Code
- [**32**](https://doi.org/10.48550/arXiv.2102.04664) — CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation
- [**33**](https://doi.org/10.48550/arXiv.2410.12381) — HumanEval: A Benchmark for Evaluating LLMs on Code Generation
- [**34**](https://doi.org/10.48550/arXiv.2108.07732) — Program Synthesis with Large Language Models
- [**35**](https://doi.org/10.48550/arXiv.2208.08227) — MultiPL-E: A Scalable Benchmark for Code Generation Across 18 Languages
- [**36**](https://doi.org/10.48550/arXiv.2404.00599) — EvoCodeBench: An Evolving Benchmark for Code Generation Models
- [**37**](https://doi.org/10.48550/arXiv.2301.09043) — CodeScore: Evaluating Code Generation by Learning Code Execution
- [**38**](https://doi.org/10.1145/3650105.3652295) — On Evaluating the Efficiency of Source Code Generated by LLMs
- [**39**](https://doi.org/10.3390/digital4010005) — Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models
- [**40**](https://doi.org/10.18653/v1/2024.emnlp-main.1118) — CodeJudge: Evaluating Code Generation with Large Language Models
- [**41**](https://doi.org/10.48550/arXiv.2408.06450) — Evaluating Language Models for Efficient Code Generation
- [**42**](https://doi.org/10.48550/arXiv.2411.11908) — LLM4DS: Evaluating Large Language Models for Data Science Code Generation
- [**43**](https://doi.org/10.18653/v1/2021.emnlp-main.685) — CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code 
Understanding and Generatio
- [**44**](https://doi.org/10.48550/arXiv.2207.11280) — PanGu-Coder: Program Synthesis with Function-Level Language Modelin
- [**45**](https://doi.org/10.48550/arXiv.2301.03988) — SantaCoder: Don’t Reach for the Stars! Benchmarking Large Language Models for Code
- [**46**](https://doi.org/10.48550/arXiv.2303.17568) — CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluation
- [**47**](https://doi.org/10.48550/arXiv.2206.08474) — XLCoST: A Benchmark for Cross-Lingual Code Intelligence
- [**48**](https://doi.org/10.1007/s41019-025-00296-9) — LLM-Based Agents for Tool Learning: A Survey
- [**49**](https://doi.org/10.1007/s11704-024-40231-1) — A survey on large language model based autonomous agents
- [**50**](https://doi.org/10.1007/s11432-023-4127-5) — Deep learning-based software engineering: progress, challenges, and opportunities
- [**51**](https://doi.org/10.1007/s44443-025-00074-7) — Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education
- [**52**](https://doi.org/10.1007/s10515-024-00451-y) — Rethinking AI code generation: a one-shot correction approach based on user feedback
- [**53**](https://doi.org/10.1007/s10462-024-10888-y) — Large language models (LLMs): survey, technical frameworks, and future challenges
- [**54**](https://doi.org/10.48550/arXiv.2509.09614) — LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering
- [**55**](https://doi.org/10.48550/arXiv.2506.10833) — Evaluating Large Language Models on Non-Code Software Engineering Tasks
- [**56**](https://doi.org/10.1007/s43681-025-00721-9) — Systematic literature review on bias mitigation in generative AI
- [**57**](https://doi.org/10.1109/ACCESS.2024.3482107) — Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges
- [**58**](https://doi.org/10.1109/ACCESS.2024.3403858) — Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot
- [**59**](https://doi.org/10.1109/ACCESS.2024.3484947) — Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation
- [**60**](https://doi.org/10.1109/ACCESS.2025.3553870) — Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems
- [**61**](https://doi.org/10.1109/ACCESS.2025.3601206) — A Systematic Literature Review of Hallucinations in Large Language Models
- [**62**](https://doi.org/10.1016/j.csi.2024.103942) — Evaluating large language models for software testing
- [**63**](https://doi.org/10.1145/3597503.3639219) — Evaluating Large Language Models in Class-Level Code Generation
- [**64**](https://doi.org/10.1109/TSE.2023.3334955) — An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation
- [**65**](https://doi.org/10.1007/s10664-025-10687-1) — An empirical evaluation of pre-trained large language models for repairing declarative formal specifications
- [**66**](https://doi.org/10.48550/arXiv.2506.01793) — Human-Centric Evaluation for Foundation Models
- [**67**](https://doi.org/10.48550/arXiv.2306.04675) — Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models
- [**68**](https://doi.org/10.48550/arXiv.2509.12395) — Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML