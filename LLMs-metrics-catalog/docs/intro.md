---
id: intro
title: Introduction
---

Welcome to the **LLMs Metrics Catalog**, a structured repository designed to document, classify, and analyze the **evaluation metrics** used to assess **Large Language Models (LLMs)** in **software engineering** tasks.

This catalog compiles a wide range of quantitative and qualitative metrics drawn from academic research and industry benchmarks, covering domains such as:

- Code generation (e.g., Pass@k, CodeBLEU, Execution Accuracy)  
- Test generation and bug fixing (e.g., Non-Trivial Assertion Rate, Timeout Rate)  
- Documentation and natural language tasks (e.g., BLEU, ROUGE-L, BERTScore)  
- Human and semantic evaluation (e.g., Subjective Score, Helpfulness, Truthfulness)

Each entry in the catalog includes:
- A definition of the metric  
- Its purpose and evaluation dimension (performance, robustness, quality, etc.)  
- The source paper or benchmark where it was introduced or used  

---

### Purpose

The goal of this catalog is to serve as a centralized reference for researchers and practitioners exploring how LLMs are evaluated in the context of software engineering.  
It helps identify which metrics dominate current literature, where there are gaps in evaluation coverage, and how new metrics contribute to improving model reliability and interpretability.

---

### Structure

The catalog is organized by metric category and evaluation goal, allowing users to easily navigate:
- Accuracy and Performance  
- Semantic and Structural Quality  
- Robustness and Stability  
- Human and Subjective Evaluation  
- Novelty and Memorization  

Each section includes definitions, examples, and contextual explanations to promote understanding and comparison across metrics.

---

### Research Context

This catalog was developed as part of a research project on LLM evaluation in software engineering, aiming to provide a reproducible and transparent reference for metric selection in academic and applied studies.

---

