# LLMs Metrics Catalog

Welcome to the **LLMs Metrics Catalog**, a structured and continuously evolving repository of **evaluation metrics** used to assess **Large Language Models (LLMs)** across **software engineering tasks** such as code generation, bug fixing, program repair, test generation, and more.

**Visit the catalog:** [https://m-arizaj.github.io/LLMs-metrics-catalog/](https://m-arizaj.github.io/LLMs-metrics-catalog/)

---

## Overview

The **LLMs Metrics Catalog** aims to **centralize, document, and categorize** metrics found in academic and industrial research on LLM evaluation.  
It provides a unified view of how LLMs are assessed in the context of **software engineering**, enabling easier comparison between studies and better understanding of metric coverage across domains.

This project was created as part of a **research initiative focused on evaluating the performance of LLMs in software engineering** â€” highlighting both traditional ML metrics and new emerging indicators specific to program synthesis and code quality.

---

## Objectives

- Collect and organize **quantitative and qualitative metrics** from peer-reviewed papers.
- Classify metrics by **domain**, **purpose**, and **evaluation level** (method, class, project).
- Support research on **benchmarking**, **reliability**, and **robustness** of LLMs.
- Provide a **publicly accessible and searchable catalog** through Docusaurus.

---

## Metric Categories

The catalog currently includes metrics across multiple categories:

| Category | Example Metrics |
|-----------|----------------|
| **Accuracy & Performance** | Accuracy, Precision, Recall, F1-score, Pass@k |
| **Semantic & Structural Quality** | CodeBLEU, AST-based metrics, Execution Accuracy |
| **Text & Natural Language** | BLEU, ROUGE-L, METEOR, BERTScore |
| **Robustness & Generalization** | Timeout Rate, Pass Rate Degradation, Consistency |
| **Human Evaluation** | Subjective Score, Helpfulness, Truthfulness |
| **Novelty & Memorization** | Similarity Score, Memorization Ratio |
| **Test Quality** | Non-Trivial Assertion Rate, Coverage-based metrics |

Each metric entry includes its **definition**, **source paper**, **domain of use**, and **intended purpose**.

---

## Data Sources

Metrics have been extracted and classified from research papers indexed in:
- IEEE Xplore  
- SpringerLink  
- ACM Digital Library  
- ScienceDirect  
- Web of Science  

---

## Project Structure

