"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[136],{3571:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"metrics/clasicas-ml/accuracy","title":"Accuracy","description":"Definition","source":"@site/docs/metrics/clasicas-ml/accuracy.md","sourceDirName":"metrics/clasicas-ml","slug":"/metrics/clasicas-ml/accuracy","permalink":"/LLMs-metrics-catalog/metrics/clasicas-ml/accuracy","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"accuracy","title":"Accuracy","sidebar_label":"Accuracy"},"sidebar":"docsSidebar","previous":{"title":"BLEU","permalink":"/LLMs-metrics-catalog/metrics/accuracy-overlap/bleu"},"next":{"title":"F1-Score","permalink":"/LLMs-metrics-catalog/metrics/clasicas-ml/f1-score"}}');var t=i(4848),r=i(8453);const c={id:"accuracy",title:"Accuracy",sidebar_label:"Accuracy"},a=void 0,l={},o=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={annotation:"annotation",h2:"h2",hr:"hr",li:"li",math:"math",mfrac:"mfrac",mo:"mo",mrow:"mrow",mtext:"mtext",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"})," is a fundamental metric that measures the proportion of correct predictions or outcomes among the total number of cases evaluated. In the context of LLMs and software engineering, it can refer to various specific measurements depending on the task:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Classification:"})," The ratio of correctly classified instances (e.g., in biomedical NLP tasks, bug detection)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Generation:"})," Often measured by ",(0,t.jsx)(n.strong,{children:"functional correctness"})," (e.g., whether the generated code passes tests like in Pass@k or Execution Accuracy ), ",(0,t.jsx)(n.strong,{children:"exact match"})," (EM) with a reference solution , or specific task success rates (e.g., Compilation Success Rate )."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question Answering:"})," Proportion of questions answered correctly (e.g., Strict Accuracy - SaCC, Lenient Accuracy - LaCC )."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"General LLM Evaluation:"})," Correctness on benchmarks testing knowledge, reasoning, or language understanding (e.g., Exact Match on MMLU )."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,t.jsx)(n.p,{children:"For classification tasks, the standard formula is:"}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mtext,{children:"Accuracy"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsxs)(n.mfrac,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mtext,{children:"True\xa0Positives\xa0(TP)"}),(0,t.jsx)(n.mo,{children:"+"}),(0,t.jsx)(n.mtext,{children:"True\xa0Negatives\xa0(TN)"})]}),(0,t.jsx)(n.mtext,{children:"Total\xa0Population\xa0(TP\xa0+\xa0TN\xa0+\xa0FP\xa0+\xa0FN)"})]})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Population (TP + TN + FP + FN)}}"})]})})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TP:"})," Correctly identified positive instances."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TN:"})," Correctly identified negative instances."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"FP:"})," Incorrectly identified positive instances (False Positives)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"FN:"})," Incorrectly identified negative instances (False Negatives)."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For generation or QA tasks, it's often simplified to:"}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mtext,{children:"Accuracy"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsxs)(n.mfrac,{children:[(0,t.jsx)(n.mtext,{children:"Number\xa0of\xa0Correct\xa0Predictions"}),(0,t.jsx)(n.mtext,{children:"Total\xa0Number\xa0of\xa0Predictions"})]})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}"})]})})}),"\n",(0,t.jsx)(n.p,{children:'Where "Correct Prediction" is defined by the specific task (e.g., passing tests, matching reference exactly, answering question correctly).'}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"To measure the overall correctness or success rate of a model on a given task. It provides a straightforward indication of how often the model gets the right answer or produces the desired output according to specific criteria."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"General Machine Learning / Classification"}),"\n",(0,t.jsx)(n.li,{children:"LLM Evaluation (General, NLP, Reasoning, Knowledge)"}),"\n",(0,t.jsx)(n.li,{children:"Software Engineering / Code Generation (Functional Correctness, Structural Accuracy)"}),"\n",(0,t.jsx)(n.li,{children:"Biomedical NLP"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal LLMs (Hallucination Evaluation - Discriminative Task)"}),"\n",(0,t.jsx)(n.li,{children:"Code Repair / Bug Fixing (implicitly, via correctness)"}),"\n",(0,t.jsx)(n.li,{children:"Software Design"}),"\n",(0,t.jsx)(n.li,{children:"Data Science Code Generation"}),"\n",(0,t.jsx)(n.li,{children:"Code Refactoring"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"GLUE, SuperGLUE, CLUE"}),"\n",(0,t.jsx)(n.li,{children:"BC5CDR, NCBI Disease, MedNLI, CHEMPROT"}),"\n",(0,t.jsx)(n.li,{children:"HumanEval, MBPP, CodeContests, APPS, SWE-bench"}),"\n",(0,t.jsx)(n.li,{children:"PubMedQA, BioASQ, MedMCQA, EMRQA"}),"\n",(0,t.jsx)(n.li,{children:"HellaSwag, OpenBookQA, TruthfulQA, MMLU, BLiMP, BoolQ, etc."}),"\n",(0,t.jsx)(n.li,{children:"AMBER"}),"\n",(0,t.jsx)(n.li,{children:"CodeXGLUE (e.g., Defect Detection, Clone Detection)"}),"\n",(0,t.jsx)(n.li,{children:"DS-1000"}),"\n",(0,t.jsx)(n.li,{children:"SELU (for Non-Code SE Tasks)"}),"\n",(0,t.jsx)(n.li,{children:"(Many others depending on specific task)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simple and Intuitive:"})," Easy to understand and interpret \u2013 represents the overall percentage of correctness."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Widely Used:"})," A standard metric across many fields, facilitating comparisons."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Misleading on Imbalanced Datasets:"})," Can give a deceptively high score if one class heavily outweighs others (e.g., predicting the majority class always). Metrics like F1-score, Precision, and Recall are often better in such cases."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Doesn't Capture Nuance:"})," Especially in generation tasks, simple accuracy (like Exact Match) might be too strict and fail to reward semantically correct but syntactically different outputs. Functional correctness metrics (like Pass@k) address this for code but might still not capture all aspects of quality (e.g., maintainability, efficiency)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Definition Varies:"}),' The specific definition of "correct" depends heavily on the task and benchmark (e.g., Exact Match vs. Passing Tests vs. Human Judgment).']}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"(Standard ML metric, widely cited across provided papers implicitly or explicitly)"}),"\n",(0,t.jsx)(n.li,{children:"File: exposing flaws.pdf"}),"\n",(0,t.jsx)(n.li,{children:"File: human centric.pdf"}),"\n",(0,t.jsx)(n.li,{children:"File: class level -c.pdf"}),"\n",(0,t.jsx)(n.li,{children:"(Excel Data: Papers 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 18, 19, 37, 40, 43, 49, 55, 61)"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function c(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);