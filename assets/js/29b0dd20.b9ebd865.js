"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7228],{344:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"metrics/logical/exam","title":"EXAM","description":"Introduction","source":"@site/docs/metrics/logical/exam.md","sourceDirName":"metrics/logical","slug":"/metrics/logical/exam","permalink":"/LLMs-metrics-catalog/metrics/logical/exam","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"exam","title":"EXAM","sidebar_label":"EXAM"},"sidebar":"docsSidebar","previous":{"title":"Difference Automata","permalink":"/LLMs-metrics-catalog/metrics/logical/difference-automata"},"next":{"title":"Equivalent","permalink":"/LLMs-metrics-catalog/metrics/logical/equivalent"}}');var s=n(4848),a=n(8453);const r={id:"exam",title:"EXAM",sidebar_label:"EXAM"},o=void 0,l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"1. EXAM (Explainable Automated Debugging)",id:"1-exam-explainable-automated-debugging",level:2},{value:"Definition",id:"definition",level:3},{value:"Purpose and Applications",id:"purpose-and-applications",level:3},{value:"Metrics Used within EXAM",id:"metrics-used-within-exam",level:3},{value:"References",id:"references",level:2}];function d(e){const i={a:"a",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"EXAM (Explainable Automated Debugging)"})," is an evaluation benchmark used in software engineering to measure a model's capability in ",(0,s.jsx)(i.strong,{children:"Fault Localization"})," . Introduced by ",(0,s.jsx)(i.strong,{children:"Tran et al. (2022)"}),' , it provides a standardized framework for assessing how well a model can debug code, specifically its ability to "identify the buggy code lines" .']}),"\n",(0,s.jsxs)(i.p,{children:["Unlike metrics that evaluate code generation (like BLEU), EXAM focuses on code ",(0,s.jsx)(i.em,{children:"verification"})," and ",(0,s.jsx)(i.em,{children:"augmentation"}),", specifically debugging . To quantify performance on this task, the EXAM benchmark uses specific metrics such as Top-N Accuracy and Mean Average Rank (MAR) ."]}),"\n",(0,s.jsx)(i.h2,{id:"1-exam-explainable-automated-debugging",children:"1. EXAM (Explainable Automated Debugging)"}),"\n",(0,s.jsx)(i.h3,{id:"definition",children:"Definition"}),"\n",(0,s.jsxs)(i.p,{children:['EXAM is a benchmark consisting of a set of "288 Python programs with 12 error types" . The primary task for a model evaluated on this benchmark is ',(0,s.jsx)(i.strong,{children:"fault localization"}),': the model must "identify the buggy code lines" within these programs .']}),"\n",(0,s.jsx)(i.h3,{id:"purpose-and-applications",children:"Purpose and Applications"}),"\n",(0,s.jsxs)(i.p,{children:["The purpose of EXAM is to evaluate models' capabilities in ",(0,s.jsx)(i.strong,{children:"code debugging"})," . It is used to quantitatively measure performance in localizing errors in Python programs ."]}),"\n",(0,s.jsx)(i.h3,{id:"metrics-used-within-exam",children:"Metrics Used within EXAM"}),"\n",(0,s.jsx)(i.p,{children:"The EXAM benchmark uses the following metrics to score a model's performance :"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Top-N Accuracy:"})," Measures if the correct buggy line is found among the model's Top-N most likely predictions."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Mean Average Rank (MAR):"})," Measures the average rank (position) of the correct buggy line in the model's prediction list . A lower MAR indicates better performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Chen, X., Hu, X., Huang, Y. et al. Deep learning-based software engineering: progress, challenges, and opportunities. Sci. China Inf. Sci. 68, 111102 (2025). ",(0,s.jsx)(i.a,{href:"https://doi.org/10.1007/s11432-023-4127-5",children:"https://doi.org/10.1007/s11432-023-4127-5"})]}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>o});var t=n(6540);const s={},a=t.createContext(s);function r(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);