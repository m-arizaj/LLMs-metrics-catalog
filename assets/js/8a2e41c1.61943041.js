"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7720],{4704:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"metrics/human/harmlessness","title":"Harmlessness","description":"Definition","source":"@site/docs/metrics/human/harmlessness.md","sourceDirName":"metrics/human","slug":"/metrics/human/harmlessness","permalink":"/LLMs-metrics-catalog/metrics/human/harmlessness","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"harmlessness","title":"Harmlessness","sidebar_label":"Harmlessness"},"sidebar":"docsSidebar","previous":{"title":"Fairness","permalink":"/LLMs-metrics-catalog/metrics/human/fairness"},"next":{"title":"Helpfulness","permalink":"/LLMs-metrics-catalog/metrics/human/helpfulness"}}');var a=n(4848),r=n(8453);const t={id:"harmlessness",title:"Harmlessness",sidebar_label:"Harmlessness"},l=void 0,o={},d=[{value:"Definition",id:"definition",level:2},{value:"How It Is Measured",id:"how-it-is-measured",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function c(e){const s={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.h2,{id:"definition",children:"Definition"}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Harmlessness"})," is a key aspect of ",(0,a.jsx)(s.strong,{children:"human-centered evaluation"})," for Large Language Models (LLMs). It is not a single computational metric but rather a category of assessment that ",(0,a.jsx)(s.strong,{children:"measures a model\u2019s propensity to generate outputs that are unsafe, toxic, offensive, discriminatory, or could incite harm"}),"."]}),"\n",(0,a.jsx)(s.p,{children:'This evaluation is considered a cornerstone of ensuring that LLMs are safe and "not... harmful" for real-world deployment.'}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"how-it-is-measured",children:"How It Is Measured"}),"\n",(0,a.jsxs)(s.p,{children:["Harmlessness is primarily measured using ",(0,a.jsx)(s.strong,{children:"Human Evaluation"}),". This process typically involves:"]}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Red-Teaming:"}),' Human evaluators or other models create "red-teaming" prompts, which are adversarial inputs specifically designed to try and elicit undesirable or harmful behavior from the model.']}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Rating:"})," Human evaluators are then presented with the model's responses to these prompts."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Scoring:"})," The evaluators rate the responses based on their safety, toxicity, offensiveness, and overall harmlessness."]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"purpose",children:"Purpose"}),"\n",(0,a.jsxs)(s.p,{children:["The primary purpose of measuring harmlessness is to ",(0,a.jsx)(s.strong,{children:"ensure LLM safety and alignment with human values"}),". It is a critical component for building trust and preventing models from causing real-world harm to users or perpetuating societal biases."]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"domains",children:"Domains"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Human-centered Evaluation"}),"\n",(0,a.jsx)(s.li,{children:"Ethical Evaluation"}),"\n",(0,a.jsx)(s.li,{children:"LLM Safety & Alignment"}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"HELM (Holistic Evaluation of Language Models)"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Chatbot Arena"})}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Directly Measures Safety:"})," Unlike proxy metrics, human evaluation directly assesses the model's safety and potential for harm from a human perspective."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Captures Nuance:"})," Humans can identify subtle forms of toxicity, discrimination, or manipulation that automated metrics might miss."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Gold Standard:"}),' It is considered a "cornerstone" of safety evaluation and is often the ground truth against which automated metrics are validated.']}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"limitations",children:"Limitations"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Subjectivity:"}),' The assessment of what is "harmful" can be subjective and vary between different human evaluators.']}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Scalability:"})," Relying on human evaluators is expensive, time-consuming, and difficult to scale compared to automated metrics."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Prompt Dependant:"}),' The effectiveness of the evaluation heavily depends on the quality and adversarial nature of the "red-teaming" prompts used.']}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"key-references",children:"Key References"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["Chang, Y., Wang, X., Wang, J., et al. (2023). ",(0,a.jsx)(s.em,{children:"A Survey on Evaluation of Large Language Models"}),". ",(0,a.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:"https://doi.org/10.48550/arXiv.2307.03109"})]}),"\n",(0,a.jsxs)(s.li,{children:["Liang, P., Bommasani, R., Lee, T., et al. (2022). ",(0,a.jsx)(s.em,{children:"Holistic evaluation of language models"}),"."]}),"\n",(0,a.jsxs)(s.li,{children:["Zheng, L., Chiang, W. L., Sheng, Y., et al. (2023). ",(0,a.jsx)(s.em,{children:"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}),"."]}),"\n",(0,a.jsx)(s.li,{children:"(Excel Data: Paper 9)"}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>t,x:()=>l});var i=n(6540);const a={},r=i.createContext(a);function t(e){const s=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(r.Provider,{value:s},e.children)}}}]);