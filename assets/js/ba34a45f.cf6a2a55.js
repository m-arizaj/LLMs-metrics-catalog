"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[527],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const a={},r=t.createContext(a);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},8860:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"metrics/accuracy-overlap/comet","title":"COMET","description":"Definition","source":"@site/docs/metrics/accuracy-overlap/comet.md","sourceDirName":"metrics/accuracy-overlap","slug":"/metrics/accuracy-overlap/comet","permalink":"/LLMs-metrics-catalog/metrics/accuracy-overlap/comet","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"comet","title":"COMET","sidebar_label":"COMET"},"sidebar":"docsSidebar","previous":{"title":"MoverScore","permalink":"/LLMs-metrics-catalog/metrics/accuracy-overlap/moverscore"},"next":{"title":"ICE-Score","permalink":"/LLMs-metrics-catalog/metrics/statistical/ice-score"}}');var a=i(4848),r=i(8453);const s={id:"comet",title:"COMET",sidebar_label:"COMET"},o=void 0,l={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",annotation:"annotation",h2:"h2",hr:"hr",li:"li",math:"math",mi:"mi",mrow:"mrow",msub:"msub",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"COMET"})," (which stands for ",(0,a.jsx)(n.strong,{children:'"COMET: A Neural Framework for MT Evaluation"'})," ) is an automatic evaluation metric for text generation. It is an ",(0,a.jsx)(n.strong,{children:"LLM-based metric"}),", meaning it leverages a pre-trained cross-lingual language model to evaluate the quality of a generated text (hypothesis) by comparing it to a source text and one or more reference texts."]}),"\n",(0,a.jsxs)(n.p,{children:["Unlike traditional metrics that rely on n-gram overlap, COMET is trained to ",(0,a.jsx)(n.strong,{children:"learn human judgments"})," of quality. It is designed to capture the semantic nuances of language more effectively."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,a.jsxs)(n.p,{children:["COMET does not use a simple, static statistical formula like BLEU. Instead, it is a ",(0,a.jsx)(n.strong,{children:"neural framework"})," that uses a pre-trained cross-lingual language model (like XLM-R) as its core."]}),"\n",(0,a.jsxs)(n.p,{children:["The model is fed the ",(0,a.jsx)(n.strong,{children:"source text"}),", the ",(0,a.jsx)(n.strong,{children:"candidate text"})," (hypothesis), and the ",(0,a.jsx)(n.strong,{children:"reference text"}),". The model's embeddings are used to produce a single quality score. This score is the output of a model that has been trained to predict the quality scores provided by human evaluators."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,a.jsxs)(n.p,{children:["The primary purpose of COMET is to provide an automatic evaluation metric that ",(0,a.jsx)(n.strong,{children:"correlates more accurately with human judgments"}),' of translation quality than previous metrics. It aims to move beyond simple surface-level text matching and leverage the deep semantic understanding of LLMs to "better capture the nuances" of language  and provide "accurate and comprehensive guidance" on a model\'s performance.']}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Machine Translation (MT)"}),"\n",(0,a.jsx)(n.li,{children:"Natural Language Generation (NLG)"}),"\n",(0,a.jsx)(n.li,{children:"LLM Evaluation / Natural Language Processing (NLP)"}),"\n",(0,a.jsx)(n.li,{children:"Code Generation / Software Engineering (used as a baseline for comparison)"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High Correlation with Human Judgment:"})," It is specifically trained to learn from and replicate human assessments of quality."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Semantic Understanding:"}),' As an LLM-based metric, it "exhibits greater semantic understanding ability" and can "better capture the nuances" of language compared to metrics that only look at surface-level text overlap.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-lingual Framework:"})," It uses cross-lingual pre-trained models, making it highly effective for its primary task of evaluating machine translation."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Requires Reference Texts:"})," COMET is a reference-based metric. It depends on one or more high-quality human-generated references, which are not always available or are expensive to create."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Poor Correlation with Code Correctness:"}),' When applied to highly specialized domains like code generation, COMET (along with other traditional NLP metrics) has been shown to have an "extremely weak correlation" with functional correctness. For example, on the APPS-Eval dataset, its correlation (',(0,a.jsx)(n.span,{className:"katex",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsx)(n.mrow,{children:(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"r"}),(0,a.jsx)(n.mi,{children:"p"})]})}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"r_p"})]})})}),") was only 0.1187."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computational Cost:"})," As a neural model, it is more computationally expensive to run than simple statistical metrics like BLEU."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Rei, R., Stewart, C., Farinha, A. C., & Lavie, A. (2020). COMET: A neural framework for MT evaluation. En Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 2685\u20132702). Association for Computational Linguistics. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.18653/v1/2020.emnlp-main.213",children:"https://doi.org/10.18653/v1/2020.emnlp-main.213"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Lin, L., Zhu, D., & Shang, J. (2024). Overview of the comprehensive evaluation of Large Language Models. En 2024 IEEE Smart World Congress (SWC) (pp. 1\u20138). IEEE. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.1109/SWC62898.2024.00231",children:"https://doi.org/10.1109/SWC62898.2024.00231"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Dong, Y., Ding, J., Jiang, X., Li, G., Li, Z., & Jin, Z. (2024). CodeScore: Evaluating code generation by learning code execution. arXiv. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2301.09043",children:"https://doi.org/10.48550/arXiv.2301.09043"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);