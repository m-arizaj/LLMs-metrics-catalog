"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1390],{7162:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"metrics/generative/mauve","title":"MAUVE","description":"Definition","source":"@site/docs/metrics/generative/mauve.md","sourceDirName":"metrics/generative","slug":"/metrics/generative/mauve","permalink":"/LLMs-metrics-catalog/metrics/generative/mauve","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"mauve","title":"MAUVE","sidebar_label":"MAUVE"},"sidebar":"docsSidebar","previous":{"title":"KID","permalink":"/LLMs-metrics-catalog/metrics/generative/kernel-distance"},"next":{"title":"SIDE","permalink":"/LLMs-metrics-catalog/metrics/generative/side"}}');var a=t(4848),s=t(8453);const r={id:"mauve",title:"MAUVE",sidebar_label:"MAUVE"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"MAUVE"})," (MAUVE Scores for Generative Models) is an automatic evaluation metric used to assess the quality of text generated by large language models."]}),"\n",(0,a.jsxs)(n.p,{children:["It is categorized as an evaluation method based on large language models, specifically a ",(0,a.jsx)(n.strong,{children:"BERT-based"})," metric. MAUVE is a method that ",(0,a.jsx)(n.strong,{children:"relies on reference texts"})," to compare the generated output against a human-written baseline."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,a.jsx)(n.p,{children:'The provided context describes MAUVE as a "BERT-based" metric. This implies it uses BERT\'s word embeddings to compare the semantic properties of the generated text and the reference text, rather than relying on simple text-form matching. The goal of such models is to move beyond surface-level features and capture deeper semantic nuances.'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,a.jsx)(n.p,{children:'The general purpose of LLM-based metrics like MAUVE is to evaluate the performance of large language models in natural language generation tasks. They aim to "better capture the nuances" between different generative tasks and provide "accurate and comprehensive guidance" on model performance, leveraging a "greater semantic understanding ability" that is more similar to human judgment.'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LLM Evaluation / Natural Language Processing (NLP)"}),"\n",(0,a.jsx)(n.li,{children:"Natural Language Generation (NLG)"}),"\n",(0,a.jsx)(n.li,{children:"Evaluation of Generative Models"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Semantic Understanding:"}),' As an LLM-based metric, it exhibits "greater semantic understanding ability" compared to traditional automated methods that focus on text form matching.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Captures Nuance:"}),' It is designed to "better capture the nuances between different generative tasks".']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human-like Evaluation:"}),' Aims to possess "intelligence similar to that of humans" in its evaluation, providing more accurate guidance on model performance.']}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Requires Reference Texts:"})," MAUVE is an evaluation method that relies on reference texts, which may not always be available or easy to procure."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Pillutla, K., Liu, L., Thickstun, J., Welleck, S., Swayamdipta, S., Zellers, R., Oh, S., Choi, Y., & Harchaoui, Z. (2023). MAUVE scores for generative models: Theory and practice. arXiv. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2212.14578",children:"https://doi.org/10.48550/arXiv.2212.14578"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Lin, L., Zhu, D., & Shang, J. (2024). Overview of the comprehensive evaluation of Large Language Models. En 2024 IEEE Smart World Congress (SWC) (pp. 1\u20138). IEEE. ",(0,a.jsx)(n.a,{href:"https://doi.org/10.1109/SWC62898.2024.00231",children:"https://doi.org/10.1109/SWC62898.2024.00231"})]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);