"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7868],{34:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>g});const r=JSON.parse('{"id":"references","title":"References","description":"This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link.","source":"@site/docs/references.md","sourceDirName":".","slug":"/references","permalink":"/LLMs-metrics-catalog/references","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"references","title":"References"},"sidebar":"docsSidebar","previous":{"title":"Surface-Form Constraints","permalink":"/LLMs-metrics-catalog/metrics/semantic/surface-form-constraints"}}');var a=i(4848),o=i(8453);const s={id:"references",title:"References"},t=void 0,l={},g=[];function h(n){const e={a:"a",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:"This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link."}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/SWC62898.2024.00231",children:(0,a.jsx)(e.strong,{children:"1"})}),' \u2014 L. Lin, D. Zhu and J. Shang, "Overview of the Comprehensive Evaluation of Large Language Models," 2024 IEEE Smart World Congress (SWC), Nadi, Fiji, 2024, pp. 1504-1512']}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2404.09135",children:(0,a.jsx)(e.strong,{children:"2"})})," \u2014 Hu, T., & Zhou, X.-H. (2024). Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://elib.dlr.de/217570/1/Bektas_Ali_MA.pdf",children:(0,a.jsx)(e.strong,{children:"3"})})," \u2014 A. Bektas (2025). Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies. Freie Universit\xe4t Berlin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2211.09110",children:(0,a.jsx)(e.strong,{children:"4"})})," \u2014 Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., R\xe9, C., Acosta-Navas, D., Hudson, D. A., \u2026 Koreeda, Y. (2022). Holistic Evaluation of Language Models. arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2310.19736",children:(0,a.jsx)(e.strong,{children:"5"})})," \u2014 Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Supryadi, Yu, L., Liu, Y., Li, J., Xiong, B., & Xiong, D. (2023). Evaluating Large Language Models: A Comprehensive Survey (Version 3). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2308.10620",children:(0,a.jsx)(e.strong,{children:"6"})})," \u2014 Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., & Wang, H. (2023). Large Language Models for Software Engineering: A Systematic Literature Review (Version 6). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2505.08903",children:(0,a.jsx)(e.strong,{children:"7"})})," \u2014 Hu, X., Niu, F., Chen, J., Zhou, X., Zhang, J., He, J., Xia, X., & Lo, D. (2025). Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks (Version 4). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2311.07397",children:(0,a.jsx)(e.strong,{children:"8"})})," \u2014 Wang, J., Wang, Y., Xu, G., Zhang, J., Gu, Y., Jia, H., Wang, J., Xu, H., Yan, M., Zhang, J., & Sang, J. (2023). AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:(0,a.jsx)(e.strong,{children:"9"})})," \u2014 Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2023). A Survey on Evaluation of Large Language Models (Version 9). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2408.16498",children:(0,a.jsx)(e.strong,{children:"10"})})," \u2014 Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., Wu, J., Wang, Y., Gao, Q., Wang, J., Ye, W., & Zhang, S. (2024). A Survey on Evaluating Large Language Models in Code Generation Tasks (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2505.20854",children:(0,a.jsx)(e.strong,{children:"11"})})," \u2014 Zhou, X., Kim, K., Zhang, T., Weyssow, M., Gomes, L. F., Yang, G., Liu, K., Xia, X., & Lo, D. (2025). An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2304.14317",children:(0,a.jsx)(e.strong,{children:"12"})})," \u2014 Zhuo, T. Y. (2023). ICE-Score: Instructing Large Language Models to Evaluate Code (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2403.08604",children:(0,a.jsx)(e.strong,{children:"13"})})," \u2014 Li, B., Wu, W., Tang, Z., Shi, L., Yang, J., Li, J., Yao, S., Qian, C., Hui, B., Zhang, Q., Yu, Z., Du, H., Yang, P., Lin, D., Peng, C., & Chen, K. (2024). Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study (Version 3). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2310.06770",children:(0,a.jsx)(e.strong,{children:"14"})})," \u2014 Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. (2023). SWE-bench: Can Language Models Resolve Real-World GitHub Issues? (Version 3). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2206.04615",children:(0,a.jsx)(e.strong,{children:"15"})})," \u2014 Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., \u2026 Wu, Z. (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1162/coli_a_00524",children:(0,a.jsx)(e.strong,{children:"16"})})," \u2014 Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., & Ahmed, N. K. (2024). Bias and Fairness in Large Language Models: A Survey. Computational Linguistics, 50(3), 1097\u20131179."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.56038/oprd.v4i1.444",children:(0,a.jsx)(e.strong,{children:"17"})})," \u2014 Ersoy, P., & Er\u015fahin, M. (2024). Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation. Orclever Proceedings of Research and Development, 4(1), 52\u201358."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.56155/978-81-955020-9-7-24",children:(0,a.jsx)(e.strong,{children:"18"})})," \u2014 Anand, A., Chopra, S., & Arora, M. (2024). Analysis of LLM Code Synthesis in Software Productivity. In Applied Intelligence and Computing (pp. 247\u2013259). Soft Computing Research Society."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2406.12655",children:(0,a.jsx)(e.strong,{children:"19"})})," \u2014 Paul, D. G., Zhu, H., & Bayley, I. (2024). Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2305.01210",children:(0,a.jsx)(e.strong,{children:"20"})})," \u2014 Liu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation (Version 3). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.4218/etrij.2023-0357",children:(0,a.jsx)(e.strong,{children:"21"})})," \u2014 Yeo, S., Ma, Y., Kim, S. C., Jun, H., & Kim, T. (2024). Framework for evaluating code generation ability of large language models. ETRI Journal, 46(1), 106\u2013117."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2401.06401",children:(0,a.jsx)(e.strong,{children:"22"})})," \u2014 Li, J., Li, G., Zhao, Y., Li, Y., Jin, Z., Zhu, H., Liu, H., Liu, K., Wang, L., Fang, Z., Wang, L., Ding, J., Zhang, X., Dong, Y., Zhu, Y., Gu, B., & Yang, M. (2024). DevEval: Evaluating Code Generation in Practical Software Projects (Version 4). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2212.10264",children:(0,a.jsx)(e.strong,{children:"23"})})," \u2014 Wang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., Nallapati, R., Ramanathan, M. K., Roth, D., & Xiang, B. (2022). ReCode: Robustness Evaluation of Code Generation Models (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2302.05527",children:(0,a.jsx)(e.strong,{children:"24"})})," \u2014 Zhou, S., Alon, U., Agarwal, S., & Neubig, G. (2023). CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1016/j.jss.2023.111741",children:(0,a.jsx)(e.strong,{children:"25"})})," \u2014 Evtikhiev, M., Bogomolov, E., Sokolov, Y., & Bryksin, T. (2023). Out of the BLEU: How should we assess quality of the Code Generation models? Journal of Systems and Software, 203, 111741."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2406.06902",children:(0,a.jsx)(e.strong,{children:"26"})})," \u2014 Yang, G., Zhou, Y., Chen, X., & Zhang, X. (2024). CodeScore-R: An Automated Robustness Metric for Assessing the FunctionalCorrectness of Code Synthesis (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.18653/v1/2024.findings-emnlp.303",children:(0,a.jsx)(e.strong,{children:"27"})})," \u2014 Zhang, Y., Wang, S., Qian, H., Wang, Z., Shang, M., Liu, L., Gouda, S. K., Ray, B., Ramanathan, M. K., Ma, X., & Deoras, A. (2024). CodeFort: Robust Training for Code Generation Models. In Findings of the Association for Computational Linguistics: EMNLP 2024 (pp. 5262\u20135277). Findings of the Association for Computational Linguistics: EMNLP 2024. Association for Computational Linguistics."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s42979-025-04241-5",children:(0,a.jsx)(e.strong,{children:"28"})})," \u2014 Bistarelli, S., Fiore, M., Mercanti, I., & Mongiello, M. (2025). Usage of Large Language Model for Code Generation Tasks: A Review. SN Computer Science, 6(6)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s10009-025-00798-x",children:(0,a.jsx)(e.strong,{children:"29"})})," \u2014 Busch, D., Bainczyk, A., Smyth, S., & Steffen, B. (2025). LLM-based code generation and system migration in language-driven engineering. International Journal on Software Tools for Technology Transfer, 27(1), 137\u2013147."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s10710-024-09494-2",children:(0,a.jsx)(e.strong,{children:"30"})})," \u2014 Hemberg, E., Moskal, S., & O\u2019Reilly, U.-M. (2024). Evolving code with a large language model. Genetic Programming and Evolvable Machines, 25(2)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2107.03374",children:(0,a.jsx)(e.strong,{children:"31"})})," \u2014 Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., \u2026 Zaremba, W. (2021). Evaluating Large Language Models Trained on Code (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2102.04664",children:(0,a.jsx)(e.strong,{children:"32"})})," \u2014 Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., \u2026 Liu, S. (2021). CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2410.12381",children:(0,a.jsx)(e.strong,{children:"33"})})," \u2014 Zhang, F., Wu, L., Bai, H., Lin, G., Li, X., Yu, X., Wang, Y., Chen, B., & Keung, J. (2024). HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks (Version 3). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2108.07732",children:(0,a.jsx)(e.strong,{children:"34"})})," \u2014 Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., & Sutton, C. (2021). Program Synthesis with Large Language Models (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2208.08227",children:(0,a.jsx)(e.strong,{children:"35"})})," \u2014 Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., Guha, A., Greenberg, M., & Jangda, A. (2022). MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation (Version 4). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2404.00599",children:(0,a.jsx)(e.strong,{children:"36"})})," \u2014 Li, J., Li, G., Zhang, X., Dong, Y., & Jin, Z. (2024). EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2301.09043",children:(0,a.jsx)(e.strong,{children:"37"})})," \u2014 Dong, Y., Ding, J., Jiang, X., Li, G., Li, Z., & Jin, Z. (2023). CodeScore: Evaluating Code Generation by Learning Code Execution (Version 4). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1145/3650105.3652295",children:(0,a.jsx)(e.strong,{children:"38"})})," \u2014 Niu, C., Zhang, T., Li, C., Luo, B., & Ng, V. (2024). On Evaluating the Efficiency of Source Code Generated by LLMs. In Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (pp. 103\u2013107). FORGE \u201924: 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering. ACM."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.3390/digital4010005",children:(0,a.jsx)(e.strong,{children:"39"})})," \u2014 Coello, C. E. A., Alimam, M. N., & Kouatly, R. (2024). Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models. Digital, 4(1), 114\u2013125."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.18653/v1/2024.emnlp-main.1118",children:(0,a.jsx)(e.strong,{children:"40"})})," \u2014 Tong, W., & Zhang, T. (2024). CodeJudge: Evaluating Code Generation with Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 20032\u201320051). Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2408.06450",children:(0,a.jsx)(e.strong,{children:"41"})})," \u2014 Liu, J., Xie, S., Wang, J., Wei, Y., Ding, Y., & Zhang, L. (2024). Evaluating Language Models for Efficient Code Generation (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2411.11908",children:(0,a.jsx)(e.strong,{children:"42"})})," \u2014 Nascimento, N., Guimaraes, E., Chintakunta, S. S., & Boominathan, S. A. (2024). LLM4DS: Evaluating Large Language Models for Data Science Code Generation (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.18653/v1/2021.emnlp-main.685",children:(0,a.jsx)(e.strong,{children:"43"})})," \u2014 Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 8696\u20138708). Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2207.11280",children:(0,a.jsx)(e.strong,{children:"44"})})," \u2014 Christopoulou, F., Lampouras, G., Gritta, M., Zhang, G., Guo, Y., Li, Z., Zhang, Q., Xiao, M., Shen, B., Li, L., Yu, H., Yan, L., Zhou, P., Wang, X., Ma, Y., Iacobacci, I., Wang, Y., Liang, G., Wei, J., \u2026 Liu, Q. (2022). PanGu-Coder: Program Synthesis with Function-Level Language Modeling (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2301.03988",children:(0,a.jsx)(e.strong,{children:"45"})})," \u2014 Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L. K., Anderson, C. J., Zi, Y., Poirier, J. L., Schoelkopf, H., Troshin, S., Abulkhanov, D., Romero, M., Lappert, M., \u2026 von Werra, L. (2023). SantaCoder: don\u2019t reach for the stars! (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2303.17568",children:(0,a.jsx)(e.strong,{children:"46"})})," \u2014 Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y., Su, T., Yang, Z., & Tang, J. (2023). CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X (Version 2). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2206.08474",children:(0,a.jsx)(e.strong,{children:"47"})})," \u2014 Zhu, M., Jain, A., Suresh, K., Ravindran, R., Tipirneni, S., & Reddy, C. K. (2022). XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s41019-025-00296-9",children:(0,a.jsx)(e.strong,{children:"48"})})," \u2014 Xu, W., Huang, C., Gao, S., & Shang, S. (2025). LLM-Based Agents for Tool Learning: A Survey. Data Science and Engineering."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s11704-024-40231-1",children:(0,a.jsx)(e.strong,{children:"49"})})," \u2014 Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., & Wen, J. (2024). A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s11432-023-4127-5",children:(0,a.jsx)(e.strong,{children:"50"})})," \u2014 Chen, X., Hu, X., Huang, Y., Jiang, H., Ji, W., Jiang, Y., Jiang, Y., Liu, B., Liu, H., Li, X., Lian, X., Meng, G., Peng, X., Sun, H., Shi, L., Wang, B., Wang, C., Wang, J., Wang, T., \u2026 Zhang, L. (2024). Deep learning-based software engineering: progress, challenges, and opportunities. Science China Information Sciences, 68(1)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s44443-025-00074-7",children:(0,a.jsx)(e.strong,{children:"51"})})," \u2014 Rong, Y., Du, T., Li, R., & Bao, W. (2025). Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education. Journal of King Saud University Computer and Information Sciences, 37(5)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s10515-024-00451-y",children:(0,a.jsx)(e.strong,{children:"52"})})," \u2014 Le, K. T., & Andrzejak, A. (2024). Rethinking AI code generation: a one-shot correction approach based on user feedback. Automated Software Engineering, 31(2)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s10462-024-10888-y",children:(0,a.jsx)(e.strong,{children:"53"})})," \u2014 Kumar, P. (2024). Large language models (LLMs): survey, technical frameworks, and future challenges. Artificial Intelligence Review, 57(10)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2509.09614",children:(0,a.jsx)(e.strong,{children:"54"})})," \u2014 Qiu, J., Liu, Z., Liu, Z., Murthy, R., Zhang, J., Chen, H., Wang, S., Zhu, M., Yang, L., Tan, J., Cen, Z., Qian, C., Heinecke, S., Yao, W., Savarese, S., Xiong, C., & Wang, H. (2025). LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2506.10833",children:(0,a.jsx)(e.strong,{children:"55"})})," \u2014 Pe\xf1a, F. C., & Herbold, S. (2025). Evaluating Large Language Models on Non-Code Software Engineering Tasks (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s43681-025-00721-9",children:(0,a.jsx)(e.strong,{children:"56"})})," \u2014 Afreen, J., Mohaghegh, M., & Doborjeh, M. (2025). Systematic literature review on bias mitigation in generative AI. AI and Ethics, 5(5), 4789\u20134841."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/ACCESS.2024.3482107",children:(0,a.jsx)(e.strong,{children:"57"})})," \u2014 Shao, M., Basit, A., Karri, R., & Shafique, M. (2024). Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges. IEEE Access, 12, 188664\u2013188706."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/ACCESS.2024.3403858",children:(0,a.jsx)(e.strong,{children:"58"})})," \u2014 S\xe1godi, Z., Siket, I., & Ferenc, R. (2024). Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot. IEEE Access, 12, 72303\u201372316."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/ACCESS.2024.3484947",children:(0,a.jsx)(e.strong,{children:"59"})})," \u2014 Black, G., Mathew Vaidyan, V., & Comert, G. (2024). Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation. IEEE Access, 12, 156065\u2013156081."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/ACCESS.2025.3553870",children:(0,a.jsx)(e.strong,{children:"60"})})," \u2014 Ko, E., & Kang, P. (2025). Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems. IEEE Access, 13, 52925\u201352938."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/ACCESS.2025.3601206",children:(0,a.jsx)(e.strong,{children:"61"})})," \u2014 Woesle, C., Fischer-Brandies, L., & Buettner, R. (2025). A Systematic Literature Review of Hallucinations in Large Language Models. IEEE Access, 13, 148231\u2013148253."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1016/j.csi.2024.103942",children:(0,a.jsx)(e.strong,{children:"62"})})," \u2014 Li, Y., Liu, P., Wang, H., Chu, J., & Wong, W. E. (2025). Evaluating large language models for software testing. Computer Standards & amp; Interfaces, 93, 103942."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1145/3597503.3639219",children:(0,a.jsx)(e.strong,{children:"63"})})," \u2014 Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., & Lou, Y. (2024). Evaluating Large Language Models in Class-Level Code Generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (pp. 1\u201313). ICSE \u201924: IEEE/ACM 46th International Conference on Software Engineering. ACM."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1109/TSE.2023.3334955",children:(0,a.jsx)(e.strong,{children:"64"})})," \u2014 Sch\xe4fer, M., Nadi, S., Eghbali, A., & Tip, F. (2024). An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. IEEE Transactions on Software Engineering, 50(1), 85\u2013105."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.1007/s10664-025-10687-1",children:(0,a.jsx)(e.strong,{children:"65"})})," \u2014 Alhanahnah, M., Rashedul Hasan, M., Xu, L., & Bagheri, H. (2025). An empirical evaluation of pre-trained large language models for repairing declarative formal specifications. Empirical Software Engineering, 30(5)."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2506.01793",children:(0,a.jsx)(e.strong,{children:"66"})})," \u2014 Guo, Y., Ji, K., Zhu, X., Wang, J., Wen, F., Li, C., Zhang, Z., & Zhai, G. (2025). Human-Centric Evaluation for Foundation Models (Version 1). arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2306.04675",children:(0,a.jsx)(e.strong,{children:"67"})})," \u2014 Stein, G., Cresswell, J. C., Hosseinzadeh, R., Sui, Y., Ross, B. L., Villecroze, V., Liu, Z., Caterini, A. L., Taylor, J. E. T., & Loaiza-Ganem, G. (2023). Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. arXiv."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://doi.org/10.48550/arXiv.2509.12395",children:(0,a.jsx)(e.strong,{children:"68"})})," \u2014 Mundhra, Y., Valk, M., & Izadi, M. (2025). Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML (Version 1). arXiv."]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>t});var r=i(6540);const a={},o=r.createContext(a);function s(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);