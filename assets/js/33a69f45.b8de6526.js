"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7868],{34:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"references","title":"References","description":"This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link.","source":"@site/docs/references.md","sourceDirName":".","slug":"/references","permalink":"/LLMs-metrics-catalog/references","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"references","title":"References"},"sidebar":"docsSidebar","previous":{"title":"Hallucination","permalink":"/LLMs-metrics-catalog/metrics/semantic/hallucination"}}');var s=r(4848),o=r(8453);const a={id:"references",title:"References"},t=void 0,l={},d=[];function h(e){const n={a:"a",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This page lists all source papers used in the LLMs Metrics Catalog. Click each ID to open the corresponding DOI link."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/SWC62898.2024.00231",children:(0,s.jsx)(n.strong,{children:"1"})})," \u2014 Overview of the Comprehensive Evaluation of Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2404.09135",children:(0,s.jsx)(n.strong,{children:"2"})})," \u2014 Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.mi.fu-berlin.de/w/SE/MArbeitReviewLLMs4SE",children:(0,s.jsx)(n.strong,{children:"3"})})," \u2014 Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2211.09110",children:(0,s.jsx)(n.strong,{children:"4"})})," \u2014 Holistic Evaluation of Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2310.19736",children:(0,s.jsx)(n.strong,{children:"5"})})," \u2014 Evaluating Large Language Models: A Comprehensive Survey"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2308.10620",children:(0,s.jsx)(n.strong,{children:"6"})})," \u2014 Large Language Models for Software Engineering: A Systematic Literature Review"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2505.08903",children:(0,s.jsx)(n.strong,{children:"7"})})," \u2014 Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2311.07397",children:(0,s.jsx)(n.strong,{children:"8"})})," \u2014 AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:(0,s.jsx)(n.strong,{children:"9"})})," \u2014 A Survey on Evaluation of Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2408.16498",children:(0,s.jsx)(n.strong,{children:"10"})})," \u2014 A Survey on Evaluating Large Language Models in Code Generation Tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2505.20854",children:(0,s.jsx)(n.strong,{children:"11"})})," \u2014 An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2304.14317",children:(0,s.jsx)(n.strong,{children:"12"})})," \u2014 ICE-Score: Instructing Large Language Models to Evaluate Code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2403.08604",children:(0,s.jsx)(n.strong,{children:"13"})})," \u2014 Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2310.06770",children:(0,s.jsx)(n.strong,{children:"14"})})," \u2014 SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2206.04615",children:(0,s.jsx)(n.strong,{children:"15"})})," \u2014 Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1162/coli_a_00524",children:(0,s.jsx)(n.strong,{children:"16"})})," \u2014 Bias and Fairness in Large Language Models: A Survey"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.56038/oprd.v4i1.444",children:(0,s.jsx)(n.strong,{children:"17"})})," \u2014 Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.56155/978-81-955020-9-7-24",children:(0,s.jsx)(n.strong,{children:"18"})})," \u2014 Analysis of LLM Code Synthesis in Software Productivity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2406.12655",children:(0,s.jsx)(n.strong,{children:"19"})})," \u2014 Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2305.01210",children:(0,s.jsx)(n.strong,{children:"20"})})," \u2014 Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language\r\nModels for Code Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.4218/etrij.2023-0357",children:(0,s.jsx)(n.strong,{children:"21"})})," \u2014 Framework for evaluating code generation ability of large language models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2401.0640",children:(0,s.jsx)(n.strong,{children:"22"})})," \u2014 DevEval: Evaluating Code Generation in Practical Software Projects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2212.10264",children:(0,s.jsx)(n.strong,{children:"23"})})," \u2014 ReCode: Robustness Evaluation of Code Generation Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2302.05527",children:(0,s.jsx)(n.strong,{children:"24"})})," \u2014 CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1016/j.jss.2023.111741",children:(0,s.jsx)(n.strong,{children:"25"})})," \u2014 Out of the BLEU: how should we assess quality of the Code Generation models?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.7544/issn1000-1239.202330715",children:(0,s.jsx)(n.strong,{children:"26"})})," \u2014 CodeScore-R: An Automated Robustness Metric for Assessing the Functional Correctness of Code\r\nSynthesis"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.18653/v1/2024.findings-emnlp.303",children:(0,s.jsx)(n.strong,{children:"27"})})," \u2014 CodeFort: Robust Training for Code Generation Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s42979-025-04241-5",children:(0,s.jsx)(n.strong,{children:"28"})})," \u2014 Usage of Large Language Model for Code Generation Tasks: A Review"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s10009-025-00798-x",children:(0,s.jsx)(n.strong,{children:"29"})})," \u2014 LLM-based code generation and system migration in language-driven engineering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s10710-024-09494-2",children:(0,s.jsx)(n.strong,{children:"30"})})," \u2014 Evolving code with a large language model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2107.03374",children:(0,s.jsx)(n.strong,{children:"31"})})," \u2014 Evaluating Large Language Models Trained on Code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2102.04664",children:(0,s.jsx)(n.strong,{children:"32"})})," \u2014 CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2410.12381",children:(0,s.jsx)(n.strong,{children:"33"})})," \u2014 HumanEval: A Benchmark for Evaluating LLMs on Code Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2108.07732",children:(0,s.jsx)(n.strong,{children:"34"})})," \u2014 Program Synthesis with Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2208.08227",children:(0,s.jsx)(n.strong,{children:"35"})})," \u2014 MultiPL-E: A Scalable Benchmark for Code Generation Across 18 Languages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2404.00599",children:(0,s.jsx)(n.strong,{children:"36"})})," \u2014 EvoCodeBench: An Evolving Benchmark for Code Generation Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2301.09043",children:(0,s.jsx)(n.strong,{children:"37"})})," \u2014 CodeScore: Evaluating Code Generation by Learning Code Execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1145/3650105.3652295",children:(0,s.jsx)(n.strong,{children:"38"})})," \u2014 On Evaluating the Efficiency of Source Code Generated by LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.3390/digital4010005",children:(0,s.jsx)(n.strong,{children:"39"})})," \u2014 Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.18653/v1/2024.emnlp-main.1118",children:(0,s.jsx)(n.strong,{children:"40"})})," \u2014 CodeJudge: Evaluating Code Generation with Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2408.06450",children:(0,s.jsx)(n.strong,{children:"41"})})," \u2014 Evaluating Language Models for Efficient Code Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2411.11908",children:(0,s.jsx)(n.strong,{children:"42"})})," \u2014 LLM4DS: Evaluating Large Language Models for Data Science Code Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.18653/v1/2021.emnlp-main.685",children:(0,s.jsx)(n.strong,{children:"43"})})," \u2014 CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code\r\nUnderstanding and Generatio"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2207.11280",children:(0,s.jsx)(n.strong,{children:"44"})})," \u2014 PanGu-Coder: Program Synthesis with Function-Level Language Modelin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2301.03988",children:(0,s.jsx)(n.strong,{children:"45"})})," \u2014 SantaCoder: Don\u2019t Reach for the Stars! Benchmarking Large Language Models for Code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2303.17568",children:(0,s.jsx)(n.strong,{children:"46"})})," \u2014 CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2206.08474",children:(0,s.jsx)(n.strong,{children:"47"})})," \u2014 XLCoST: A Benchmark for Cross-Lingual Code Intelligence"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s41019-025-00296-9",children:(0,s.jsx)(n.strong,{children:"48"})})," \u2014 LLM-Based Agents for Tool Learning: A Survey"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s11704-024-40231-1",children:(0,s.jsx)(n.strong,{children:"49"})})," \u2014 A survey on large language model based autonomous agents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s11432-023-4127-5",children:(0,s.jsx)(n.strong,{children:"50"})})," \u2014 Deep learning-based software engineering: progress, challenges, and opportunities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s44443-025-00074-7",children:(0,s.jsx)(n.strong,{children:"51"})})," \u2014 Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s10515-024-00451-y",children:(0,s.jsx)(n.strong,{children:"52"})})," \u2014 Rethinking AI code generation: a one-shot correction approach based on user feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s10462-024-10888-y",children:(0,s.jsx)(n.strong,{children:"53"})})," \u2014 Large language models (LLMs): survey, technical frameworks, and future challenges"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2509.09614",children:(0,s.jsx)(n.strong,{children:"54"})})," \u2014 LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2506.10833",children:(0,s.jsx)(n.strong,{children:"55"})})," \u2014 Evaluating Large Language Models on Non-Code Software Engineering Tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s43681-025-00721-9",children:(0,s.jsx)(n.strong,{children:"56"})})," \u2014 Systematic literature review on bias mitigation in generative AI"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ACCESS.2024.3482107",children:(0,s.jsx)(n.strong,{children:"57"})})," \u2014 Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ACCESS.2024.3403858",children:(0,s.jsx)(n.strong,{children:"58"})})," \u2014 Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ACCESS.2024.3484947",children:(0,s.jsx)(n.strong,{children:"59"})})," \u2014 Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ACCESS.2025.3553870",children:(0,s.jsx)(n.strong,{children:"60"})})," \u2014 Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/ACCESS.2025.3601206",children:(0,s.jsx)(n.strong,{children:"61"})})," \u2014 A Systematic Literature Review of Hallucinations in Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1016/j.csi.2024.103942",children:(0,s.jsx)(n.strong,{children:"62"})})," \u2014 Evaluating large language models for software testing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1145/3597503.3639219",children:(0,s.jsx)(n.strong,{children:"63"})})," \u2014 Evaluating Large Language Models in Class-Level Code Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1109/TSE.2023.3334955",children:(0,s.jsx)(n.strong,{children:"64"})})," \u2014 An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s10664-025-10687-1",children:(0,s.jsx)(n.strong,{children:"65"})})," \u2014 An empirical evaluation of pre-trained large language models for repairing declarative formal specifications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2506.01793",children:(0,s.jsx)(n.strong,{children:"66"})})," \u2014 Human-Centric Evaluation for Foundation Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2306.04675",children:(0,s.jsx)(n.strong,{children:"67"})})," \u2014 Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2509.12395",children:(0,s.jsx)(n.strong,{children:"68"})})," \u2014 Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML"]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>t});var i=r(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);