"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[6806],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9746:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"metrics/efficiency/inference","title":"Inference (Efficiency and Verification)","description":"Introduction","source":"@site/docs/metrics/efficiency/inference.md","sourceDirName":"metrics/efficiency","slug":"/metrics/efficiency/inference","permalink":"/LLMs-metrics-catalog/metrics/efficiency/inference","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"inference","title":"Inference (Efficiency and Verification)","sidebar_label":"Inference"},"sidebar":"docsSidebar","previous":{"title":"Energy & Resource","permalink":"/LLMs-metrics-catalog/metrics/efficiency/energy-metrics"},"next":{"title":"Verbosity","permalink":"/LLMs-metrics-catalog/metrics/efficiency/verbosity"}}');var r=i(4848),s=i(8453);const a={id:"inference",title:"Inference (Efficiency and Verification)",sidebar_label:"Inference"},o=void 0,c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"1. Inference Efficiency (Runtime / Speed)",id:"1-inference-efficiency-runtime--speed",level:2},{value:"Definition",id:"definition",level:3},{value:"Formula (HELM Idealized Runtime)",id:"formula-helm-idealized-runtime",level:3},{value:"Purpose",id:"purpose",level:3},{value:"Applications",id:"applications",level:3},{value:"Limitations",id:"limitations",level:3},{value:"2. Behavioral Model Inference (Success Rate)",id:"2-behavioral-model-inference-success-rate",level:2},{value:"Definition",id:"definition-1",level:3},{value:"Purpose",id:"purpose-1",level:3},{value:"Applications",id:"applications-1",level:3},{value:"3. Comparative Summary",id:"3-comparative-summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",annotation:"annotation",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:'In the evaluation of Large Language Models (LLMs) and the systems they generate, "Inference" is a term used to describe metrics in two distinct categories:'}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference Efficiency:"}),' This is the most common usage. It refers to the performance of a model during the generation (or "inference") phase, focusing on speed, resource consumption, and runtime. Metrics include ',(0,r.jsx)(n.strong,{children:"Inference Runtime"})," and ",(0,r.jsx)(n.strong,{children:"Speed of Inference"})," (Sources 4, 49)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Behavioral Model Inference:"})," This is a specialized verification metric used in Software Engineering. It refers to the success rate of ",(0,r.jsx)(n.em,{children:"inferring"})," (i.e., automatically learning) a correct behavioral model, such as an automaton, from a system's outputs to validate its functional correctness (Source 29)."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This document covers both variants based on the provided contexts."}),"\n",(0,r.jsx)(n.h2,{id:"1-inference-efficiency-runtime--speed",children:"1. Inference Efficiency (Runtime / Speed)"}),"\n",(0,r.jsx)(n.h3,{id:"definition",children:"Definition"}),"\n",(0,r.jsxs)(n.p,{children:["Inference Efficiency metrics measure the computational cost and speed of a language model when it is performing its primary function: generating text (making predictions). This is distinct from ",(0,r.jsx)(n.em,{children:"Training Efficiency"}),", which measures the resources used to create the model (Source 4). These metrics are crucial for assessing the practical usability and deployment cost of a model."]}),"\n",(0,r.jsx)(n.p,{children:"HELM (Holistic Evaluation of Language Models) proposes several specific metrics for this (Source 4):"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Raw Runtime:"})," The actual measured wall-clock time for a request, which includes computational time plus noise from network latency or queuing (contention)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Denoised Inference Runtime:"})," The estimated runtime using the model provider's specific hardware and software, but with the noise from contention (queuing) factored out."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Idealized Inference Runtime:"})," A standardized metric estimating the runtime on a uniform, optimized hardware and software stack (e.g., NVIDIA A100 GPUs) to allow for fair comparisons ",(0,r.jsx)(n.em,{children:"between"})," different models (Source 4)."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"formula-helm-idealized-runtime",children:"Formula (HELM Idealized Runtime)"}),"\n",(0,r.jsx)(n.p,{children:"The total time for an idealized runtime can be modeled as a function of prompt processing time and token generation time (Source 4):"}),"\n",(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsxs)(n.mrow,{children:[(0,r.jsx)(n.mtext,{children:"Total\xa0time"}),(0,r.jsx)(n.mo,{children:"="}),(0,r.jsx)(n.mi,{children:"F"}),(0,r.jsx)(n.mo,{stretchy:"false",children:"("}),(0,r.jsx)(n.mtext,{children:"num_prompt_tokens"}),(0,r.jsx)(n.mo,{stretchy:"false",children:")"}),(0,r.jsx)(n.mo,{children:"+"}),(0,r.jsx)(n.mi,{children:"g"}),(0,r.jsx)(n.mo,{children:"\u22c5"}),(0,r.jsx)(n.mtext,{children:"num_output_tokens"})]}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\text{Total time} = F(\\text{num\\_prompt\\_tokens}) + g \\cdot \\text{num\\_output\\_tokens}"})]})})}),"\n",(0,r.jsx)(n.p,{children:"where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsx)(n.mi,{children:"F"})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"F"})]})})})," = Function modeling the runtime of processing the prompt."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsx)(n.mi,{children:"g"})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"g"})]})})})," = Runtime (cost) of generating each additional output token."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,r.jsx)(n.p,{children:"The purpose is to measure the practical performance of an LLM. High inference efficiency (low runtime, high speed) is critical for user-facing applications (like chatbots), mobile applications, and autonomous agents that require fast responses (Sources 4, 49)."}),"\n",(0,r.jsx)(n.h3,{id:"applications",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"LLM Evaluation (General) (Source 4)"}),"\n",(0,r.jsx)(n.li,{children:"Autonomous Agents (Source 49)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Comparing inference efficiency fairly is difficult, as performance depends heavily on non-model factors (Source 4)."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware and Software:"})," The type and number of accelerators (GPUs/TPUs) and software optimizations (e.g., Megatron) drastically affect speed (Source 4)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contention:"})," For models accessed via API, performance can vary based on server load and queuing, which is why ",(0,r.jsx)(n.em,{children:"Denoised Runtime"})," is proposed (Source 4)."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2-behavioral-model-inference-success-rate",children:"2. Behavioral Model Inference (Success Rate)"}),"\n",(0,r.jsx)(n.h3,{id:"definition-1",children:"Definition"}),"\n",(0,r.jsxs)(n.p,{children:["This metric, specific to Learning-Based Testing and Language-Driven Engineering (LDE), does ",(0,r.jsx)(n.em,{children:"not"})," measure computational speed. Instead, it measures the ",(0,r.jsx)(n.strong,{children:"success rate (%)"})," of automatically ",(0,r.jsx)(n.em,{children:"inferring"})," a correct behavioral model (e.g., a Mealy machine or automaton) from a generated application (Source 29)."]}),"\n",(0,r.jsxs)(n.p,{children:['In this context (Source 29), an LLM is used for system migration (e.g., JavaScript to TypeScript). The "learnability-by-design" approach then uses a tool (like Malwa) to test the resulting application and ',(0,r.jsx)(n.em,{children:"infer"})," its behavior as a formal model. The success rate reflects whether this inferred model is correct, thereby validating the LLM's migration task (Source 29)."]}),"\n",(0,r.jsx)(n.h3,{id:"purpose-1",children:"Purpose"}),"\n",(0,r.jsx)(n.p,{children:"The purpose is to automatically validate the functional correctness and reliability of LLM-generated code or system migrations. It verifies that the generated system behaves as intended, or identifies behavioral differences (using difference automata) between the original and migrated systems (Source 29)."}),"\n",(0,r.jsx)(n.h3,{id:"applications-1",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Software Engineering (Verification / Reliability) (Source 29)"}),"\n",(0,r.jsx)(n.li,{children:"System Migration (Source 29)"}),"\n",(0,r.jsx)(n.li,{children:"Language-Driven Engineering (LDE) (Source 29)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-comparative-summary",children:"3. Comparative Summary"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metric"}),(0,r.jsx)(n.th,{children:"Category"}),(0,r.jsx)(n.th,{children:"Measures"}),(0,r.jsx)(n.th,{children:"Typical Domain"}),(0,r.jsx)(n.th,{children:"Key Sources"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Inference Efficiency (Runtime/Speed)"})}),(0,r.jsx)(n.td,{children:"Efficiency"}),(0,r.jsx)(n.td,{children:"Computational cost, time (s), or tokens/sec during text generation."}),(0,r.jsx)(n.td,{children:"LLM Evaluation, Autonomous Agents"}),(0,r.jsx)(n.td,{children:"4, 49"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Behavioral Model Inference Rate"})}),(0,r.jsx)(n.td,{children:"Verification / Reliability"}),(0,r.jsxs)(n.td,{children:["Success rate (%) of automatically ",(0,r.jsx)(n.em,{children:"deriving"})," a correct behavioral automaton from a system."]}),(0,r.jsx)(n.td,{children:"Software Engineering, LDE"}),(0,r.jsx)(n.td,{children:"29"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Busch, D., Bainczyk, A., & Smyth, S. (2025). LLM-based code generation and system migration in language-driven engineering. International Journal on Software Tools for Technology Transfer, 27, 137\u2013147. ",(0,r.jsx)(n.a,{href:"https://doi.org/10.1007/s10009-025-00798-x",children:"https://doi.org/10.1007/s10009-025-00798-x"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., R\xe9, C., Acosta-Navas, D., Hudson, D. A., ... Koreeda, Y. (2022). Holistic evaluation of language models (HELM). arXiv. ",(0,r.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2211.09110",children:"https://doi.org/10.48550/arXiv.2211.09110"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., & Wen, J.-R. (2023). A survey on large language model based autonomous agents. arXiv. ",(0,r.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2308.11432",children:"https://doi.org/10.48550/arXiv.2308.11432"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);