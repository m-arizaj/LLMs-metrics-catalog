"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1933],{5502:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"metrics/semantic/gpt-metrics","title":"GPT-based Metrics (LLM-as-a-Judge)","description":"Introduction","source":"@site/docs/metrics/semantic/gpt-metrics.md","sourceDirName":"metrics/semantic","slug":"/metrics/semantic/gpt-metrics","permalink":"/LLMs-metrics-catalog/metrics/semantic/gpt-metrics","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"gpt-metrics","title":"GPT-based Metrics (LLM-as-a-Judge)","sidebar_label":"GPT-based Metrics"},"sidebar":"docsSidebar","previous":{"title":"Faithfulness","permalink":"/LLMs-metrics-catalog/metrics/semantic/faithfulness"},"next":{"title":"Coherence","permalink":"/LLMs-metrics-catalog/metrics/semantic/coherence"}}');var a=n(4848),s=n(8453);const r={id:"gpt-metrics",title:"GPT-based Metrics (LLM-as-a-Judge)",sidebar_label:"GPT-based Metrics"},l=void 0,o={},d=[{value:"Introduction",id:"introduction",level:2},{value:"1. GPTScore",id:"1-gptscore",level:2},{value:"Definition",id:"definition",level:3},{value:"Purpose",id:"purpose",level:3},{value:"Applications",id:"applications",level:3},{value:"Advantages",id:"advantages",level:3},{value:"2. GPT-4V Overall Rating",id:"2-gpt-4v-overall-rating",level:2},{value:"Definition",id:"definition-1",level:3},{value:"Purpose",id:"purpose-1",level:3},{value:"Applications",id:"applications-1",level:3},{value:"3. Comparative Summary",id:"3-comparative-summary",level:2},{value:"References",id:"references",level:2}];function c(e){const t={a:"a",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(t.p,{children:["A recent paradigm in evaluating Large Language Models (LLMs) involves using an LLM itself as the evaluator. This approach, often called ",(0,a.jsx)(t.strong,{children:'"LLM-as-a-judge,"'})," leverages the advanced reasoning and language understanding capabilities of models like GPT to assess the quality of generated text or code."]}),"\n",(0,a.jsxs)(t.p,{children:["These metrics work by crafting a specific prompt that instructs a powerful model (e.g., GPT-4) to compare a candidate output against a reference or a set of criteria and provide a score. This method has gained prominence because its results often show a ",(0,a.jsx)(t.strong,{children:"higher correlation with human judgments"})," than traditional metrics."]}),"\n",(0,a.jsxs)(t.p,{children:["Two prominent examples of this approach are ",(0,a.jsx)(t.strong,{children:"GPTScore"}),", for general text evaluation, and ",(0,a.jsx)(t.strong,{children:"GPT-4V Overall Rating"}),", for multimodal code evaluation."]}),"\n",(0,a.jsx)(t.h2,{id:"1-gptscore",children:"1. GPTScore"}),"\n",(0,a.jsx)(t.h3,{id:"definition",children:"Definition"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"GPTScore"}),' is an automatic evaluation method that uses an LLM (like GPT-3 or GPT-4) to score the quality of generated text. It operates by providing the "judge" LLM with the context, a reference text, and the candidate (model-generated) text. It then prompts the LLM to provide both a ',(0,a.jsx)(t.strong,{children:'"final score"'})," (e.g., on a 1-5 scale) and a ",(0,a.jsx)(t.strong,{children:'"detailed evaluation"'})," (a textual explanation) for its assessment."]}),"\n",(0,a.jsx)(t.h3,{id:"purpose",children:"Purpose"}),"\n",(0,a.jsx)(t.p,{children:"To provide a new automatic evaluation metric for text generation that achieves a higher correlation with human judgments than previous metrics like BLEU or ROUGE."}),"\n",(0,a.jsx)(t.h3,{id:"applications",children:"Applications"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"LLM Evaluation / NLP"}),"\n",(0,a.jsx)(t.li,{children:"General Text Generation"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"High Human Correlation:"})," Has been shown to correlate more strongly with human evaluation."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Explainable:"})," Unlike traditional metrics that output only a number, GPTScore also generates a detailed textual explanation for its score."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"2-gpt-4v-overall-rating",children:"2. GPT-4V Overall Rating"}),"\n",(0,a.jsx)(t.h3,{id:"definition-1",children:"Definition"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.strong,{children:"GPT-4V Overall Rating"})," is a metric used within the ",(0,a.jsx)(t.strong,{children:"Plot2Code"})," benchmark to evaluate multimodal code generation (i.e., generating code from an image of a plot). It specifically uses the GPT-4V (Vision) model to analyze the generated code alongside the source plot image and assign an ",(0,a.jsx)(t.strong,{children:'"overall quality rating... on a scale of 1-5"'}),"."]}),"\n",(0,a.jsx)(t.h3,{id:"purpose-1",children:"Purpose"}),"\n",(0,a.jsx)(t.p,{children:'To serve as a "key metric for assessing the overall quality" of generated code in a complex, multimodal, image-to-code task. It is used as a scalable, automated alternative to expensive human evaluation for this specific domain.'}),"\n",(0,a.jsx)(t.h3,{id:"applications-1",children:"Applications"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Software Engineering / Code Generation"}),"\n",(0,a.jsx)(t.li,{children:"Human Evaluation / Multimodal Quality"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"3-comparative-summary",children:"3. Comparative Summary"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{style:{textAlign:"left"},children:"Metric"}),(0,a.jsx)(t.th,{style:{textAlign:"left"},children:"Base Model"}),(0,a.jsx)(t.th,{style:{textAlign:"left"},children:"Input Modality"}),(0,a.jsx)(t.th,{style:{textAlign:"left"},children:"Task Domain"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{style:{textAlign:"left"},children:(0,a.jsx)(t.strong,{children:"GPTScore"})}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"LLM (e.g., GPT-3, GPT-4)"}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Text-only (Context, Reference, Candidate)"}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"General Text Generation (NLP)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{style:{textAlign:"left"},children:(0,a.jsx)(t.strong,{children:"GPT-4V Overall Rating"})}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Multimodal LLM (GPT-4V)"}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Multimodal (Image + Code)"}),(0,a.jsx)(t.td,{style:{textAlign:"left"},children:"Code Generation from Plots"})]})]})]}),"\n",(0,a.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["Lin, L., Zhu, D., & Shang, J. (2024, December 2-7). Overview of the comprehensive evaluation of large language models [Presentaci\xf3n de art\xedculo]. 2024 IEEE Smart World Congress (SWC), Nadi, Fiji. ",(0,a.jsx)(t.a,{href:"https://doi.org/10.1109/SWC62898.2024.00231",children:"https://doi.org/10.1109/SWC62898.2024.00231"})]}),"\n",(0,a.jsxs)(t.li,{children:["Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., Wu, J., Wang, Y., Gao, Q., Wang, J., Ye, W., & Zhang, S. (2025). A survey on evaluating large language models in code generation tasks (arXiv:2408.16498v2). arXiv. ",(0,a.jsx)(t.a,{href:"https://doi.org/10.48550/arXiv.2408.16498",children:"https://doi.org/10.48550/arXiv.2408.16498"})]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>l});var i=n(6540);const a={},s=i.createContext(a);function r(e){const t=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);