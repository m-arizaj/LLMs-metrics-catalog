"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[8665],{280:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>c,contentTitle:()=>l,default:()=>o,frontMatter:()=>a,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"metrics/semantic/chair","title":"CHAIR","description":"Introduction","source":"@site/docs/metrics/semantic/chair.md","sourceDirName":"metrics/semantic","slug":"/metrics/semantic/chair","permalink":"/LLMs-metrics-catalog/metrics/semantic/chair","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"chair","title":"CHAIR","sidebar_label":"CHAIR"},"sidebar":"docsSidebar","previous":{"title":"Coherence","permalink":"/LLMs-metrics-catalog/metrics/semantic/coherence"},"next":{"title":"Semantic","permalink":"/LLMs-metrics-catalog/metrics/semantic/"}}');var r=e(4848),t=e(8453);const a={id:"chair",title:"CHAIR",sidebar_label:"CHAIR"},l=void 0,c={},h=[{value:"Introduction",id:"introduction",level:2},{value:"Formula",id:"formula",level:2},{value:"AMBER Integration (2024 Update)",id:"amber-integration-2024-update",level:2},{value:"Variants and Dimensions",id:"variants-and-dimensions",level:2},{value:"Interpretation",id:"interpretation",level:2},{value:"References",id:"references",level:2}];function d(n){const i={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",msubsup:"msubsup",msup:"msup",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(i.p,{children:["CHAIR is a visual-textual consistency metric designed to quantify object hallucination, cases where a model mentions objects that are not present in an image. Introduced by Rohrbach et al. (2018) in Object Hallucination in Image Captioning (EMNLP 2018), it evaluates the alignment between generated captions and ground-truth visual object annotations, thus measuring how faithful the description is to the image content.\r\nUnlike traditional captioning metrics such as CIDEr or SPICE, which only assess textual similarity to reference captions, CHAIR measures the semantic accuracy of image grounding.",(0,r.jsx)(i.br,{}),"\n","In 2024, the AMBER benchmark (Wang et al., 2024) extended CHAIR as part of a multi-dimensional hallucination evaluation suite for Multimodal Large Language Models (MLLMs), integrating it into the AMBER Score for combined generative and discriminative assessment."]}),"\n",(0,r.jsx)(i.h2,{id:"formula",children:"Formula"}),"\n",(0,r.jsx)(i.p,{children:"CHAIR has two primary variants:"}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"CHAIR_i = \\frac{|H|}{|M|}"})]})})}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"s"})]}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{children:"H"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"CHAIR_s = \\frac{|S_H|}{|S|}"})]})})}),"\n",(0,r.jsx)(i.p,{children:"Where:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"|H|"})]})})})," = number of hallucinated objects (mentioned but not present)."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"|M|"})]})})})," = total number of objects mentioned in the caption."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{children:"H"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"|S_H|"})]})})})," = number of sentences containing at least one hallucinated object."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"|S|"})]})})})," = total number of generated sentences."]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["A lower CHAIR value indicates better grounding \u2014 fewer hallucinated mentions.",(0,r.jsx)(i.br,{}),"\n","In practice, object presence is validated against ground-truth labels from datasets such as MSCOCO or AMBER\u2019s annotated images."]}),"\n",(0,r.jsx)(i.h2,{id:"amber-integration-2024-update",children:"AMBER Integration (2024 Update)"}),"\n",(0,r.jsx)(i.p,{children:"Within AMBER, CHAIR is computed on generated MLLM responses as follows (Wang et al., 2024):"}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"l"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msup,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"b"}),(0,r.jsx)(i.mi,{children:"j"})]}),(0,r.jsx)(i.mo,{children:"\u2229"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"b"}),(0,r.jsx)(i.mi,{children:"j"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"l"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msubsup,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"b"}),(0,r.jsx)(i.mi,{children:"j"})]}),(0,r.jsx)(i.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"CHAIR(R) = 1 - \\frac{len(R'{obj} \\cap A{obj})}{len(R'_{obj})}"})]})})}),"\n",(0,r.jsx)(i.p,{children:"Where:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsxs)(i.msubsup,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"b"}),(0,r.jsx)(i.mi,{children:"j"})]}),(0,r.jsx)(i.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"R'_{obj}"})]})})})," = set of object nouns extracted from model responses."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"b"}),(0,r.jsx)(i.mi,{children:"j"})]})]})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"A_{obj}"})]})})})," = annotated ground-truth object set for the image."]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["CHAIR forms part of the AMBER Score, a composite evaluation integrating CHAIR for generative hallucination and ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"F"}),(0,r.jsx)(i.mn,{children:"1"})]})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"F_1"})]})})})," for discriminative performance:"]}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"E"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mtext,{children:"\u2009"}),(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mn,{children:"2"})]}),(0,r.jsx)(i.mtext,{children:"\u2009"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mo,{children:"+"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"F"}),(0,r.jsx)(i.mn,{children:"1"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"AMBER\\,Score = \\frac{1}{2}\\,(1 - CHAIR + F_1)"})]})})}),"\n",(0,r.jsx)(i.p,{children:"This unified formulation rewards models that both minimize hallucinations and maintain high discriminative accuracy."}),"\n",(0,r.jsx)(i.h2,{id:"variants-and-dimensions",children:"Variants and Dimensions"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"Variant"}),(0,r.jsx)(i.th,{children:"Description"}),(0,r.jsx)(i.th,{children:"Context of Use"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mo,{children:"\u2217"}),(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"_"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"\u2217"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"*CHAIR\\_i*"})]})})})}),(0,r.jsx)(i.td,{children:"Instance-level hallucination rate \u2014 ratio of false object mentions to all mentioned objects."}),(0,r.jsx)(i.td,{children:"Image Captioning (MSCOCO, Flickr30k)"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mo,{children:"\u2217"}),(0,r.jsx)(i.mi,{children:"C"}),(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mi,{children:"A"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"_"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mo,{children:"\u2217"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"*CHAIR\\_s*"})]})})})}),(0,r.jsx)(i.td,{children:"Sentence-level hallucination rate \u2014 fraction of captions with any hallucinated object."}),(0,r.jsx)(i.td,{children:"Captioning Quality Evaluation"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.em,{children:"AMBER\u2013CHAIR"})}),(0,r.jsx)(i.td,{children:"Modernized variant incorporated in AMBER; measures visual hallucination across both generative and discriminative tasks."}),(0,r.jsx)(i.td,{children:"MLLMs (GPT-4V, LLaVA, InstructBLIP, etc.)"})]})]})]}),"\n",(0,r.jsx)(i.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Low CHAIR"})," \u2192 Captions accurately describe visible content (faithful grounding)."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"High CHAIR"})," \u2192 Model hallucinates non-existent objects or attributes."]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"CHAIR provides insights into hallucination origin:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Visual misclassification \u2014 misinterpreting visual input."}),"\n",(0,r.jsx)(i.li,{children:"Language prior bias \u2014 predicting words based on co-occurrence rather than visual cues."}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["Rohrbach et al. (2018) also introduced image consistency and language consistency analyses, showing that models with higher hallucination rates produce predictions more aligned with their language model than with visual evidence.",(0,r.jsx)(i.br,{}),"\n","The metric was later generalized in AMBER to quantify hallucinations of:"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Existence"})," (missing/spurious objects)"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Attribute"})," (incorrect states, numbers, or actions)"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Relation"})," (incorrect spatial or interaction relations)"]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["CHAIR quantifies faithfulness, not fluency.",(0,r.jsx)(i.br,{}),"\n","It serves as a critical complement to text-based metrics (CIDEr, SPICE, METEOR), allowing researchers to evaluate visual hallucinations and semantic grounding simultaneously.",(0,r.jsx)(i.br,{}),"\n","Its integration into AMBER (2024) demonstrates how hallucination metrics evolve from image captioning to multi-modal reasoning for LLMs."]}),"\n",(0,r.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T., & Saenko, K. (2018)."}),(0,r.jsx)(i.br,{}),"\n","Object Hallucination in Image Captioning.",(0,r.jsx)(i.br,{}),"\n","Proceedings of EMNLP 2018.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://aclanthology.org/D18-1437",children:"https://aclanthology.org/D18-1437"})]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Wang, J., Wang, Y., Xu, G., Zhang, J., Gu, Y., Jia, H., et al. (2024)."}),(0,r.jsx)(i.br,{}),"\n","AMBER: An LLM-Free Multi-Dimensional Benchmark for MLLM Hallucination Evaluation.",(0,r.jsx)(i.br,{}),"\n","arXiv:2311.07397 [cs.CL].",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://arxiv.org/abs/2311.07397",children:"https://arxiv.org/abs/2311.07397"})]}),"\n"]}),"\n"]})]})}function o(n={}){const{wrapper:i}={...(0,t.R)(),...n.components};return i?(0,r.jsx)(i,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>a,x:()=>l});var s=e(6540);const r={},t=s.createContext(r);function a(n){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function l(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),s.createElement(t.Provider,{value:i},n.children)}}}]);