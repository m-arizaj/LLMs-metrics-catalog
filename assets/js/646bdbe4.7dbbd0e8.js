"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1988],{7070:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"metrics/human/helpfulness","title":"Helpfulness","description":"Definition","source":"@site/docs/metrics/human/helpfulness.md","sourceDirName":"metrics/human","slug":"/metrics/human/helpfulness","permalink":"/LLMs-metrics-catalog/metrics/human/helpfulness","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"helpfulness","title":"Helpfulness","sidebar_label":"Helpfulness"},"sidebar":"docsSidebar","previous":{"title":"Harmlessness","permalink":"/LLMs-metrics-catalog/metrics/human/harmlessness"},"next":{"title":"Honesty","permalink":"/LLMs-metrics-catalog/metrics/human/honesty"}}');var t=n(4848),r=n(8453);const a={id:"helpfulness",title:"Helpfulness",sidebar_label:"Helpfulness"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"How It Is Measured",id:"how-it-is-measured",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function h(e){const s={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Helpfulness"})," is a key category of ",(0,t.jsx)(s.strong,{children:"human-centered evaluation"})," for Large Language Models (LLMs) . It is a qualitative assessment that measures how well an LLM's response ",(0,t.jsx)(s.strong,{children:"satisfies a user's request, answers their question, or accomplishes the instructed task"}),"."]}),"\n",(0,t.jsx)(s.p,{children:'It is the primary metric for evaluating a model\'s utility and capability, often balanced against "Harmlessness" to ensure that responses are not only safe but also functional and useful.'}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"how-it-is-measured",children:"How It Is Measured"}),"\n",(0,t.jsxs)(s.p,{children:["Helpfulness is measured through ",(0,t.jsx)(s.strong,{children:"Human Evaluation"}),', often in a "battle" or pairwise comparison format. The most common benchmarks for this are:']}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Chatbot Arena:"}),' Human users interact with two different anonymous models, ask them the same question, and then vote for which model provided the "better" (i.e., more helpful and thorough) response.']}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"HELM (Holistic Evaluation of Language Models):"})," Uses human evaluations to score model outputs on various criteria, including accuracy and helpfulness for specific tasks."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"In these evaluations, human raters typically score responses based on criteria like relevance, accuracy, completeness, and overall quality in addressing the prompt."}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsxs)(s.p,{children:["The purpose of measuring helpfulness is to ",(0,t.jsx)(s.strong,{children:"quantify the utility and performance of an LLM from a user's perspective"}),". It is the main metric for determining if a model is good at following instructions and providing high-quality, relevant answers, which is the primary goal of assistant-style models."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Human-centered Evaluation"}),"\n",(0,t.jsx)(s.li,{children:"LLM Evaluation / NLP"}),"\n",(0,t.jsx)(s.li,{children:"Chatbot Performance"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"HELM (Holistic Evaluation of Language Models)"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"Chatbot Arena"})}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"MT-Bench"})," (often used in conjunction with Chatbot Arena)"]}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Aligns with User Goals:"})," Directly measures what most users care about\u2014getting a good answer."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Captures Nuance:"})," Human evaluators can reward responses that are not just factually correct but also well-written, comprehensive, and appropriately toned."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:'"Gold Standard" for Utility:'}),' It serves as the most reliable "ground truth" for a model\'s practical capability, often revealing gaps that automated metrics miss.']}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Subjectivity:"}),' What one user finds "helpful" another might find overly verbose or simplistic. The metric is inherently subjective.']}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Scalability:"})," It is very ",(0,t.jsx)(s.strong,{children:"expensive and slow"})," to collect human judgments compared to running automated, computational metrics."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Evaluator Bias:"})," The ratings can be influenced by the knowledge, preferences, and potential biases of the human evaluators."]}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Chang, Y., Wang, X., Wang, J., et al. (2023). ",(0,t.jsx)(s.em,{children:"A Survey on Evaluation of Large Language Models"}),". ",(0,t.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:"https://doi.org/10.48550/arXiv.2307.03109"})]}),"\n",(0,t.jsxs)(s.li,{children:["Zheng, L., Chiang, W. L., Sheng, Y., et al. (2023). ",(0,t.jsx)(s.em,{children:"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}),"."]}),"\n",(0,t.jsxs)(s.li,{children:["Liang, P., Bommasani, R., Lee, T., et al. (2022). ",(0,t.jsx)(s.em,{children:"Holistic evaluation of language models"}),"."]}),"\n",(0,t.jsx)(s.li,{children:"(Excel Data: Paper 9)"}),"\n"]})]})}function d(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>a,x:()=>l});var i=n(6540);const t={},r=i.createContext(t);function a(e){const s=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:s},e.children)}}}]);