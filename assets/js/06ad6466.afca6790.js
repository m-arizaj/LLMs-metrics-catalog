"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[5696],{6845:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"metrics/semantic/semantic-metrics","title":"Semantic Metrics","description":"Introduction","source":"@site/docs/metrics/semantic/semantic.md","sourceDirName":"metrics/semantic","slug":"/metrics/semantic/","permalink":"/LLMs-metrics-catalog/metrics/semantic/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"semantic-metrics","title":"Semantic Metrics","sidebar_label":"Semantic"},"sidebar":"docsSidebar","previous":{"title":"CHAIR","permalink":"/LLMs-metrics-catalog/metrics/semantic/chair"},"next":{"title":"Sentiment Polarity","permalink":"/LLMs-metrics-catalog/metrics/semantic/sentiment-polarity-shift"}}');var s=n(4848),r=n(8453);const a={id:"semantic-metrics",title:"Semantic Metrics",sidebar_label:"Semantic"},l=void 0,c={},o=[{value:"Introduction",id:"introduction",level:2},{value:"1. Semantic Correctness (Seed Quality)",id:"1-semantic-correctness-seed-quality",level:2},{value:"Definition",id:"definition",level:3},{value:"Purpose",id:"purpose",level:3},{value:"Applications",id:"applications",level:3},{value:"2. Semantic Perturbation (Robustness Testing)",id:"2-semantic-perturbation-robustness-testing",level:2},{value:"Definition",id:"definition-1",level:3},{value:"Purpose",id:"purpose-1",level:3},{value:"Applications",id:"applications-1",level:3},{value:"3. Comparative Summary",id:"3-comparative-summary",level:2},{value:"References",id:"references",level:2}];function d(e){const t={a:"a",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(t.p,{children:["In the context of code generation and evaluation, ",(0,s.jsx)(t.strong,{children:"semantic"}),' metrics and techniques move beyond assessing surface-level syntax (i.e., "is this valid code?") to evaluate the underlying ',(0,s.jsx)(t.em,{children:"meaning"}),", ",(0,s.jsx)(t.em,{children:"logic"}),", and ",(0,s.jsx)(t.em,{children:"intent"})," of the code or the problem description."]}),"\n",(0,s.jsxs)(t.p,{children:['This category includes metrics that evaluate the "semantic quality" of a generated output (like a fuzzing seed) as well as benchmark techniques designed to ',(0,s.jsx)(t.em,{children:"test"})," a model's semantic understanding by intentionally altering a problem's meaning."]}),"\n",(0,s.jsx)(t.h2,{id:"1-semantic-correctness-seed-quality",children:"1. Semantic Correctness (Seed Quality)"}),"\n",(0,s.jsx)(t.h3,{id:"definition",children:"Definition"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Semantic Correctness"})," is a metric used to evaluate the quality of LLM-generated inputs, particularly fuzzing seeds. Rather than just generating random or syntactically valid strings, this metric assesses whether the generated seeds are ",(0,s.jsx)(t.em,{children:"semantically valid"})," and ",(0,s.jsx)(t.em,{children:"meaningful"})," according to the expected data format of the target function (e.g., generating a valid YAML document for a YAML parser, not just random text)."]}),"\n",(0,s.jsx)(t.h3,{id:"purpose",children:"Purpose"}),"\n",(0,s.jsxs)(t.p,{children:["To measure the quality and effectiveness of LLM-generated seeds for fuzzing, determining if the model understands the ",(0,s.jsx)(t.em,{children:"intent"})," of the target function's data requirements."]}),"\n",(0,s.jsx)(t.h3,{id:"applications",children:"Applications"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"LLM-driven Fuzzing"}),"\n",(0,s.jsx)(t.li,{children:"Security Testing"}),"\n",(0,s.jsx)(t.li,{children:"LLM Evaluation"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"2-semantic-perturbation-robustness-testing",children:"2. Semantic Perturbation (Robustness Testing)"}),"\n",(0,s.jsx)(t.h3,{id:"definition-1",children:"Definition"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Semantic Perturbation"})," is not a direct output metric but rather a ",(0,s.jsx)(t.em,{children:"benchmark technique"}),' used to evaluate the robustness of a code generation model. It involves applying "alterations to problem meaning for added complexity" to the problem description.\r\nFor example, a problem might be altered from "calculate the sum of all ',(0,s.jsx)(t.em,{children:"even"}),' numbers" to "calculate the sum of all ',(0,s.jsx)(t.em,{children:"odd"}),' numbers." This tests whether the model is truly understanding the semantic details of the request or just repeating a learned pattern.']}),"\n",(0,s.jsx)(t.h3,{id:"purpose-1",children:"Purpose"}),"\n",(0,s.jsx)(t.p,{children:"To assess an LLM's robustness and its ability to handle nuanced changes in a problem's semantic requirements, which is a more advanced test than simple surface-level or syntactic perturbations."}),"\n",(0,s.jsx)(t.h3,{id:"applications-1",children:"Applications"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Evaluating the robustness of code generation models."}),"\n",(0,s.jsx)(t.li,{children:"Creating more challenging benchmark problems."}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"3-comparative-summary",children:"3. Comparative Summary"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Metric / Technique"}),(0,s.jsx)(t.th,{children:"Type"}),(0,s.jsx)(t.th,{children:"Measures"}),(0,s.jsx)(t.th,{children:"Typical Domain"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Semantic Correctness"})}),(0,s.jsx)(t.td,{children:"Output Metric"}),(0,s.jsx)(t.td,{children:"The structural and logical validity of generated data (e.g., fuzz seeds)."}),(0,s.jsx)(t.td,{children:"Fuzzing, Security"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Semantic Perturbation"})}),(0,s.jsx)(t.td,{children:"Evaluation Technique"}),(0,s.jsxs)(t.td,{children:["A model's robustness by altering the ",(0,s.jsx)(t.em,{children:"meaning"})," of a problem's prompt."]}),(0,s.jsx)(t.td,{children:"Benchmark Robustness"})]})]})]}),"\n",(0,s.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Black, G., Vaidyan, V. M., & Comert, G. (2024). ",(0,s.jsx)(t.em,{children:"Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation"}),". IEEE Access. ",(0,s.jsx)(t.a,{href:"https://doi.org/10.1109/ACCESS.2024.3484947",children:"https://doi.org/10.1109/ACCESS.2024.3484947"})]}),"\n",(0,s.jsxs)(t.li,{children:["Anand, A., Chopra, S., & Arora, M. (2025). ",(0,s.jsx)(t.em,{children:"Analysis of LLM Code Synthesis in Software Productivity"}),". In M. Saraswat & R. Kumari (eds.), Applied Intelligence and Computing. ",(0,s.jsx)(t.a,{href:"https://doi.org/10.56155/978-81-955020-9-7-24",children:"https://doi.org/10.56155/978-81-955020-9-7-24"})]}),"\n",(0,s.jsx)(t.li,{children:"(Excel Data: Papers 59, 18)"}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>l});var i=n(6540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);