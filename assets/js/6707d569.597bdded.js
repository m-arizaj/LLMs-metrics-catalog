"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7787],{5912:(e,i,s)=>{s.r(i),s.d(i,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>o});const n=JSON.parse('{"id":"metrics/semantic/faithfulness","title":"Faithfulness (Design Fidelity)","description":"Definition","source":"@site/docs/metrics/semantic/faithfulness.md","sourceDirName":"metrics/semantic","slug":"/metrics/semantic/faithfulness","permalink":"/LLMs-metrics-catalog/metrics/semantic/faithfulness","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"faithfulness","title":"Faithfulness (Design Fidelity)","sidebar_label":"Faithfulness"},"sidebar":"docsSidebar","previous":{"title":"Factuality","permalink":"/LLMs-metrics-catalog/metrics/semantic/factuality"},"next":{"title":"GPT-based Metrics","permalink":"/LLMs-metrics-catalog/metrics/semantic/gpt-metrics"}}');var t=s(4848),a=s(8453);const r={id:"faithfulness",title:"Faithfulness (Design Fidelity)",sidebar_label:"Faithfulness"},l=void 0,c={},o=[{value:"Definition",id:"definition",level:2},{value:"How It Is Evaluated",id:"how-it-is-evaluated",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const i={a:"a",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Faithfulness"})," is an evaluation metric used within the ",(0,t.jsx)(i.strong,{children:"DevEval"}),' benchmark to measure "Design Fidelity" . It is not a single score but a principle of evaluation, specifically assessing the extent to which a Large Language Model (LLM) adheres to specified instructions when generating software design artifacts.']}),"\n",(0,t.jsxs)(i.p,{children:["The core idea is to measure how accurately and strictly the model's output (e.g., UML diagrams, architecture design) aligns with the given ",(0,t.jsx)(i.strong,{children:"Product Requirement Document (PRD)"}),', ensuring all functionalities are met "without making any hallucinations and additions".']}),"\n",(0,t.jsx)(i.p,{children:'This metric is typically assessed using an "LLM-as-a-judge" approach.'}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"how-it-is-evaluated",children:"How It Is Evaluated"}),"\n",(0,t.jsx)(i.p,{children:"Faithfulness is evaluated differently across the sub-tasks of software design:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"For UML Class Diagrams:"}),' The metric evaluates if the generated conceptual classes, their relationships (inheritance, aggregation, composition), cardinalities, and class names "accurately represent the essentials outlined in the PRD".']}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"For UML Sequence Diagrams:"}),' The evaluation checks how "accurately and comprehensively" the diagram reflects the system\'s intended behavior as specified in the PRD. This includes capturing system events (with and without parameters) and ensuring the design is coherent with the class diagrams.']}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"For Architecture Design:"}),' The metric verifies that the file tree structure is in "strict accordance with the given PRD and UML class diagrams," ensuring a consistent development process.']}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsxs)(i.p,{children:["The purpose of the Faithfulness metric is to quantify a model's ability to ",(0,t.jsx)(i.strong,{children:"strictly follow detailed, document-level requirements"})," and translate them into accurate, corresponding design artifacts. This measures a critical component of design fidelity beyond just generating plausible-sounding designs."]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Software Engineering"}),"\n",(0,t.jsx)(i.li,{children:"Software Design"}),"\n",(0,t.jsx)(i.li,{children:"General LLM Evaluation"}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"DevEval"}),' (specifically the "Software Design" task)']}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Measures a crucial real-world software engineering skill: adherence to specifications."}),"\n",(0,t.jsx)(i.li,{children:'Directly targets and penalizes model "hallucinations" or unrequested feature additions.'}),"\n",(0,t.jsx)(i.li,{children:"Moves evaluation beyond simple code generation to the critical upstream phase of design planning."}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["The metric relies on an ",(0,t.jsx)(i.strong,{children:'"LLM-as-a-judge"'})," for evaluation, which may introduce its own biases or inconsistencies."]}),"\n",(0,t.jsx)(i.li,{children:"It is a subjective assessment of alignment rather than a quantitative, replicable numerical score."}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["Li, B., Wu, W., Tang, Z., & Shi, S. (2024). Prompting large language models to tackle the full software development lifecycle: A case study. arXiv. ",(0,t.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.2403.08604",children:"https://doi.org/10.48550/arXiv.2403.08604"})]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,s)=>{s.d(i,{R:()=>r,x:()=>l});var n=s(6540);const t={},a=n.createContext(t);function r(e){const i=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);