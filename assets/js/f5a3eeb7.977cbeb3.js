"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[2028],{1437:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"metrics/robustness/security-analysis-score","title":"Security Analysis Score (SAS)","description":"Definition","source":"@site/docs/metrics/robustness/security-analysis-score.md","sourceDirName":"metrics/robustness","slug":"/metrics/robustness/security-analysis-score","permalink":"/LLMs-metrics-catalog/metrics/robustness/security-analysis-score","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"security-analysis-score","title":"Security Analysis Score (SAS)","sidebar_label":"Security Analysis Score"},"sidebar":"docsSidebar","previous":{"title":"Robustness","permalink":"/LLMs-metrics-catalog/metrics/robustness/robustness"},"next":{"title":"Reliability","permalink":"/LLMs-metrics-catalog/metrics/robustness/reliability"}}');var t=i(4848),r=i(8453);const a={id:"security-analysis-score",title:"Security Analysis Score (SAS)",sidebar_label:"Security Analysis Score"},o=void 0,l={},c=[{value:"Definition",id:"definition",level:2},{value:"Calculation (General Idea)",id:"calculation-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const s={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(s.p,{children:["The ",(0,t.jsx)(s.strong,{children:"Security Analysis Score (SAS)"})," is a metric used to evaluate the security posture of code, typically generated by a Large Language Model (LLM). It provides a quantitative vulnerability assessment based on established industry frameworks, most notably the ",(0,t.jsx)(s.strong,{children:"OWASP Top 10"})," (Open Web Application Security Project), and is implemented using static analysis techniques."]}),"\n",(0,t.jsx)(s.p,{children:"The score specifically evaluates the presence or absence of common, critical security issues, such as:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"SQL Injection"}),"\n",(0,t.jsx)(s.li,{children:"Cross-Site Scripting (XSS)"}),"\n",(0,t.jsx)(s.li,{children:"Buffer Overflows"}),"\n",(0,t.jsx)(s.li,{children:"Insecure Cryptographic Practices"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"calculation-general-idea",children:"Calculation (General Idea)"}),"\n",(0,t.jsx)(s.p,{children:"Unlike metrics with a simple mathematical formula, the SAS is typically derived from the output of Static Analysis Security Testing (SAST) tools."}),"\n",(0,t.jsxs)(s.p,{children:["The calculation is generally based on the ",(0,t.jsx)(s.strong,{children:"identification, count, and classification"})," of vulnerabilities found in the code. A higher score indicates fewer or less severe vulnerabilities, while a lower score indicates significant security risks. It is a key component of the ",(0,t.jsx)(s.strong,{children:"Code Quality Assessment"})," dimension in modern software engineering benchmarks."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsxs)(s.p,{children:["The primary purpose of the SAS is to move evaluation beyond mere functional correctness and assess the ",(0,t.jsx)(s.strong,{children:"safety and robustness"})," of generated code. It quantifies a model's ability to produce code that is not only functional but also secure against common attack vectors, which is a critical aspect of real-world software engineering."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Software Engineering"}),"\n",(0,t.jsx)(s.li,{children:"Long-Context Code Evaluation"}),"\n",(0,t.jsx)(s.li,{children:"Code Quality Assessment"}),"\n",(0,t.jsx)(s.li,{children:"Security Vulnerability Assessment"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Real-World Relevance:"})," Measures a critical aspect of code quality (security) that is often overlooked by metrics focused only on functional correctness or textual similarity."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Standardized:"})," By basing the assessment on industry-standard frameworks like OWASP, the metric provides a well-understood and widely accepted basis for evaluation."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Actionable:"}),' Provides specific feedback on security flaws (e.g., "SQL injection found"), which is more actionable for model improvement than a simple pass/fail score.']}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Tool-Dependent:"})," The score's accuracy and comprehensiveness are highly dependent on the underlying static analysis tools used for detection."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Potential for False Positives/Negatives:"})," Static analysis is known for sometimes misidentifying issues (false positives) or, more commonly, missing complex, logic-based vulnerabilities that require dynamic execution to detect."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Scope:"})," Primarily focused on ",(0,t.jsx)(s.em,{children:"common"})," vulnerabilities (like those in the OWASP Top 10) and may not cover novel or highly domain-specific security flaws."]}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(s.p,{children:["Qiu, J., Liu, Z., Liu, Z., Murthy, R., Zhang, J., Chen, H., Wang, S., Zhu, M., Yang, L., Tan, J., Cen, Z., Qian, C., Heinecke, S., Yao, W., Savarese, S., Xiong, C., & Wang, H. (2025). LoCoBench: A benchmark for long-context large language models in complex software engineering. arXiv. ",(0,t.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.2509.09614",children:"https://doi.org/10.48550/arXiv.2509.09614"})]})]})}function u(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,s,i)=>{i.d(s,{R:()=>a,x:()=>o});var n=i(6540);const t={},r=n.createContext(t);function a(e){const s=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),n.createElement(r.Provider,{value:s},e.children)}}}]);