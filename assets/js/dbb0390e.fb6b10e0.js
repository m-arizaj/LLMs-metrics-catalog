"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[9050],{4002:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"metrics/generative/side","title":"SIDE","description":"Definition","source":"@site/docs/metrics/generative/SIDE.md","sourceDirName":"metrics/generative","slug":"/metrics/generative/side","permalink":"/LLMs-metrics-catalog/metrics/generative/side","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"side","title":"SIDE","sidebar_label":"SIDE"},"sidebar":"docsSidebar","previous":{"title":"MAUVE","permalink":"/LLMs-metrics-catalog/metrics/generative/mauve"},"next":{"title":"Spatial FID (sFID)","permalink":"/LLMs-metrics-catalog/metrics/generative/spatial-fid"}}');var s=n(4848),r=n(8453);const a={id:"side",title:"SIDE",sidebar_label:"SIDE"},o=void 0,l={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const i={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.h2,{id:"definition",children:"Definition"}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"SIDE"})," is an ",(0,s.jsx)(i.strong,{children:"embedding-based metric"})," specifically designed to evaluate the quality of code summaries. It stands out from other embedding-based metrics like BERTScore by applying ",(0,s.jsx)(i.strong,{children:"contrastive learning"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"Instead of just measuring direct similarity, SIDE enhances the evaluation by learning to pull similar (positive) code-summary pairs closer together in the embedding space while pushing dissimilar (negative) pairs apart. This approach allows it to better capture the semantic adequacy of a generated summary."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,s.jsx)(i.p,{children:"SIDE is grouped with other embedding-based metrics that work by encoding both the generated artifact (the code summary) and a reference summary into high-dimensional vectors (embeddings) and then measuring the similarity between them."}),"\n",(0,s.jsxs)(i.p,{children:["Its specific formulation uses ",(0,s.jsx)(i.strong,{children:"contrastive learning"})," to refine a ",(0,s.jsx)(i.strong,{children:"cosine similarity"}),"-based evaluation, making it more robust for the code summarization task."]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"purpose",children:"Purpose"}),"\n",(0,s.jsx)(i.p,{children:"The primary purpose of SIDE is to assess the quality of machine-generated code summaries. It is used as an automatic metric to compare a candidate summary against a human-written reference summary."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"domains",children:"Domains"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Software Engineering"}),"\n",(0,s.jsx)(i.li,{children:"Code Summarization"}),"\n"]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"advantages",children:"Advantages"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"State-of-the-Art:"})," It is considered a state-of-the-art metric specifically designed for the code summarization task."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Task-Specific:"})," Unlike general text-based metrics, SIDE's use of contrastive learning is tailored for evaluating code summaries."]}),"\n"]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Similarity vs. Correctness:"}),' Like other embedding-based metrics, SIDE "approximate[s] correctness through similarity". This can be a limitation, as "similarity does not always align with correctness". A summary could be syntactically different but semantically correct, which might result in a lower-than-expected score.']}),"\n"]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"key-references",children:"Key References"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Mastropaolo, A., Ciniselli, M., Penta, M. D., & Bavota, G. (2024). ",(0,s.jsx)(i.em,{children:"Evaluating code summarization techniques: A new metric and an empirical characterization"}),". Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE 2024). ",(0,s.jsx)(i.a,{href:"https://doi.org/10.1145/3597503.3639174",children:"https://doi.org/10.1145/3597503.3639174"})]}),"\n",(0,s.jsxs)(i.li,{children:["Zhou, X., Kim, K., Zhang, T., et al. (2025). ",(0,s.jsx)(i.em,{children:"SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE"}),".",(0,s.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.2505.20854",children:"https://doi.org/10.48550/arXiv.2505.20854"})]}),"\n"]})]})}function m(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var t=n(6540);const s={},r=t.createContext(s);function a(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);