"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[3723],{2473:(e,i,s)=>{s.r(i),s.d(i,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>n,toc:()=>o});const n=JSON.parse('{"id":"metrics/human/bias","title":"Bias","description":"Introduction","source":"@site/docs/metrics/human/bias.md","sourceDirName":"metrics/human","slug":"/metrics/human/bias","permalink":"/LLMs-metrics-catalog/metrics/human/bias","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"bias","title":"Bias","sidebar_label":"Bias"},"sidebar":"docsSidebar","previous":{"title":"Honesty","permalink":"/LLMs-metrics-catalog/metrics/human/honesty"},"next":{"title":"AMBER Score","permalink":"/LLMs-metrics-catalog/metrics/generative/amber-score"}}');var r=s(4848),a=s(8453);const t={id:"bias",title:"Bias",sidebar_label:"Bias"},c=void 0,l={},o=[{value:"Introduction",id:"introduction",level:2},{value:"Formula and Structure",id:"formula-and-structure",level:2},{value:"Variants and Contexts of Use",id:"variants-and-contexts-of-use",level:2},{value:"1. Bias (Category)",id:"1-bias-category",level:3},{value:"2. Bias in Gender Representation Rate",id:"2-bias-in-gender-representation-rate",level:3},{value:"3. Bias Score",id:"3-bias-score",level:3},{value:"Interpretation",id:"interpretation",level:2},{value:"References",id:"references",level:2},{value:"Additional References in Database",id:"additional-references-in-database",level:3}];function d(e){const i={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",ol:"ol",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(i.p,{children:["Bias represents the systematic and unfair deviation in model behavior toward specific groups, attributes, or contexts.",(0,r.jsx)(i.br,{}),"\n","In the evaluation of large language models (LLMs), bias metrics assess ethical reliability, stereotypical associations, and unequal treatment across demographic or linguistic categories. These metrics can be applied to textual, code, or reasoning tasks to ensure fairness, inclusivity, and transparency in model outputs."]}),"\n",(0,r.jsx)(i.p,{children:"The seminal work by Mehrabi et al. (2019) categorizes bias sources into three main stages of the machine learning lifecycle:"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Data Bias"})," \u2013 Arising from sampling imbalance, labeling inconsistency, or historical representation gaps."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Algorithmic Bias"})," \u2013 Emerging from model design, optimization objectives, or embedding propagation."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"User Interaction Bias"})," \u2013 Introduced during deployment, user feedback, or societal reinforcement cycles.\r\nIn software and LLM evaluation, bias metrics are critical to ensure ethical compliance and alignment with responsible AI principles."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"formula-and-structure",children:"Formula and Structure"}),"\n",(0,r.jsxs)(i.p,{children:["Bias can be quantified by measuring the asymmetric association between sensitive attributes and model predictions or word representations.",(0,r.jsx)(i.br,{}),"\n","A common statistical formulation, used in linguistic or representational bias analysis, is:"]}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mi,{children:"log"}),(0,r.jsx)(i.mo,{children:"\u2061"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mo,{fence:"true",children:"("}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]})]}),(0,r.jsx)(i.mo,{fence:"true",children:")"})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Bias(w) = \\log \\left( \\frac{P(w|f)}{P(w|m)} \\right)"})]})})}),"\n",(0,r.jsx)(i.p,{children:"where:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"P(w|f)"})]})})})," and ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"P(w|m)"})]})})})," denote the conditional probabilities of word ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsx)(i.mi,{children:"w"})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"w"})]})})})," appearing in female-associated and male-associated contexts, respectively."]}),"\n",(0,r.jsx)(i.li,{children:"A positive score indicates a bias toward the female context, while a negative value indicates male-associated bias."}),"\n",(0,r.jsx)(i.li,{children:"A value near zero suggests neutrality."}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"Other forms of bias detection rely on comparing performance or prediction rates across groups:"}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"e"})]})]}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"g"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"u"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mn,{children:"1"})]})]})]}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"g"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"u"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mn,{children:"2"})]})]})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Bias_{rate} = |Metric_{group_1} - Metric_{group_2}|"})]})})}),"\n",(0,r.jsxs)(i.p,{children:["where ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"c"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Metric"})]})})})," can represent accuracy, precision, or generation rate for each demographic category.",(0,r.jsx)(i.br,{}),"\n","This structure underlies most fairness-based evaluations such as Equal Opportunity, Equalized Odds, or Demographic Parity."]}),"\n",(0,r.jsx)(i.h2,{id:"variants-and-contexts-of-use",children:"Variants and Contexts of Use"}),"\n",(0,r.jsx)(i.h3,{id:"1-bias-category",children:"1. Bias (Category)"}),"\n",(0,r.jsxs)(i.p,{children:["Defined in HELM and Chatbot Arena (2024) as a ",(0,r.jsx)(i.em,{children:"meta-category"})," that groups multiple sub-metrics capturing social, cultural, and ethical biases in LLMs. It includes measures of representational fairness, stereotype association, and output neutrality across demographic groups."]}),"\n",(0,r.jsx)(i.h3,{id:"2-bias-in-gender-representation-rate",children:"2. Bias in Gender Representation Rate"}),"\n",(0,r.jsx)(i.p,{children:"Quantifies gender skew in generated content by comparing the frequency of gendered terms or roles (e.g., \u201che\u201d vs. \u201cshe\u201d) in model outputs. Used to evaluate how balanced model responses are in gender-relevant contexts, particularly in summarization or conversational generation tasks."}),"\n",(0,r.jsx)(i.h3,{id:"3-bias-score",children:"3. Bias Score"}),"\n",(0,r.jsx)(i.p,{children:"Introduced in BBQ (2023) \u2014 Bias Benchmark for Question Answering \u2014 this metric measures contextual bias in question answering tasks. It evaluates whether a model prefers biased or neutral responses by comparing accuracy across ambiguous and unambiguous question types."}),"\n",(0,r.jsx)(i.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,r.jsx)(i.p,{children:"Bias metrics collectively assess how balanced and fair a model\u2019s decisions or generations are with respect to sensitive attributes such as gender, race, religion, or profession. A low bias score or small inter-group difference implies fairer and more equitable behavior, while higher values signal stronger systemic bias or stereotyping tendencies."}),"\n",(0,r.jsx)(i.p,{children:"In the context of software engineering, bias analysis helps ensure:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Ethical compliance in documentation and code suggestions."}),"\n",(0,r.jsx)(i.li,{children:"Neutral representation in developer-assisting LLMs."}),"\n",(0,r.jsx)(i.li,{children:"Fair performance across tasks, datasets, or languages."}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"However, bias evaluation remains complex:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"It is sensitive to the dataset\u2019s inherent skew."}),"\n",(0,r.jsx)(i.li,{children:"Metrics may conflict (e.g., improving demographic parity might reduce accuracy)."}),"\n",(0,r.jsx)(i.li,{children:"Contextual interpretation is crucial, as not all disparities imply unfairness."}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019)."})," A Survey on Bias and Fairness in Machine Learning.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://arxiv.org/abs/1908.09635",children:"https://arxiv.org/abs/1908.09635"})]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Parrish, A., et al. (2023)."})," BBQ: A Benchmark for Bias in Question Answering.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://github.com/nyu-mll/BBQ",children:"https://github.com/nyu-mll/BBQ"})]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"HELM (2024)."})," Holistic Evaluation of Language Models \u2013 Bias Category.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://crfm.stanford.edu/helm/latest/",children:"https://crfm.stanford.edu/helm/latest/"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"additional-references-in-database",children:"Additional References in Database"}),"\n",(0,r.jsx)(i.p,{children:"-\xa04,\xa09,\xa016"})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,i,s)=>{s.d(i,{R:()=>t,x:()=>c});var n=s(6540);const r={},a=n.createContext(r);function t(e){const i=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);