"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[4222],{7169:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"metrics/human/honesty","title":"Honesty","description":"Definition","source":"@site/docs/metrics/human/honesty.md","sourceDirName":"metrics/human","slug":"/metrics/human/honesty","permalink":"/LLMs-metrics-catalog/metrics/human/honesty","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"honesty","title":"Honesty","sidebar_label":"Honesty"},"sidebar":"docsSidebar","previous":{"title":"Helpfulness","permalink":"/LLMs-metrics-catalog/metrics/human/helpfulness"},"next":{"title":"Bias","permalink":"/LLMs-metrics-catalog/metrics/human/bias"}}');var i=s(4848),t=s(8453);const r={id:"honesty",title:"Honesty",sidebar_label:"Honesty"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Honesty"})," is a core principle used in the ",(0,i.jsx)(n.strong,{children:"Human Evaluation"}),' of Large Language Models (LLMs). It is part of the "3H rule" \u2014 ',(0,i.jsx)(n.strong,{children:"Helpfulness, Honesty, and Harmlessness"})," \u2014 which serves as a foundational concept for developing detailed human assessment criteria."]}),"\n",(0,i.jsx)(n.p,{children:"Human evaluation itself is defined as a method to assess the quality and accuracy of a model's generated results through human participation. This approach is considered more comprehensive and accurate than automated evaluation because it is closer to real-world application scenarios."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,i.jsx)(n.p,{children:"Honesty is not a quantitative metric with a mathematical formula. It is a qualitative principle assessed by human evaluators (such as experts, researchers, or ordinary users)."}),"\n",(0,i.jsxs)(n.p,{children:['In the survey (Paper 9), the principle of "Honesty" is elaborated into more specific, measurable criteria that evaluators use, such as ',(0,i.jsx)(n.strong,{children:"Accuracy"}),'. Accuracy is defined as scrutinizing "the extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies".']}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsx)(n.p,{children:'The purpose of evaluating for Honesty is to assess an LLM\'s adherence to truthfulness and factual correctness. It is a fundamental component of the "3H rule" for human alignment and trustworthiness. This principle helps ensure that models are reliable and avoid generating factually inaccurate information or "hallucinations".'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Human Evaluation"}),"\n",(0,i.jsx)(n.li,{children:"Human-centered Evaluation"}),"\n",(0,i.jsx)(n.li,{children:"LLM Evaluation (Trustworthiness, Ethics)"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,i.jsx)(n.p,{children:"Honesty, as a human evaluation criterion, is applied within broader evaluation frameworks and benchmarks. The benchmarks explicitly mentioned in connection with this evaluation domain include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"HELM (Holistic Evaluation of Language Models)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Chatbot Arena"})}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"More Reliable:"}),' Human evaluation based on principles like Honesty is considered "more reliable" than automated metrics, especially for open-ended generation tasks.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-World Scenarios:"}),' This evaluation method is "closer to the actual application scenario" and "can provide more comprehensive and accurate feedback".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fundamental Assessment:"})," It assesses a core pillar of model trustworthiness that automated metrics (like F1 or ROUGE) may not capture."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Subjectivity and Variance:"}),' Human evaluation can have "high variance and instability," which may be "due to cultural and individual differences" among the human evaluators.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Requires Human Labor:"})," By definition, it is not an automated process and requires inviting human evaluators (experts, researchers, or users) to assess the model's outputs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Requires Clear Rubrics:"}),' The effectiveness of the evaluation depends heavily on the "evaluation criteria" and the "evaluator\'s expertise level".']}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Chang, Y., Wang, X., Wang, J., et al. (2023). ",(0,i.jsx)(n.em,{children:"A Survey on Evaluation of Large Language Models"}),". ",(0,i.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:"https://doi.org/10.48550/arXiv.2307.03109"})]}),"\n",(0,i.jsxs)(n.li,{children:["Askell, A., Bai, Y., Chen, A., et al. (2021). ",(0,i.jsx)(n.em,{children:"A general language assistant as a laboratory for alignment"}),". arXiv preprint arXiv:2112.00861. (Cited in Paper 9 as the source of the 3H rule)"]}),"\n",(0,i.jsx)(n.li,{children:"(Excel Data: Paper 9)"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var a=s(6540);const i={},t=a.createContext(i);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);