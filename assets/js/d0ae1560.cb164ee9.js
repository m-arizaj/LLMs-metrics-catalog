"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1745],{2522:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"metrics/ranking/reward-score","title":"Reward Score","description":"Definition","source":"@site/docs/metrics/ranking/reward-score.md","sourceDirName":"metrics/ranking","slug":"/metrics/ranking/reward-score","permalink":"/LLMs-metrics-catalog/metrics/ranking/reward-score","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"reward-score","title":"Reward Score","sidebar_label":"Reward Score"},"sidebar":"docsSidebar","previous":{"title":"RImp","permalink":"/LLMs-metrics-catalog/metrics/ranking/relative-improvement"},"next":{"title":"Hallucination","permalink":"/LLMs-metrics-catalog/metrics/semantic/hallucination"}}');var s=i(4848),t=i(8453);const a={id:"reward-score",title:"Reward Score",sidebar_label:"Reward Score"},o=void 0,l={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Reward Score"})," (also referred to as reward/score) is an objective, quantitative metric used in the evaluation of LLM-based autonomous agents. It is a primary metric in the category of ",(0,s.jsx)(n.strong,{children:"Task Success"}),", which measures how well an agent can successfully complete its goals."]}),"\n",(0,s.jsx)(n.p,{children:"This metric is typically defined by the environment or benchmark itself, where the agent receives a score based on its performance."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,s.jsxs)(n.p,{children:["The specific formula for the Reward Score is task-dependent and defined by the benchmark being used. It is a quantitative value where ",(0,s.jsx)(n.strong,{children:"higher values indicate greater task completion ability"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"For example, in a benchmark like WebShop, the reward score could be based on a combination of task completion and other factors."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,s.jsx)(n.p,{children:"The purpose of the Reward Score is to provide a \"concrete, measurable insight\"  into an agent's performance and its ability to achieve its defined objectives. It is one of the key metrics used to quantify an agent's task completion ability."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LLM Agents / Autonomous Agents"}),"\n",(0,s.jsx)(n.li,{children:"Objective Evaluation of Agents"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Objective:"}),' It is an objective metric that provides "concrete, measurable insights" into an agent\'s performance, as opposed to subjective human judgments.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Direct Measurement:"})," It directly measures the agent's ability to complete a task successfully."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Comparable:"})," As a quantitative score, it allows for direct comparison and tracking of performance over time between different agents."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'While the metric itself is objective, it may not capture all aspects of an agent\'s performance, such as "intelligence or user-friendliness," which are better assessed via subjective evaluation.'}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Wang, L., Ma, C., Feng, X. et al. A survey on large language model based autonomous agents. Front. Comput. Sci. 18, 186345 (2024). ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1007/s11704-024-40231-1",children:"https://doi.org/10.1007/s11704-024-40231-1"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J. B., Shu, T., & Gan, C. (2024). ",(0,s.jsx)(n.em,{children:"Building cooperative embodied agents modularly with large language models"}),". ",(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2307.02485",children:"https://doi.org/10.48550/arXiv.2307.02485"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., & Cao, Y. (2023). ",(0,s.jsx)(n.em,{children:"ReAct: synergizing reasoning and acting in language models"}),". ",(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2210.03629",children:"https://doi.org/10.48550/arXiv.2210.03629"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Mehta, N., Teruel, M., Sanz, P. F., Deng, X., Awadallah, A. H., & Kiseleva, J. (2024). ",(0,s.jsx)(n.em,{children:"Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback"}),".",(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2304.10750",children:"https://doi.org/10.48550/arXiv.2304.10750"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);