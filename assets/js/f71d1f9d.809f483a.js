"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[2859],{5284:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"metrics/human/user-related-metrics","title":"User-Related Metrics (Rating & Satisfaction)","description":"Introduction","source":"@site/docs/metrics/human/User-related-metrics.md","sourceDirName":"metrics/human","slug":"/metrics/human/user-related-metrics","permalink":"/LLMs-metrics-catalog/metrics/human/user-related-metrics","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"user-related-metrics","title":"User-Related Metrics (Rating & Satisfaction)","sidebar_label":"User-Related Metrics"},"sidebar":"docsSidebar","previous":{"title":"Usefulness Score","permalink":"/LLMs-metrics-catalog/metrics/human/usefulness-score"},"next":{"title":"General Principles","permalink":"/LLMs-metrics-catalog/metrics/human/general-principles"}}');var n=i(4848),r=i(8453);const a={id:"user-related-metrics",title:"User-Related Metrics (Rating & Satisfaction)",sidebar_label:"User-Related Metrics"},l=void 0,o={},c=[{value:"Introduction",id:"introduction",level:2},{value:"1. User Rating",id:"1-user-rating",level:2},{value:"Definition",id:"definition",level:3},{value:"Purpose",id:"purpose",level:3},{value:"2. User Satisfaction",id:"2-user-satisfaction",level:2},{value:"Definition",id:"definition-1",level:3},{value:"Purpose",id:"purpose-1",level:3},{value:"3. Comparative Summary",id:"3-comparative-summary",level:2},{value:"References",id:"references",level:2}];function d(e){const t={a:"a",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(t.p,{children:["In the evaluation of code generation models, ",(0,n.jsx)(t.strong,{children:"User-Related Metrics"})," are a class of ",(0,n.jsx)(t.strong,{children:"feedback-based evaluation"})," methods. These metrics are essential for assessing the practical quality of generated code by incorporating human judgment and expertise."]}),"\n",(0,n.jsxs)(t.p,{children:["Unlike automated metrics that check for correctness or similarity, user-related metrics focus on the practical ",(0,n.jsx)(t.strong,{children:"usability"}),", ",(0,n.jsx)(t.strong,{children:"helpfulness"}),", and ",(0,n.jsx)(t.strong,{children:"overall experience"})," of a developer using an LLM as a tool. This approach helps evaluate aspects like readability, maintainability, and real-world effectiveness, which are crucial for a model's practical application."]}),"\n",(0,n.jsx)(t.h2,{id:"1-user-rating",children:"1. User Rating"}),"\n",(0,n.jsx)(t.h3,{id:"definition",children:"Definition"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"User Rating"})," is a subjective metric used in the ",(0,n.jsx)(t.strong,{children:"RealHumanEval"}),' benchmark to measure the perceived "usability and helpfulness" of model-generated code.']}),"\n",(0,n.jsxs)(t.p,{children:["Experienced programmers (213 participants in the study) were asked to complete real programming tasks using LLM assistance (either auto-completion or chat support). Afterward, they rated the model's contribution on a ",(0,n.jsx)(t.strong,{children:"scale from 1 to 5"}),"."]}),"\n",(0,n.jsx)(t.h3,{id:"purpose",children:"Purpose"}),"\n",(0,n.jsx)(t.p,{children:"The goal of the User Rating metric is to capture a subjective measure of a model's practical usability. It helps identify discrepancies between a user's preference and the model's actual performance, providing insights for user-centric optimizations."}),"\n",(0,n.jsx)(t.h2,{id:"2-user-satisfaction",children:"2. User Satisfaction"}),"\n",(0,n.jsx)(t.h3,{id:"definition-1",children:"Definition"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"User Satisfaction"})," is a subjective metric used in the ",(0,n.jsx)(t.strong,{children:"Copilot Evaluation Harness"}),' benchmark to measure a developer\'s "overall experience" when using GitHub Copilot.']}),"\n",(0,n.jsxs)(t.p,{children:["Developers used Copilot within Visual Studio Code to complete a variety of tasks (e.g., code generation, bug fixing, documentation). They then provided subjective feedback on the tool's helpfulness, accuracy, and quality, including a satisfaction rating on a ",(0,n.jsx)(t.strong,{children:"scale from 1 to 5"}),"."]}),"\n",(0,n.jsx)(t.h3,{id:"purpose-1",children:"Purpose"}),"\n",(0,n.jsx)(t.p,{children:"This metric aims to comprehensively evaluate a model's performance in a real-world development environment. It helps assess its practical applicability and efficiency gains by directly polling developers about their experience."}),"\n",(0,n.jsx)(t.h2,{id:"3-comparative-summary",children:"3. Comparative Summary"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Metric"}),(0,n.jsx)(t.th,{children:"Definition"}),(0,n.jsx)(t.th,{children:"Scale"}),(0,n.jsx)(t.th,{children:"Benchmark"}),(0,n.jsx)(t.th,{children:"Focus"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"User Rating"})}),(0,n.jsx)(t.td,{children:'Measures the "usability and helpfulness" of generated code.'}),(0,n.jsx)(t.td,{children:"1 to 5"}),(0,n.jsx)(t.td,{children:"RealHumanEval"}),(0,n.jsx)(t.td,{children:"Helpfulness in completing a task"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"User Satisfaction"})}),(0,n.jsx)(t.td,{children:'Measures the "overall experience" of using the LLM tool.'}),(0,n.jsx)(t.td,{children:"1 to 5"}),(0,n.jsx)(t.td,{children:"Copilot Evaluation Harness"}),(0,n.jsx)(t.td,{children:"Overall tool experience and quality"})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., Wu, J., Wang, Y., Gao, Q., Wang, J., Ye, W., & Zhang, S. (2025). ",(0,n.jsx)(t.em,{children:"A Survey on Evaluating Large Language Models in Code Generation Tasks"}),". ",(0,n.jsx)(t.a,{href:"https://doi.org/10.48550/arXiv.2408.16498",children:"https://doi.org/10.48550/arXiv.2408.16498"})]}),"\n",(0,n.jsx)(t.li,{children:"(Excel Data: Paper 10)"}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>l});var s=i(6540);const n={},r=s.createContext(n);function a(e){const t=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);