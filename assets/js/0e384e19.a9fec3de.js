"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[976],{2053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"intro","title":"Introduction","description":"Welcome to the LLMs Metrics Catalog, a structured repository designed to document, classify, and analyze the evaluation metrics used to assess Large Language Models (LLMs) in software engineering tasks.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/LLMs-metrics-catalog/intro","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"intro","title":"Introduction"},"sidebar":"docsSidebar","next":{"title":"Accuracy","permalink":"/LLMs-metrics-catalog/metrics/clasicas-ml/accuracy"}}');var i=t(4848),s=t(8453);const a={id:"intro",title:"Introduction"},o=void 0,c={},l=[{value:"Purpose",id:"purpose",level:3},{value:"Structure",id:"structure",level:3},{value:"Research Context",id:"research-context",level:3}];function d(e){const n={br:"br",h3:"h3",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Welcome to the ",(0,i.jsx)(n.strong,{children:"LLMs Metrics Catalog"}),", a structured repository designed to document, classify, and analyze the ",(0,i.jsx)(n.strong,{children:"evaluation metrics"})," used to assess ",(0,i.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," in ",(0,i.jsx)(n.strong,{children:"software engineering"})," tasks."]}),"\n",(0,i.jsx)(n.p,{children:"This catalog compiles a wide range of quantitative and qualitative metrics drawn from academic research and industry benchmarks, covering domains such as:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Code generation (e.g., Pass@k, CodeBLEU, Execution Accuracy)"}),"\n",(0,i.jsx)(n.li,{children:"Test generation and bug fixing (e.g., Non-Trivial Assertion Rate, Timeout Rate)"}),"\n",(0,i.jsx)(n.li,{children:"Documentation and natural language tasks (e.g., BLEU, ROUGE-L, BERTScore)"}),"\n",(0,i.jsx)(n.li,{children:"Human and semantic evaluation (e.g., Subjective Score, Helpfulness, Truthfulness)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Each entry in the catalog includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A definition of the metric"}),"\n",(0,i.jsx)(n.li,{children:"Its purpose and evaluation dimension (performance, robustness, quality, etc.)"}),"\n",(0,i.jsx)(n.li,{children:"The source paper or benchmark where it was introduced or used"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsxs)(n.p,{children:["The goal of this catalog is to serve as a centralized reference for researchers and practitioners exploring how LLMs are evaluated in the context of software engineering.",(0,i.jsx)(n.br,{}),"\n","It helps identify which metrics dominate current literature, where there are gaps in evaluation coverage, and how new metrics contribute to improving model reliability and interpretability."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"structure",children:"Structure"}),"\n",(0,i.jsx)(n.p,{children:"The catalog is organized by metric category and evaluation goal, allowing users to easily navigate:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Accuracy and Performance"}),"\n",(0,i.jsx)(n.li,{children:"Semantic and Structural Quality"}),"\n",(0,i.jsx)(n.li,{children:"Robustness and Stability"}),"\n",(0,i.jsx)(n.li,{children:"Human and Subjective Evaluation"}),"\n",(0,i.jsx)(n.li,{children:"Novelty and Memorization"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Each section includes definitions, examples, and contextual explanations to promote understanding and comparison across metrics."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"research-context",children:"Research Context"}),"\n",(0,i.jsx)(n.p,{children:"This catalog was developed as part of a research project on LLM evaluation in software engineering, aiming to provide a reproducible and transparent reference for metric selection in academic and applied studies."}),"\n",(0,i.jsx)(n.hr,{})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);