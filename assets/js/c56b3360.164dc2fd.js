"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1385],{5129:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"metrics/human/subjective-score","title":"Subjective Score","description":"Definition","source":"@site/docs/metrics/human/subjective-score.md","sourceDirName":"metrics/human","slug":"/metrics/human/subjective-score","permalink":"/LLMs-metrics-catalog/metrics/human/subjective-score","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"subjective-score","title":"Subjective Score","sidebar_label":"Subjective Score"},"sidebar":"docsSidebar","previous":{"title":"Stereotype Score","permalink":"/LLMs-metrics-catalog/metrics/human/stereotype-score"},"next":{"title":"Usefulness Score","permalink":"/LLMs-metrics-catalog/metrics/human/usefulness-score"}}');var t=s(4848),r=s(8453);const a={id:"subjective-score",title:"Subjective Score",sidebar_label:"Subjective Score"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Evaluation Dimensions",id:"evaluation-dimensions",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",annotation:"annotation",em:"em",h2:"h2",hr:"hr",li:"li",math:"math",mfrac:"mfrac",mo:"mo",mrow:"mrow",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.strong,{children:"Subjective Score"})," is a metric used in Human-Centric Evaluation (HCE) to assess foundation models based on human perception, experience, and practical application, rather than on automated, objective quiz performance."]}),"\n",(0,t.jsx)(n.p,{children:"It is determined by human evaluators who assign a rating to a model's performance during a collaborative, open-ended task. In the HCE framework, this is specifically implemented as a five-point discrete scale:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"1: Very Weak"}),"\n",(0,t.jsx)(n.li,{children:"2: Weak"}),"\n",(0,t.jsx)(n.li,{children:"3: Moderate"}),"\n",(0,t.jsx)(n.li,{children:"4: Strong"}),"\n",(0,t.jsx)(n.li,{children:"5: Very Strong"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,t.jsx)(n.p,{children:"The final score is calculated as the arithmetic mean of all ratings provided by the human evaluators across the different dimensions."}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mtext,{children:"Subjective\xa0Score"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsxs)(n.mfrac,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mo,{children:"\u2211"}),(0,t.jsx)(n.mtext,{children:"Evaluator\xa0Ratings"})]}),(0,t.jsx)(n.mtext,{children:"Number\xa0of\xa0Ratings"})]})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\text{Subjective Score} = \\frac{\\sum \\text{Evaluator Ratings}}{\\text{Number of Ratings}}"})]})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"The primary purpose of a Subjective Score is to bridge the gap left by traditional, model-centric benchmarks (like MMLU). It aims to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Capture essential subjective dimensions of human experience, such as user satisfaction, response naturalness, and contextual adaptability."}),"\n",(0,t.jsx)(n.li,{children:"Evaluate a model's performance in realistic, open-ended, and complex tasks that mimic real-world applications, rather than simple Q&A formats."}),"\n",(0,t.jsx)(n.li,{children:"Assess how well a model aligns with user expectations and practical utility."}),"\n",(0,t.jsx)(n.li,{children:"Guide model optimization toward solving real-world human-centric problems."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Human-AI Cooperation"}),"\n",(0,t.jsx)(n.li,{children:"Research Assistance"}),"\n",(0,t.jsx)(n.li,{children:"Subjective Assessment"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-dimensions",children:"Evaluation Dimensions"}),"\n",(0,t.jsx)(n.p,{children:"The HCE framework uses the Subjective Score to rate models across three core dimensions, which are further broken down into sub-metrics:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Problem-Solving Ability:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analytical Accuracy (A.A.):"})," How precisely the model resolves the problem."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comprehensiveness (Compre.):"})," The extent to which the model considers all relevant aspects."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Assistance Efficiency (A.E.):"})," How effectively the model saves the user time."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Information Quality:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Information Reliability (I.R.):"})," Whether the information is accurate, up-to-date, and free from errors."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Exploration Depth (E.D.):"})," The level of detail, relevance, and thoroughness of the information provided."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Interaction Experience:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Content Relevance (C.R.):"})," How closely the responses align with the user's query."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback Adaptability (F.A.):"})," The model's ability to adjust its responses based on user feedback."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Expression Naturalness (E.N.):"})," Whether the responses are natural, coherent, and highly readable."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response Timeliness (R.T.):"})," How promptly the model delivers answers."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reflects Human Experience:"})," It is a human-centric metric that captures nuanced qualities like satisfaction, naturalness, and adaptability, which objective metrics miss."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-World Relevance:"})," Evaluates models in complex, open-ended scenarios (like research) that are more representative of practical applications than static datasets."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Avoids Contamination:"})," Less susceptible to data contamination, where models are trained on benchmark datasets and achieve inflated scores."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Dimensional:"})," Provides a systematic, multi-dimensional scoring design, unlike ad-hoc or single-metric subjective evaluations."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost and Scalability:"})," Relies on human participants, making it costly and time-consuming to scale (the study involved over 540 experiments). The paper suggests future work on automation to reduce this cost."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Subjectivity and Bias:"})," The scores are based on human perception and can be influenced by individual evaluator differences. For example, some evaluators prefer detailed responses, while others prefer concise ones."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Influenced by Style:"})," A model's expression style (e.g., Grok 3's use of rhetorical questions) can significantly influence an evaluator's assessment of the experience, independent of the content quality."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Guo, Y., Wang, J., Ji, K., Wen, F., Zhang, Z., Zhu, X., Li, C., & Zhai, G. (2025). ",(0,t.jsx)(n.em,{children:"Human-Centric Evaluation for Foundation Models"}),". ",(0,t.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2506.01793",children:"https://doi.org/10.48550/arXiv.2506.01793"})]}),"\n",(0,t.jsx)(n.li,{children:"(Excel Data: Paper 66)"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);