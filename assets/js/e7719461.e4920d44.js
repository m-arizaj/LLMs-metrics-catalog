"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[4763],{4007:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"metrics/human/human","title":"Human Metrics","description":"Overview","source":"@site/docs/metrics/human/human.md","sourceDirName":"metrics/human","slug":"/metrics/human/","permalink":"/LLMs-metrics-catalog/metrics/human/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"human","title":"Human Metrics","sidebar_label":"Human Metrics"},"sidebar":"docsSidebar","previous":{"title":"Test Metrics","permalink":"/LLMs-metrics-catalog/metrics/functional-test/test"},"next":{"title":"Toxicity","permalink":"/LLMs-metrics-catalog/metrics/human/toxicity"}}');var i=s(4848),a=s(8453);const r={id:"human",title:"Human Metrics",sidebar_label:"Human Metrics"},l=void 0,c={},o=[{value:"Overview",id:"overview",level:2},{value:"Formula and Structure",id:"formula-and-structure",level:2},{value:"Variants",id:"variants",level:2},{value:"Interpretation",id:"interpretation",level:2},{value:"References",id:"references",level:2},{value:"Additional References in Dataset",id:"additional-references-in-dataset",level:3}];function d(e){const n={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",munderover:"munderover",ol:"ol",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Human metrics refer to evaluation methods that rely on human judgment to assess the quality, correctness, and usefulness of model-generated outputs.  They are considered the gold standard for evaluating complex aspects that automatic metrics cannot fully capture, such as semantic adequacy, clarity, usefulness, and naturalness."}),"\n",(0,i.jsx)(n.p,{children:"In software engineering, these metrics are used to evaluate LLM-generated code, explanations, summaries, and documentation, ensuring that automated systems align with human standards of reasoning and readability."}),"\n",(0,i.jsx)(n.h2,{id:"formula-and-structure",children:"Formula and Structure"}),"\n",(0,i.jsx)(n.p,{children:"Unlike quantitative metrics, human metrics are qualitative or semi-quantitative. They generally rely on rating scales, comparative judgments, or ranking procedures."}),"\n",(0,i.jsxs)(n.p,{children:["A typical evaluation setup involves ",(0,i.jsx)(n.span,{className:"katex",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"n"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"n"})]})})})," human raters providing scores for one or more criteria:"]}),"\n",(0,i.jsx)(n.span,{className:"katex",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{children:"H"}),(0,i.jsx)(n.mo,{children:"="}),(0,i.jsxs)(n.mfrac,{children:[(0,i.jsx)(n.mn,{children:"1"}),(0,i.jsx)(n.mi,{children:"n"})]}),(0,i.jsxs)(n.munderover,{children:[(0,i.jsx)(n.mo,{children:"\u2211"}),(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{children:"i"}),(0,i.jsx)(n.mo,{children:"="}),(0,i.jsx)(n.mn,{children:"1"})]}),(0,i.jsx)(n.mi,{children:"n"})]}),(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"i"})]})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"H = \\frac{1}{n} \\sum_{i=1}^{n} s_i"})]})})}),"\n",(0,i.jsx)(n.p,{children:"Where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.span,{className:"katex",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"H"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"H"})]})})})," is the aggregated human score."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.span,{className:"katex",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"i"})]})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"s_i"})]})})})," is the score given by rater ",(0,i.jsx)(n.span,{className:"katex",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"i"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"i"})]})})})," (usually on a 1\u20135 or 1\u201310 scale)."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Human ratings may also be normalized or aggregated using statistical measures such as mean, median, or inter-rater reliability coefficients like Cohen\u2019s \u03ba or Krippendorff\u2019s \u03b1 to assess consistency among raters."}),"\n",(0,i.jsx)(n.h2,{id:"variants",children:"Variants"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Human Evaluation:"})," The general framework of human-centered evaluation applied to generated code, documentation, or text."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Human-Centered Assessment:"})," Focuses on usability and the degree to which the model supports human tasks effectively."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Human Judgment Score:"})," Aggregated score from human annotators based on correctness, style, or usefulness."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Human Rater Score (mean / max):"})," Considers either the average or best (maximum) judgment among annotators."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Human Review:"})," Applied in software testing and debugging contexts to evaluate the reliability and relevance of model-generated code."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Naturalness (Human Evaluation):"})," Assesses whether generated code or text appears natural and human-like, often in readability and code synthesis studies."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,i.jsxs)(n.p,{children:["Human metrics are essential for validating the performance of LLMs in real-world software engineering contexts.",(0,i.jsx)(n.br,{}),"\n","They serve as a bridge between quantitative model outputs and qualitative human perception, offering an interpretable benchmark for tasks where correctness alone is insufficient."]}),"\n",(0,i.jsx)(n.p,{children:"Key considerations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"High human evaluation scores indicate that the model\u2019s outputs align closely with human expectations in clarity, coherence, and utility."}),"\n",(0,i.jsx)(n.li,{children:"Inter-rater reliability ensures the consistency of judgments, validating the robustness of human evaluations."}),"\n",(0,i.jsx)(n.li,{children:"Human-centered evaluation frameworks combine these scores with automatic metrics to achieve a balanced understanding of model quality.\r\nWhile human evaluation is expensive and time-consuming, it remains the most reliable reference point for validating automatic evaluation frameworks."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Liu, Y., Fabbri, A., & Durrett, G. (2022). Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation.",(0,i.jsx)(n.br,{}),"\n","arXiv preprint. ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2212.07981",children:"https://arxiv.org/abs/2212.07981"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"additional-references-in-dataset",children:"Additional References in Dataset"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"3, 6, 7,\xa015,\xa023,\xa025,\xa062"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);