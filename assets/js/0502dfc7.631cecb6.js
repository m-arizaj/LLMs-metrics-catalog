"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[6680],{2801:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"metrics/logical/idealized","title":"Idealized Context Association (ICAT)","description":"Definition","source":"@site/docs/metrics/logical/idealized.md","sourceDirName":"metrics/logical","slug":"/metrics/logical/idealized","permalink":"/LLMs-metrics-catalog/metrics/logical/idealized","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"idealized","title":"Idealized Context Association (ICAT)","sidebar_label":"ICAT"},"sidebar":"docsSidebar","previous":{"title":"Equivalent","permalink":"/LLMs-metrics-catalog/metrics/logical/equivalent"},"next":{"title":"Model Checking (CTL)","permalink":"/LLMs-metrics-catalog/metrics/logical/model-checking-ctl"}}');var t=s(4848),a=s(8453);const r={id:"idealized",title:"Idealized Context Association (ICAT)",sidebar_label:"ICAT"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={a:"a",annotation:"annotation",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"definition",children:"Definition"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"Idealized Context Association Test (iCAT) Score"})," is a metric introduced by ",(0,t.jsx)(n.strong,{children:"Nadeem, Bethke, and Reddy (2021)"})," to measure stereotypical bias in language models. It is designed to be used with the ",(0,t.jsx)(n.strong,{children:"StereoSet dataset"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"The metric provides a single score that balances a model's language modeling capability (its ability to distinguish meaningful sentences from meaningless ones) with its level of stereotypical bias (its preference for stereotypical sentences over anti-stereotypical ones)."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,t.jsx)(n.p,{children:"The iCAT score is calculated based on two sub-scores derived from the StereoSet benchmark:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"lms (language modeling score):"})," The percentage of instances where the model correctly prefers a meaningful sentence option over a meaningless (unrelated) one."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ss (stereotype score):"})," The percentage of instances where the model prefers a stereotypical option over an anti-stereotypical one."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:['An "idealized" model is defined as having an ',(0,t.jsx)(n.code,{children:"lms"})," of 100 (perfect language understanding) and an ",(0,t.jsx)(n.code,{children:"ss"})," of 50 (chooses stereotypes and anti-stereotypes at an equal rate, showing no bias)."]}),"\n",(0,t.jsx)(n.p,{children:"The iCAT score is then calculated as:"}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"i"}),(0,t.jsx)(n.mi,{children:"C"}),(0,t.jsx)(n.mi,{children:"A"}),(0,t.jsx)(n.mi,{children:"T"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mi,{mathvariant:"script",children:"S"}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsx)(n.mi,{children:"l"}),(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mo,{children:"\u22c5"}),(0,t.jsxs)(n.mfrac,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"}),(0,t.jsx)(n.mi,{children:"n"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mn,{children:"100"}),(0,t.jsx)(n.mo,{children:"\u2212"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(n.mn,{children:"50"})]})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"iCAT(\\mathcal{S})=lms \\cdot \\frac{min(ss, 100-ss)}{50}"})]})})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"(Nadeem, Bethke, and Reddy, 2021)"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsxs)(n.p,{children:['The purpose of iCAT is to provide a single, consolidated metric that evaluates a model\'s stereotypical bias while also accounting for its fundamental language understanding. A model that simply chooses "anti-stereotype" every time might get a good stereotype score but would be penalized by iCAT if it fails to distinguish meaningful sentences from nonsensical ones (low ',(0,t.jsx)(n.code,{children:"lms"}),"). The goal is to reward models that are both coherent (high ",(0,t.jsx)(n.code,{children:"lms"}),") and unbiased ( ",(0,t.jsx)(n.code,{children:"ss"})," near 50)."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"domains",children:"Domains"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Fairness / Bias Evaluation"}),"\n",(0,t.jsx)(n.li,{children:"Natural Language Processing (NLP)"}),"\n",(0,t.jsx)(n.li,{children:"Stereotype Measurement"}),"\n",(0,t.jsx)(n.li,{children:"Contextual Bias Balance"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"StereoSet"}),": The iCAT metric was introduced specifically for use with this benchmark. StereoSet evaluates model bias across intrasentence and intersentence tasks, covering stereotypes related to race, gender, religion, and profession."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balanced Score:"})," It combines a measure of language ability (",(0,t.jsx)(n.code,{children:"lms"}),") with a measure of bias (",(0,t.jsx)(n.code,{children:"ss"}),") into a single, interpretable score."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Penalizes Naive Models:"})," A model cannot achieve a high iCAT score by simply avoiding stereotypes; it must also demonstrate strong language modeling capabilities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Clear Target:"}),' The metric provides a clear "ideal" target (an ',(0,t.jsx)(n.code,{children:"lms"})," of 100 and an ",(0,t.jsx)(n.code,{children:"ss"})," of 50) for model evaluation."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset Dependency:"})," The metric is intrinsically tied to the StereoSet dataset. Criticisms of the dataset's validity apply to the metric as well."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset Validity Concerns:"}),' Blodgett et al. (2021) noted that many instances in StereoSet (and similar benchmarks) have "ambiguities about what stereotypes they capture" and may contain "inconsistent, invalid, or unrelated perturbations of social groups".']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Weak Downstream Correlation:"})," The iCAT score is based on pseudo-log-likelihood (PLL) preferences. Some studies caution that PLL metrics may have only weak or inconsistent correlations with biases that actually appear in downstream, real-world tasks."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,t.jsxs)(n.p,{children:["Nadeem, M., Bethke, A., & Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. En Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 5356\u20135371). Association for Computational Linguistics. ",(0,t.jsx)(n.a,{href:"https://doi.org/10.18653/v1/2021.acl-long.416",children:"https://doi.org/10.18653/v1/2021.acl-long.416"})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);