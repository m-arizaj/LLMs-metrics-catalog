"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[8404],{7269:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>m});const s=JSON.parse('{"id":"metrics/efficiency/memory","title":"Memory Metrics","description":"Overview","source":"@site/docs/metrics/efficiency/memory.md","sourceDirName":"metrics/efficiency","slug":"/metrics/efficiency/memory","permalink":"/LLMs-metrics-catalog/metrics/efficiency/memory","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"memory","title":"Memory Metrics","sidebar_label":"Memory Metrics"},"sidebar":"docsSidebar","previous":{"title":"Cost Metrics","permalink":"/LLMs-metrics-catalog/metrics/efficiency/cost-metrics"},"next":{"title":"Perplexity","permalink":"/LLMs-metrics-catalog/metrics/efficiency/perplexity"}}');var r=n(4848),c=n(8453);const l={id:"memory",title:"Memory Metrics",sidebar_label:"Memory Metrics"},t=void 0,d={},m=[{value:"Overview",id:"overview",level:2},{value:"Formula and Structure",id:"formula-and-structure",level:2},{value:"Variants",id:"variants",level:2},{value:"Applications in Software Engineering",id:"applications-in-software-engineering",level:2},{value:"Interpretation",id:"interpretation",level:2},{value:"References",id:"references",level:2},{value:"Additional References in Dataset",id:"additional-references-in-dataset",level:3}];function a(e){const i={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mo:"mo",mrow:"mrow",msub:"msub",mtext:"mtext",munder:"munder",ol:"ol",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,c.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(i.p,{children:["Memory-related metrics quantify how efficiently a model utilizes system memory during inference, training, or evaluation.",(0,r.jsx)(i.br,{}),"\n","In the context of Large Language Models (LLMs) and software engineering, these metrics are essential to evaluate scalability, efficiency, and resource constraints, especially when dealing with long-context reasoning, code generation, or multi-session tasks.\r\nRecent studies, such as HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading (Luo et al., 2025), have emphasized the importance of quantifying and reducing memory usage to enable high-performance inference under limited GPU resources.",(0,r.jsx)(i.br,{}),"\n","Memory metrics are used to assess both performance-efficiency trade-offs and memorization behavior in generative and analytical models."]}),"\n",(0,r.jsx)(i.h2,{id:"formula-and-structure",children:"Formula and Structure"}),"\n",(0,r.jsx)(i.p,{children:"Depending on the metric variant, memory-related evaluation can be expressed as:"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.em,{children:"Memory Usage"})}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"y"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"U"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"g"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.munder,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"max"}),(0,r.jsx)(i.mo,{children:"\u2061"})]}),(0,r.jsx)(i.mi,{children:"t"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"t"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Memory\\ Usage = \\max_t(M_t)"})]})})}),"\n",(0,r.jsxs)(i.p,{children:["where ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"t"})]})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"M_t"})]})})})," represents the memory consumed at time step ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsx)(i.mi,{children:"t"})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"t"})]})})}),"."]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.em,{children:"Memory Efficiency"})}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"y"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"E"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"y"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"d"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"T"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"k"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"s"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"y"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"U"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"g"}),(0,r.jsx)(i.mi,{children:"e"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Memory\\ Efficiency = \\frac{Processed\\ Tokens}{Memory\\ Usage}"})]})})}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.em,{children:"Memorization Ratio"})}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"M"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"z"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"d"}),(0,r.jsx)(i.mi,{children:"u"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"d"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"s"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"T"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"l"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"s"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"s"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Memorization\\ Ratio = \\frac{Reproduced\\ Instances}{Total\\ Instances}"})]})})}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.em,{children:"Multi-Session Memory Retention"})}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"S"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"l"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"v"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"v"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"d"})]})]})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"R"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"l"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"v"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mtext,{children:"\xa0"}),(0,r.jsx)(i.mi,{children:"I"}),(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsx)(i.mi,{children:"f"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsx)(i.mi,{children:"r"}),(0,r.jsx)(i.mi,{children:"m"}),(0,r.jsx)(i.mi,{children:"a"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mi,{children:"o"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"n"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"x"}),(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"c"}),(0,r.jsx)(i.mi,{children:"t"}),(0,r.jsx)(i.mi,{children:"e"}),(0,r.jsx)(i.mi,{children:"d"})]})]})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"Retention\\ Score = \\frac{Relevant\\ Information_{retrieved}}{Relevant\\ Information_{expected}}"})]})})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"These formulations allow assessing not only memory consumption but also the trade-offs between efficiency, recall, and contextual stability."}),"\n",(0,r.jsx)(i.h2,{id:"variants",children:"Variants"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Memory Usage:"})," Quantifies memory consumption during model execution (in GB or percentage of GPU utilization)."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Memorization Ratio:"})," Captures how much of the output content stems from memorized examples rather than generalization."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Multi-Session Memory Retention:"})," Measures a model\u2019s ability to retain and reuse information over extended interactions or context windows.",(0,r.jsx)(i.br,{}),"\n","Each of these variants can be combined with runtime or accuracy metrics to form efficiency composites (for instance, tokens per second per GB)."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"applications-in-software-engineering",children:"Applications in Software Engineering"}),"\n",(0,r.jsxs)(i.p,{children:["Memory metrics have been applied across multiple software engineering benchmarks.",(0,r.jsx)(i.br,{}),"\n","Benchmarks such as EffiBench and Mercury use memory usage to analyze the balance between performance and efficiency in code generation tasks.",(0,r.jsx)(i.br,{}),"\n","Other works, like LoCoBench, explore multi-session memory retention to assess long-context utilization in systems that process extended code or documentation sequences.",(0,r.jsx)(i.br,{}),"\n","Meanwhile, datasets such as CIFAR10 and ImageNet1k employ the memorization ratio to quantify how much generative models reproduce or recall previously seen patterns rather than generalize.",(0,r.jsx)(i.br,{}),"\n","Together, these applications illustrate how memory evaluation provides insights into both the computational and cognitive efficiency of LLM-based systems."]}),"\n",(0,r.jsx)(i.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,r.jsx)(i.p,{children:"In LLM-based systems, memory metrics reflect a trade-off between model capacity and hardware constraints:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"High memory usage may signal inefficient architecture or unoptimized caching."}),"\n",(0,r.jsx)(i.li,{children:"Low memory efficiency can indicate wasteful computation or redundant token processing."}),"\n",(0,r.jsx)(i.li,{children:"Memorization ratio highlights risks of overfitting or data leakage, relevant for model reliability."}),"\n",(0,r.jsx)(i.li,{children:"Memory retention is critical for long-context reasoning and conversational consistency."}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"These metrics serve as fundamental indicators of system-level optimization and scalability in both research and applied settings."}),"\n",(0,r.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["Luo, S., Xu, M., Zhang, W., & Zhou, A. (2025). HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading. arXiv preprint. ",(0,r.jsx)(i.a,{href:"https://arxiv.org/abs/2502.12574",children:"https://arxiv.org/abs/2502.12574"})]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"additional-references-in-dataset",children:"Additional References in Dataset"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"10,\xa021,\xa054,\xa067"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,c.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>t});var s=n(6540);const r={},c=s.createContext(r);function l(e){const i=s.useContext(c);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function t(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(c.Provider,{value:i},e.children)}}}]);