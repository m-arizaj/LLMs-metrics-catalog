"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1632],{380:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>t,default:()=>o,frontMatter:()=>c,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"metrics/efficiency/perplexity","title":"Perplexity","description":"Introduction","source":"@site/docs/metrics/efficiency/perplexity.md","sourceDirName":"metrics/efficiency","slug":"/metrics/efficiency/perplexity","permalink":"/LLMs-metrics-catalog/metrics/efficiency/perplexity","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"perplexity","title":"Perplexity","sidebar_label":"Perplexity"},"sidebar":"docsSidebar","previous":{"title":"Memory Metrics","permalink":"/LLMs-metrics-catalog/metrics/efficiency/memory"},"next":{"title":"CPU Utilization","permalink":"/LLMs-metrics-catalog/metrics/efficiency/cpu"}}');var r=n(4848),l=n(8453);const c={id:"perplexity",title:"Perplexity",sidebar_label:"Perplexity"},t=void 0,a={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Mathematical Definition",id:"mathematical-definition",level:2},{value:"Relationship with Bits-per-Byte",id:"relationship-with-bits-per-byte",level:2},{value:"Variants",id:"variants",level:2},{value:"Interpretation",id:"interpretation",level:2},{value:"Limitations",id:"limitations",level:2},{value:"References",id:"references",level:2},{value:"Additional References in Database",id:"additional-references-in-database",level:2}];function h(e){const i={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",mspace:"mspace",msub:"msub",msup:"msup",mtext:"mtext",munderover:"munderover",ol:"ol",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(i.p,{children:["Perplexity (",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"L"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL"})]})})}),") and Bits-per-Byte (",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"B"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"BPB"})]})})}),") are fundamental metrics for evaluating language modeling quality in both natural language processing and code generation tasks. They quantify how well a model predicts sequences of tokens or bytes. That is, how \u201csurprised\u201d the model is by the observed data. A lower perplexity or BPB indicates better predictive performance and higher language modeling efficiency.",(0,r.jsx)(i.br,{}),"\n","While Perplexity is standard for token-based models (e.g., words or subwords), BPB is more suitable for byte- or character-level modeling, often used in compression-based evaluations and cross-domain LLM comparisons."]}),"\n",(0,r.jsx)(i.h2,{id:"mathematical-definition",children:"Mathematical Definition"}),"\n",(0,r.jsxs)(i.p,{children:["For a corpus of ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsx)(i.mrow,{children:(0,r.jsx)(i.mi,{children:"N"})}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"N"})]})})})," tokens with probabilities ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:":"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mn,{children:"1"})]})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"P(w_i|w_{1:i-1})"})]})})}),", Perplexity is defined as:"]}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mi,{children:"exp"}),(0,r.jsx)(i.mo,{children:"\u2061"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mo,{fence:"true",children:"("}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsxs)(i.munderover,{children:[(0,r.jsx)(i.mo,{children:"\u2211"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mn,{children:"1"})]}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsx)(i.mi,{children:"ln"}),(0,r.jsx)(i.mo,{children:"\u2061"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:":"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mn,{children:"1"})]})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"}),(0,r.jsx)(i.mo,{fence:"true",children:")"})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\ln P(w_i|w_{1:i-1})\\right)"})]})})}),"\n",(0,r.jsx)(i.p,{children:"or equivalently in base 2:"}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.msup,{children:[(0,r.jsx)(i.mn,{children:"2"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL = 2^{H(p)}"})]})})}),"\n",(0,r.jsxs)(i.p,{children:["where ",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"H"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"p"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"H(p)"})]})})})," is the cross-entropy of the model over the sequence.",(0,r.jsx)(i.br,{}),"\n","Intuitively, it measures the average branching factor, the number of equally likely choices the model considers at each step."]}),"\n",(0,r.jsx)(i.h2,{id:"relationship-with-bits-per-byte",children:"Relationship with Bits-per-Byte"}),"\n",(0,r.jsx)(i.p,{children:"Bits-per-Byte measures the average information content required to encode each byte (or character) under the model\u2019s probability distribution:"}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsxs)(i.munderover,{children:[(0,r.jsx)(i.mo,{children:"\u2211"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mn,{children:"1"})]}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"log"}),(0,r.jsx)(i.mo,{children:"\u2061"})]}),(0,r.jsx)(i.mn,{children:"2"})]}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:":"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mn,{children:"1"})]})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"BPB = \\frac{1}{N} \\sum_{i=1}^{N} -\\log_2 P(w_i|w_{1:i-1})"})]})})}),"\n",(0,r.jsxs)(i.p,{children:["It represents the entropy in bits, directly comparable across datasets and languages.",(0,r.jsx)(i.br,{}),"\n","The relationship between the two metrics is given by:"]}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.msup,{children:[(0,r.jsx)(i.mn,{children:"2"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"B"})]})]}),(0,r.jsx)(i.mspace,{width:"1em"}),(0,r.jsx)(i.mtext,{children:"and"}),(0,r.jsx)(i.mspace,{width:"1em"}),(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"B"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsxs)(i.msub,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"log"}),(0,r.jsx)(i.mo,{children:"\u2061"})]}),(0,r.jsx)(i.mn,{children:"2"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL = 2^{BPB} \\quad \\text{and} \\quad BPB = \\log_2(PPL)"})]})})}),"\n",(0,r.jsx)(i.p,{children:"This equivalence makes BPB a linear, interpretable form of Perplexity \u2014 expressing model uncertainty in bits instead of exponential space."}),"\n",(0,r.jsx)(i.h2,{id:"variants",children:"Variants"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:["Unigram-Normalized Perplexity (",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mi,{children:"u"})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL_u"})]})})}),")",(0,r.jsx)(i.br,{}),"\n","Proposed by Roh et al. (2020) to correct bias from vocabulary size:"]}),"\n",(0,r.jsx)(i.span,{className:"katex",children:(0,r.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(i.semantics,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"L"}),(0,r.jsx)(i.mi,{children:"u"})]}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mi,{children:"exp"}),(0,r.jsx)(i.mo,{children:"\u2061"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mo,{fence:"true",children:"("}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsxs)(i.munderover,{children:[(0,r.jsx)(i.mo,{children:"\u2211"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"="}),(0,r.jsx)(i.mn,{children:"1"})]}),(0,r.jsx)(i.mi,{children:"N"})]}),(0,r.jsx)(i.mi,{children:"ln"}),(0,r.jsx)(i.mo,{children:"\u2061"}),(0,r.jsxs)(i.mfrac,{children:[(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mi,{mathvariant:"normal",children:"\u2223"}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mn,{children:"1"}),(0,r.jsx)(i.mo,{children:":"}),(0,r.jsx)(i.mi,{children:"i"}),(0,r.jsx)(i.mo,{children:"\u2212"}),(0,r.jsx)(i.mn,{children:"1"})]})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]}),(0,r.jsxs)(i.mrow,{children:[(0,r.jsx)(i.mi,{children:"P"}),(0,r.jsx)(i.mo,{stretchy:"false",children:"("}),(0,r.jsxs)(i.msub,{children:[(0,r.jsx)(i.mi,{children:"w"}),(0,r.jsx)(i.mi,{children:"i"})]}),(0,r.jsx)(i.mo,{stretchy:"false",children:")"})]})]}),(0,r.jsx)(i.mo,{fence:"true",children:")"})]})]}),(0,r.jsx)(i.annotation,{encoding:"application/x-tex",children:"PPL_u = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\ln \\frac{P(w_i|w_{1:i-1})}{P(w_i)}\\right)"})]})})}),"\n",(0,r.jsxs)(i.p,{children:["This normalization measures the ",(0,r.jsx)(i.em,{children:"information gain"})," of the model over a unigram baseline, improving cross-dataset comparability."]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:"Token-Level vs Byte-Level Evaluation"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Token-level PPL is computed over word or subword sequences."}),"\n",(0,r.jsx)(i.li,{children:"Byte-level BPB evaluates the model\u2019s performance independently of tokenization, making it ideal for multilingual or mixed-data benchmarks."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"Low Perplexity or BPB:"})," Model confidently predicts upcoming tokens (better generalization)."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"High Perplexity or BPB:"})," Model exhibits uncertainty or overfitting."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"PPL Sensitivity:"})," Strongly influenced by vocabulary size and tokenization scheme."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.em,{children:"BPB Advantage:"})," Vocabulary-agnostic and ideal for cross-domain comparison."]}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"Meister & Cotterell (2021) emphasizes that while Perplexity remains central, it does not capture higher-level linguistic structure (e.g., syntactic diversity or semantic consistency). Thus, it should be complemented by distributional or semantic measures when evaluating LLMs."}),"\n",(0,r.jsx)(i.h2,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Sensitive to tokenization and corpus preprocessing."}),"\n",(0,r.jsx)(i.li,{children:"Not a direct indicator of semantic correctness or task-specific usefulness."}),"\n",(0,r.jsx)(i.li,{children:"Difficult to compare across languages without normalization."}),"\n",(0,r.jsx)(i.li,{children:"May favor large models with higher capacity but poor calibration."}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Roh, J., Oh, S.-H., & Lee, S.-Y. (2020)."})," Unigram-Normalized Perplexity as a Language Model Performance Measure with Different Vocabulary Sizes.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://arxiv.org/abs/2011.13220",children:"https://arxiv.org/abs/2011.13220"})]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.em,{children:"Meister, C., & Cotterell, R. (2021)."})," Language Model Evaluation Beyond Perplexity.",(0,r.jsx)(i.br,{}),"\n",(0,r.jsx)(i.a,{href:"https://arxiv.org/abs/2106.00085",children:"https://arxiv.org/abs/2106.00085"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"additional-references-in-database",children:"Additional References in Database"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"2,\xa04,\xa028"}),"\n"]})]})}function o(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>c,x:()=>t});var s=n(6540);const r={},l=s.createContext(r);function c(e){const i=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function t(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),s.createElement(l.Provider,{value:i},e.children)}}}]);