"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[2412],{2881:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"metrics/efficiency/cpu","title":"CPU Utilization","description":"Introduction","source":"@site/docs/metrics/efficiency/cpu.md","sourceDirName":"metrics/efficiency","slug":"/metrics/efficiency/cpu","permalink":"/LLMs-metrics-catalog/metrics/efficiency/cpu","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"cpu","title":"CPU Utilization","sidebar_label":"CPU Utilization"},"sidebar":"docsSidebar","previous":{"title":"Perplexity","permalink":"/LLMs-metrics-catalog/metrics/efficiency/perplexity"},"next":{"title":"Composite","permalink":"/LLMs-metrics-catalog/metrics/architectural/composite"}}');var r=i(4848),s=i(8453);const c={id:"cpu",title:"CPU Utilization",sidebar_label:"CPU Utilization"},a=void 0,l={},o=[{value:"Introduction",id:"introduction",level:2},{value:"Formula and Computation",id:"formula-and-computation",level:2},{value:"Evaluation Context",id:"evaluation-context",level:2},{value:"Application in LLM Inference",id:"application-in-llm-inference",level:2},{value:"Example Metrics (4th Gen Intel Xeon Scalable Processor)",id:"example-metrics-4th-gen-intel-xeon-scalable-processor",level:3},{value:"Interpretation",id:"interpretation",level:2},{value:"Relation to Other Metrics",id:"relation-to-other-metrics",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",annotation:"annotation",br:"br",em:"em",h2:"h2",h3:"h3",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["CPU Utilization is a performance and efficiency metric that quantifies how effectively the computational resources of a Central Processing Unit (CPU) are used during Large Language Model inference or code execution.",(0,r.jsx)(n.br,{}),"\n","In the context of LLM evaluation, it measures the percentage of CPU cycles actively processing tokens versus idle or waiting states, serving as a critical indicator of inference throughput, latency, and energy efficiency."]}),"\n",(0,r.jsxs)(n.p,{children:["This metric was emphasized by Chen et al. (2025) in A Survey on Evaluating Large Language Models in Code Generation Tasks, where CPU utilization, execution time, and energy consumption were introduced as part of performance and efficiency evaluation metrics within EffiBench and Mercury frameworks.",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.em,{children:"Ditto et al. (2024)"})," later operationalized this metric in Inference Acceleration for Large Language Models on CPUs, demonstrating its role in optimizing LLM inference performance through parallelization, batching, and NUMA node isolation."]}),"\n",(0,r.jsx)(n.h2,{id:"formula-and-computation",children:"Formula and Computation"}),"\n",(0,r.jsx)(n.p,{children:"CPU utilization is defined as the proportion of time the CPU spends executing active processes relative to the total available processing time:"}),"\n",(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsxs)(n.mrow,{children:[(0,r.jsx)(n.mtext,{children:"CPU\xa0Utilization"}),(0,r.jsx)(n.mo,{stretchy:"false",children:"("}),(0,r.jsx)(n.mi,{mathvariant:"normal",children:"%"}),(0,r.jsx)(n.mo,{stretchy:"false",children:")"}),(0,r.jsx)(n.mo,{children:"="}),(0,r.jsxs)(n.mfrac,{children:[(0,r.jsxs)(n.msub,{children:[(0,r.jsx)(n.mi,{children:"T"}),(0,r.jsx)(n.mtext,{children:"active"})]}),(0,r.jsxs)(n.msub,{children:[(0,r.jsx)(n.mi,{children:"T"}),(0,r.jsx)(n.mtext,{children:"total"})]})]}),(0,r.jsx)(n.mo,{children:"\xd7"}),(0,r.jsx)(n.mn,{children:"100"})]}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\text{CPU Utilization} (\\%) = \\frac{T_\\text{active}}{T_\\text{total}} \\times 100"})]})})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsxs)(n.msub,{children:[(0,r.jsx)(n.mi,{children:"T"}),(0,r.jsx)(n.mtext,{children:"active"})]})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"T_\\text{active}"})]})})})," = time spent performing active computation (e.g., matrix multiplications, token generation)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.span,{className:"katex",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsxs)(n.msub,{children:[(0,r.jsx)(n.mi,{children:"T"}),(0,r.jsx)(n.mtext,{children:"total"})]})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"T_\\text{total}"})]})})})," = total wall-clock time of the inference session."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For LLM-specific evaluations, CPU utilization is often correlated with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Processed Token Throughput"})," (tokens/sec processed)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Generated Token Throughput"})," (tokens/sec emitted)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Execution Time"})," (total inference duration)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Energy per 1k Tokens"})," (Watt-hours per 1,000 tokens)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These metrics together describe both computational efficiency and environmental impact during LLM inference."}),"\n",(0,r.jsx)(n.h2,{id:"evaluation-context",children:"Evaluation Context"}),"\n",(0,r.jsxs)(n.p,{children:["CPU-based inference metrics are typically integrated into ",(0,r.jsx)(n.em,{children:"EffiBench"})," and ",(0,r.jsx)(n.em,{children:"Mercury"}),", two major benchmarking frameworks for LLM-generated code efficiency:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Benchmark"}),(0,r.jsx)(n.th,{children:"Metric Components"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"EffiBench (Huang et al., 2024)"})}),(0,r.jsx)(n.td,{children:"Execution Time, Memory Usage, CPU Utilization, Code Complexity"}),(0,r.jsx)(n.td,{children:"Assesses code efficiency and runtime behavior of LLM outputs in standardized environments."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"Mercury (Du et al., 2024)"})}),(0,r.jsx)(n.td,{children:"Execution Time, Memory Usage, Energy Consumption, CPU Utilization"}),(0,r.jsx)(n.td,{children:"Focuses on real execution performance, measuring total compute and energy cost."})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Both frameworks use CPU utilization as a measure of how efficiently LLM-generated code or inference processes use available computational resources."}),"\n",(0,r.jsx)(n.h2,{id:"application-in-llm-inference",children:"Application in LLM Inference"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Ditto et al. (2024)"})," propose a practical implementation for CPU-based inference metrics using ",(0,r.jsx)(n.em,{children:"Intel\xae Xeon\xae Scalable Processors"}),".",(0,r.jsx)(n.br,{}),"\n","Their study introduced the Bud Inference Engine, a parallelized framework achieving up to ",(0,r.jsx)(n.em,{children:"22\xd7 improvement"})," in generated tokens/sec on CPUs compared to baseline runs, with ",(0,r.jsx)(n.em,{children:"~49% lower power consumption"})," versus GPU-based inference."]}),"\n",(0,r.jsx)(n.h3,{id:"example-metrics-4th-gen-intel-xeon-scalable-processor",children:"Example Metrics (4th Gen Intel Xeon Scalable Processor)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"vCPU"}),(0,r.jsx)(n.th,{children:"Processed Tokens/s"}),(0,r.jsx)(n.th,{children:"Generated Tokens/s"}),(0,r.jsx)(n.th,{children:"CPU Utilization"}),(0,r.jsx)(n.th,{children:"Power (1k tokens)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"bigcode/starcoderbase-3B"}),(0,r.jsx)(n.td,{children:"32"}),(0,r.jsx)(n.td,{children:"992.36"}),(0,r.jsx)(n.td,{children:"167.58"}),(0,r.jsx)(n.td,{children:"\u2248 85 %"}),(0,r.jsx)(n.td,{children:"613 W"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CodeLlama-7B (hf)"}),(0,r.jsx)(n.td,{children:"32"}),(0,r.jsx)(n.td,{children:"593.74"}),(0,r.jsx)(n.td,{children:"93.69"}),(0,r.jsx)(n.td,{children:"\u2248 82 %"}),(0,r.jsx)(n.td,{children:"\u2014"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLaMA-2-7B (hf)"}),(0,r.jsx)(n.td,{children:"NUMA \xd7 4"}),(0,r.jsx)(n.td,{children:"1852.32"}),(0,r.jsx)(n.td,{children:"305.30"}),(0,r.jsx)(n.td,{children:"> 90 %"}),(0,r.jsx)(n.td,{children:"\u2014"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Higher CPU utilization correlates with improved throughput and reduced latency, particularly when inference workloads are distributed across NUMA nodes and optimized with AVX/AMX instructions."}),"\n",(0,r.jsx)(n.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,r.jsx)(n.p,{children:"High CPU utilization in LLM inference indicates:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Efficient batching and memory management (reduced fragmentation)."}),"\n",(0,r.jsx)(n.li,{children:"Balanced load across CPU cores (parallel token generation)."}),"\n",(0,r.jsx)(n.li,{children:"Lower energy cost per token generated."}),"\n",(0,r.jsx)(n.li,{children:"Increased throughput (tokens/sec) without significant latency penalties.\r\nHowever, over-utilization (>95 %) may indicate CPU saturation, potentially causing thermal throttling or degraded latency under concurrent workloads."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"relation-to-other-metrics",children:"Relation to Other Metrics"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metric"}),(0,r.jsx)(n.th,{children:"Focus"}),(0,r.jsx)(n.th,{children:"Relationship to CPU Utilization"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"Execution Time"})}),(0,r.jsx)(n.td,{children:"Total time to process an input"}),(0,r.jsx)(n.td,{children:"Inversely proportional under fixed load"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"Memory Usage"})}),(0,r.jsx)(n.td,{children:"RAM consumption during inference"}),(0,r.jsx)(n.td,{children:"Often correlated; inefficient memory allocation lowers CPU utilization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"Energy Consumption"})}),(0,r.jsx)(n.td,{children:"Power cost per operation"}),(0,r.jsx)(n.td,{children:"Positively correlated; higher utilization can mean better performance per watt"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.em,{children:"Throughput (tokens/s)"})}),(0,r.jsx)(n.td,{children:"Number of tokens processed per second"}),(0,r.jsx)(n.td,{children:"Directly proportional; core efficiency indicator"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Quantifies ",(0,r.jsx)(n.em,{children:"hardware efficiency"})," beyond correctness metrics (e.g., Pass@k, BLEU)."]}),"\n",(0,r.jsxs)(n.li,{children:["Enables ",(0,r.jsx)(n.em,{children:"sustainable evaluation"}),", connecting compute usage to environmental cost."]}),"\n",(0,r.jsxs)(n.li,{children:["Offers a ",(0,r.jsx)(n.em,{children:"hardware-agnostic benchmark"})," for CPU-based deployments (Xeon, EPYC, ARM)."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Results depend heavily on hardware architecture (e.g., number of cores, memory bandwidth)."}),"\n",(0,r.jsx)(n.li,{children:"High utilization does not always translate to low latency in small-batch settings."}),"\n",(0,r.jsx)(n.li,{children:"Not directly comparable to GPU metrics due to architectural differences (parallel vs SIMD)."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Ditto P. S., Jithin V. G., & Adarsh M. S. (2024)."})," Inference Acceleration for Large Language Models on CPUs.\r\n",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2406.07553",children:"https://arxiv.org/abs/2406.07553"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., et al. (2025)."})," A Survey on Evaluating Large Language Models in Code Generation Tasks.\r\n",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2408.16498",children:"https://arxiv.org/abs/2408.16498"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Huang, D., Zhang, J. M., Qing, Y., & Cui, H. (2024)."})," EffiBench: Benchmarking the Efficiency of Automatically Generated Code.\r\n",(0,r.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2402.02037",children:"https://doi.org/10.48550/arXiv.2402.02037"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Du, M., Luu, A. T., Ji, B., & Ng, S. K. (2024)."})," Mercury: A Code Efficiency Benchmark for Code Large Language Models. ",(0,r.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2402.07844",children:"https://doi.org/10.48550/arXiv.2402.07844"})]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function c(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);