"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[8503],{3876:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"metrics/human/usefulness-score","title":"Usefulness Score","description":"Definition","source":"@site/docs/metrics/human/Usefulness score.md","sourceDirName":"metrics/human","slug":"/metrics/human/usefulness-score","permalink":"/LLMs-metrics-catalog/metrics/human/usefulness-score","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"usefulness-score","title":"Usefulness Score","sidebar_label":"Usefulness Score"},"sidebar":"docsSidebar","previous":{"title":"Subjective Score","permalink":"/LLMs-metrics-catalog/metrics/human/subjective-score"},"next":{"title":"User-Related Metrics","permalink":"/LLMs-metrics-catalog/metrics/human/user-related-metrics"}}');var r=n(4848),t=n(8453);const l={id:"usefulness-score",title:"Usefulness Score",sidebar_label:"Usefulness Score"},a=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (GrC:\\Users\\Daniel\\Downloads\\VSC\\Metricas\\12.pdfading Scale)",id:"formula-grcusersdanieldownloadsvscmetricas12pdfading-scale",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const s={a:"a",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.h2,{id:"definition",children:"Definition"}),"\n",(0,r.jsxs)(s.p,{children:["The ",(0,r.jsx)(s.strong,{children:"Usefulness Score"})," is a human-centric, subjective evaluation metric designed to assess the quality of generated code snippets based on their helpfulness to a human developer."]}),"\n",(0,r.jsx)(s.p,{children:"It is not an automated metric but rather a rubric for human annotation, where experienced developers grade a generated code snippet on a 0-4 discrete scale. This score is a key metric for measuring human preference alignment."}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"formula-grcusersdanieldownloadsvscmetricas12pdfading-scale",children:"Formula (GrC:\\Users\\Daniel\\Downloads\\VSC\\Metricas\\12.pdfading Scale)"}),"\n",(0,r.jsxs)(s.p,{children:["The metric is defined by the following 5-point scale, as detailed in the ",(0,r.jsx)(s.code,{children:"ICE-Score"})," paper's appendix:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Score 0 (Totally Useless):"}),' "Snippet is not at all helpful, it is irrelevant to the problem."']}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Score 1 (Slightly Helpful):"}),' "Snippet is slightly helpful, it contains information relevant to the problem, but it is easier to write the solution from scratch."']}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Score 2 (Somewhat Helpful):"}),' "Snippet is somewhat helpful, it requires significant changes (compared to the size of the snippet), but is still useful."']}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Score 3 (Helpful / Almost Useful):"}),' "Snippet is helpful, but needs to be slightly changed to solve the problem."']}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Score 4 (Very Helpful):"}),' "Snippet is very helpful, it solves the problem."']}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"purpose",children:"Purpose"}),"\n",(0,r.jsx)(s.p,{children:"The purpose of the Usefulness Score is to capture a human developer's judgment of a code snippet's quality, which is often missed by traditional token-matching metrics like BLEU. It directly measures how well a generated snippet satisfies a user's requirements and how much effort would be saved (or wasted) by using it. It serves as a human-preference ground truth for training and evaluating other automated metrics (like ICE-Score)."}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"domains",children:"Domains"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Software Engineering"}),"\n",(0,r.jsx)(s.li,{children:"Code Generation"}),"\n",(0,r.jsx)(s.li,{children:"Human Preference Alignment"}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"advantages",children:"Advantages"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Human-Aligned:"})," By definition, this metric has a high correlation with human judgment and preference, which token-matching metrics like BLEU lack."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Captures Semantics:"})," It assesses the ",(0,r.jsx)(s.em,{children:"semantic logic"})," and ",(0,r.jsx)(s.em,{children:"practical value"})," of the code, not just its textual similarity to a single reference answer."]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost and Scalability:"}),' Relies on manual grading from "experienced software developers", which is expensive, time-consuming, and difficult to scale compared to automated metrics.']}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Subjectivity:"}),' As a human-based score, it can have variability. The paper notes that even the human-written reference code in CoNaLa only achieved an average "Usefulness" score of 3.4 out of 4.']}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"key-references",children:"Key References"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["Zhuo, T. Y. (2024). ",(0,r.jsx)(s.em,{children:"ICE-Score: Instructing Large Language Models to Evaluate Code"}),". ",(0,r.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.2304.14317",children:"https://doi.org/10.48550/arXiv.2304.14317"})]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["Evtikhiev, M., Bogomolov, E., Sokolov, Y., & Bryksin, T. (2023). ",(0,r.jsx)(s.em,{children:"Out of the bleu: how should we assess quality of the code generation models?"}),"\r\n",(0,r.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.2208.03133",children:"https://doi.org/10.48550/arXiv.2208.03133"})]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["Yin, P., Deng, B., Chen, E., Vasilescu, B., & Neubig, G. (2018). ",(0,r.jsx)(s.em,{children:"Learning to mine aligned code and natural language pairs from stack overflow"}),".\r\n",(0,r.jsx)(s.a,{href:"https://doi.org/10.48550/arXiv.1805.08949",children:"https://doi.org/10.48550/arXiv.1805.08949"})]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"(Excel Data: Paper 12)"}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>l,x:()=>a});var i=n(6540);const r={},t=i.createContext(r);function l(e){const s=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);