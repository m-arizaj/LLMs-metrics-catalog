"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[1250],{5238:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"metrics/creativity/readability","title":"Readability","description":"Definition","source":"@site/docs/metrics/creativity/readability.md","sourceDirName":"metrics/creativity","slug":"/metrics/creativity/readability","permalink":"/LLMs-metrics-catalog/metrics/creativity/readability","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"readability","title":"Readability","sidebar_label":"Readability"},"sidebar":"docsSidebar","previous":{"title":"Rarity Score","permalink":"/LLMs-metrics-catalog/metrics/creativity/rarity-score"},"next":{"title":"Vendi Score","permalink":"/LLMs-metrics-catalog/metrics/creativity/vendi-score"}}');var a=n(4848),r=n(8453);const s={id:"readability",title:"Readability",sidebar_label:"Readability"},l=void 0,o={},c=[{value:"Definition",id:"definition",level:2},{value:"Formula (General Idea)",id:"formula-general-idea",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Domains",id:"domains",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Limitations",id:"limitations",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const i={a:"a",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h2,{id:"definition",children:"Definition"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Readability"})," is a criterion used to evaluate the ",(0,a.jsx)(i.strong,{children:"Linguistic Quality"})," of text generated by Large Language Models (LLMs)."]}),"\n",(0,a.jsxs)(i.p,{children:["In the context of the source paper, Readability is not treated as a standalone automatic metric but as a key component of ",(0,a.jsx)(i.strong,{children:"Fluency"})," , which is a fundamental criterion for the ",(0,a.jsx)(i.strong,{children:"Human Evaluation"})," of LLMs. Fluency assesses the model's ability to produce content that flows smoothly. A text with high readability is grammatically correct, maintains a consistent tone and style, and ensures a seamless user experience."]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"formula-general-idea",children:"Formula (General Idea)"}),"\n",(0,a.jsxs)(i.p,{children:["In the context of the source paper (Chang et al., 2024), Readability is treated as a component of ",(0,a.jsx)(i.strong,{children:"Fluency"}),", a criterion used in ",(0,a.jsx)(i.strong,{children:"Human Evaluation"})," rather than a specific automatic formula."]}),"\n",(0,a.jsx)(i.p,{children:"It is a qualitative assessment made by human evaluators who judge the quality of the generated text. This contrasts with automatic metrics like ROUGE or F1-Score which are calculated based on token overlap."}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"purpose",children:"Purpose"}),"\n",(0,a.jsx)(i.p,{children:"The primary purpose of evaluating Readability (as part of Fluency) is to assess the quality of the generated text from a human perspective. This evaluation aims to:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:'Ensure the text "flows smoothly".'}),"\n",(0,a.jsx)(i.li,{children:'Verify that the text is "grammatically correct".'}),"\n",(0,a.jsx)(i.li,{children:'Confirm the model maintains a "consistent tone and style".'}),"\n",(0,a.jsx)(i.li,{children:'Measure if the text provides a "seamless user experience".'}),"\n",(0,a.jsx)(i.li,{children:'Determine if the model "avoids awkward expressions and abrupt shifts in language or topic".'}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"domains",children:"Domains"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"General LLM Evaluation"}),"\n",(0,a.jsx)(i.li,{children:"Human Evaluation of LLMs"}),"\n",(0,a.jsx)(i.li,{children:"Open-ended generation tasks"}),"\n",(0,a.jsx)(i.li,{children:"Open-domain conversations"}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"More Accurate Feedback:"}),' As a human evaluation metric, it is "closer to the actual application scenario" and provides "more comprehensive and accurate feedback" than automatic metrics.']}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"More Reliable for Generation:"}),' It is considered "more reliable" for evaluating open-generation tasks where standard reference-based metrics (like ROUGE or BLEU) are not suitable.']}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Captures Nuance:"})," It assesses subtle linguistic qualities like smooth flow, tone, and style, which automatic formulas often fail to capture."]}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"limitations",children:"Limitations"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"High Variance:"}),' As a human-judged metric, it can suffer from "high variance and instability".']}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Subjectivity:"}),' Evaluation can be influenced by "cultural and individual differences" among the human evaluators.']}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Requires Rigor:"}),' To be reliable, the evaluation process requires "thoughtful attention" to crucial factors, such as the number of evaluators, their expertise level, and clear evaluation rubrics.']}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.h2,{id:"key-references",children:"Key References"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Primary Source (Application):"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2024). ",(0,a.jsx)(i.em,{children:"A Survey on Evaluation of Large Language Models"}),". ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.2307.03109",children:"https://doi.org/10.48550/arXiv.2307.03109"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Cited for Fluency/Readability Definition:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["Van Der Lee, C., Gatt, A., Van Miltenburg, E., Wubben, S., & Krahmer, E. (2019). ",(0,a.jsx)(i.em,{children:"Best practices for the human evaluation of automatically generated text"}),". Proceedings of the 12th International Conference on Natural Language Generation. ",(0,a.jsx)(i.a,{href:"https://doi.org/10.18653/v1/W19-8643",children:"https://doi.org/10.18653/v1/W19-8643"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>l});var t=n(6540);const a={},r=t.createContext(a);function s(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);