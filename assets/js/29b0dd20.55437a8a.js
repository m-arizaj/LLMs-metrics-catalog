"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[7228],{344:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"metrics/logical/exam","title":"EXAM","description":"Introduction","source":"@site/docs/metrics/logical/exam.md","sourceDirName":"metrics/logical","slug":"/metrics/logical/exam","permalink":"/LLMs-metrics-catalog/metrics/logical/exam","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"exam","title":"EXAM","sidebar_label":"EXAM"},"sidebar":"docsSidebar","previous":{"title":"Difference Automata","permalink":"/LLMs-metrics-catalog/metrics/logical/difference-automata"},"next":{"title":"Equivalent","permalink":"/LLMs-metrics-catalog/metrics/logical/equivalent"}}');var s=i(4848),a=i(8453);const r={id:"exam",title:"EXAM",sidebar_label:"EXAM"},o=void 0,l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"1. EXAM (Explainable Automated Debugging)",id:"1-exam-explainable-automated-debugging",level:2},{value:"Definition",id:"definition",level:3},{value:"Purpose and Applications",id:"purpose-and-applications",level:3},{value:"Metrics Used within EXAM",id:"metrics-used-within-exam",level:3},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"EXAM (Explainable Automated Debugging)"})," is an evaluation benchmark used in software engineering to measure a model's capability in ",(0,s.jsx)(n.strong,{children:"Fault Localization"})," . Introduced by ",(0,s.jsx)(n.strong,{children:"Tran et al. (2022)"}),' , it provides a standardized framework for assessing how well a model can debug code, specifically its ability to "identify the buggy code lines" .']}),"\n",(0,s.jsxs)(n.p,{children:["Unlike metrics that evaluate code generation (like BLEU), EXAM focuses on code ",(0,s.jsx)(n.em,{children:"verification"})," and ",(0,s.jsx)(n.em,{children:"augmentation"}),", specifically debugging . To quantify performance on this task, the EXAM benchmark uses specific metrics such as Top-N Accuracy and Mean Average Rank (MAR) ."]}),"\n",(0,s.jsx)(n.h2,{id:"1-exam-explainable-automated-debugging",children:"1. EXAM (Explainable Automated Debugging)"}),"\n",(0,s.jsx)(n.h3,{id:"definition",children:"Definition"}),"\n",(0,s.jsxs)(n.p,{children:['EXAM is a benchmark consisting of a set of "288 Python programs with 12 error types" . The primary task for a model evaluated on this benchmark is ',(0,s.jsx)(n.strong,{children:"fault localization"}),': the model must "identify the buggy code lines" within these programs .']}),"\n",(0,s.jsx)(n.h3,{id:"purpose-and-applications",children:"Purpose and Applications"}),"\n",(0,s.jsxs)(n.p,{children:["The purpose of EXAM is to evaluate models' capabilities in ",(0,s.jsx)(n.strong,{children:"code debugging"})," . It is used to quantitatively measure performance in localizing errors in Python programs ."]}),"\n",(0,s.jsx)(n.h3,{id:"metrics-used-within-exam",children:"Metrics Used within EXAM"}),"\n",(0,s.jsx)(n.p,{children:"The EXAM benchmark uses the following metrics to score a model's performance :"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top-N Accuracy:"})," Measures if the correct buggy line is found among the model's Top-N most likely predictions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Mean Average Rank (MAR):"})," Measures the average rank (position) of the correct buggy line in the model's prediction list . A lower MAR indicates better performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Chen, X., Hu, X., Huang, Y. et al. Deep learning-based software engineering: progress, challenges, and opportunities. Sci. China Inf. Sci. 68, 111102 (2025). ",(0,s.jsx)(n.a,{href:"https://doi-org.ezproxy.uniandes.edu.co/10.1007/s11432-023-4127-5",children:"https://doi-org.ezproxy.uniandes.edu.co/10.1007/s11432-023-4127-5"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Tran, L. H., Le, T., & Nguyen, A. T. (2022). ",(0,s.jsx)(n.em,{children:"EXAM: a benchmark for explainable automated debugging"}),". In: Proc IEEE/ACM 44th International Conference on Software Engineering."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Zhang, Z., Chen, J., Wu, D., et al. (2025). ",(0,s.jsx)(n.em,{children:"A survey of large language models for code: generation, augmentation, and verification"}),". Sci Sin Inform."]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);