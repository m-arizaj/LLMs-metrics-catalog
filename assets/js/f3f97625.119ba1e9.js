"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[2777],{7401:(e,r,s)=>{s.r(r),s.d(r,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"metrics/statistical/average","title":"Average Metrics","description":"Introduction","source":"@site/docs/metrics/statistical/average.md","sourceDirName":"metrics/statistical","slug":"/metrics/statistical/average","permalink":"/LLMs-metrics-catalog/metrics/statistical/average","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"average","title":"Average Metrics","sidebar_label":"Average Metrics"},"sidebar":"docsSidebar","previous":{"title":"Rate Metrics","permalink":"/LLMs-metrics-catalog/metrics/statistical/rate"},"next":{"title":"AST Metrics","permalink":"/LLMs-metrics-catalog/metrics/code-structural/ast-metrics"}}');var n=s(4848),i=s(8453);const a={id:"average",title:"Average Metrics",sidebar_label:"Average Metrics"},o=void 0,c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Common Average-Based Measures",id:"common-average-based-measures",level:2},{value:"1. Average Issues Found",id:"1-average-issues-found",level:3},{value:"2. Average Number of Lines of Code (LOC)",id:"2-average-number-of-lines-of-code-loc",level:3},{value:"3. Average Percentage Beats",id:"3-average-percentage-beats",level:3},{value:"4. Average Lines of Code in Context",id:"4-average-lines-of-code-in-context",level:3},{value:"5. Average Problem Words",id:"5-average-problem-words",level:3},{value:"Interpretation",id:"interpretation",level:2},{value:"References in Database",id:"references-in-database",level:2}];function d(e){const r={br:"br",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,n.jsxs)(r.p,{children:["Average metrics represent aggregate measures that summarize the behavior or structural characteristics of model outputs.",(0,n.jsx)(r.br,{}),"\n","Unlike accuracy or precision, these metrics do not evaluate correctness directly; instead, they quantify aspects such as average code size, contextual length, or the frequency of issues found.",(0,n.jsx)(r.br,{}),"\n","They are commonly used for ",(0,n.jsx)(r.em,{children:"descriptive assessment"}),", helping interpret how efficiently or consistently a model produces results across tasks."]}),"\n",(0,n.jsx)(r.h2,{id:"common-average-based-measures",children:"Common Average-Based Measures"}),"\n",(0,n.jsx)(r.h3,{id:"1-average-issues-found",children:"1. Average Issues Found"}),"\n",(0,n.jsxs)(r.p,{children:["Used in LoCoBench (2025) to quantify ",(0,n.jsx)(r.em,{children:"code quality degradation"})," by counting the mean number of architectural or logical issues identified across evaluation runs.",(0,n.jsx)(r.br,{}),"\n","A lower value indicates higher consistency and better code integrity.",(0,n.jsx)(r.br,{}),"\n","This metric serves as a proxy for the robustness and maintainability of LLM-generated code."]}),"\n",(0,n.jsx)(r.h3,{id:"2-average-number-of-lines-of-code-loc",children:"2. Average Number of Lines of Code (LOC)"}),"\n",(0,n.jsxs)(r.p,{children:["Measures the ",(0,n.jsx)(r.em,{children:"mean code length"})," produced by the model, typically across a benchmark such as MBPP (2024).",(0,n.jsx)(r.br,{}),"\n","It helps evaluate verbosity, compactness, and adherence to minimal coding standards.",(0,n.jsx)(r.br,{}),"\n","While shorter code may imply efficiency, overly compressed solutions may sacrifice readability or modularity."]}),"\n",(0,n.jsx)(r.h3,{id:"3-average-percentage-beats",children:"3. Average Percentage Beats"}),"\n",(0,n.jsxs)(r.p,{children:["Introduced in LeetCodeEval (2024), this metric reports the ",(0,n.jsx)(r.em,{children:"mean relative efficiency"})," of model-generated solutions compared to existing user submissions.",(0,n.jsx)(r.br,{}),"\n","It indicates how often model code performs above average in runtime or memory usage, expressed as a percentile."]}),"\n",(0,n.jsx)(r.h3,{id:"4-average-lines-of-code-in-context",children:"4. Average Lines of Code in Context"}),"\n",(0,n.jsxs)(r.p,{children:["Used in DS-1000 (2025) to capture the ",(0,n.jsx)(r.em,{children:"contextual size"})," of input or supporting code provided to the model.",(0,n.jsx)(r.br,{}),"\n","It reflects task complexity, influencing reasoning performance in multi-file or multi-function scenarios."]}),"\n",(0,n.jsx)(r.h3,{id:"5-average-problem-words",children:"5. Average Problem Words"}),"\n",(0,n.jsxs)(r.p,{children:["Also from DS-1000 (2025), this metric measures the ",(0,n.jsx)(r.em,{children:"average linguistic complexity"})," of problem descriptions.",(0,n.jsx)(r.br,{}),"\n","It helps estimate the cognitive load and natural language difficulty faced by the model during problem comprehension."]}),"\n",(0,n.jsx)(r.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,n.jsx)(r.p,{children:"Average metrics provide a high-level statistical perspective on LLM performance:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"They reveal trends in efficiency, verbosity, or quality across datasets."}),"\n",(0,n.jsx)(r.li,{children:"They are complementary to accuracy-based metrics, helping diagnose trade-offs between correctness and style."}),"\n",(0,n.jsx)(r.li,{children:"When used longitudinally, they indicate model stability and predictability over multiple runs or problem sets."}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["In software engineering evaluation, averages are crucial for ",(0,n.jsx)(r.em,{children:"understanding context scale"})," (e.g., average LOC or dependency length) and ",(0,n.jsx)(r.em,{children:"quantifying improvement or degradation trends"})," across model versions."]}),"\n",(0,n.jsx)(r.h2,{id:"references-in-database",children:"References in Database"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"54,\xa039,\xa038,\xa018"}),"\n"]})]})}function u(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,r,s)=>{s.d(r,{R:()=>a,x:()=>o});var t=s(6540);const n={},i=t.createContext(n);function a(e){const r=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),t.createElement(i.Provider,{value:r},e.children)}}}]);