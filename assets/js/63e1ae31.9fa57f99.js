"use strict";(self.webpackChunkll_ms_metrics_catalog=self.webpackChunkll_ms_metrics_catalog||[]).push([[3602],{7256:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"metrics/architectural/comprehensiveness","title":"Comprehensiveness Score","description":"Introduction","source":"@site/docs/metrics/architectural/comprehensiveness.md","sourceDirName":"metrics/architectural","slug":"/metrics/architectural/comprehensiveness","permalink":"/LLMs-metrics-catalog/metrics/architectural/comprehensiveness","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"comprehensiveness","title":"Comprehensiveness Score","sidebar_label":"Comprehensiveness Score"},"sidebar":"docsSidebar","previous":{"title":"System Thinking Score (STS)","permalink":"/LLMs-metrics-catalog/metrics/architectural/system-thinking-score"},"next":{"title":"Dialogue Similarities","permalink":"/LLMs-metrics-catalog/metrics/creativity/dialogue-similarities"}}');var t=i(4848),r=i(8453);const l={id:"comprehensiveness",title:"Comprehensiveness Score",sidebar_label:"Comprehensiveness Score"},c=void 0,o={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Formula",id:"formula",level:2},{value:"Variants",id:"variants",level:2},{value:"<em>1. Coverage Completeness</em>",id:"1-coverage-completeness",level:3},{value:"<em>2. Documentation Completeness</em>",id:"2-documentation-completeness",level:3},{value:"<em>3. Requirement Fulfillment</em>",id:"3-requirement-fulfillment",level:3},{value:"Applications in Software Engineering",id:"applications-in-software-engineering",level:2},{value:"<em>1. Evaluating completeness in long-context tasks</em>",id:"1-evaluating-completeness-in-long-context-tasks",level:3},{value:"<em>2. Capturing semantic correctness not visible to tests</em>",id:"2-capturing-semantic-correctness-not-visible-to-tests",level:3},{value:"<em>3. Complementing correctness and design metrics</em>",id:"3-complementing-correctness-and-design-metrics",level:3},{value:"Interpretation",id:"interpretation",level:2},{value:"<em>What a high CS means</em>",id:"what-a-high-cs-means",level:3},{value:"<em>Strengths</em>",id:"strengths",level:3},{value:"<em>Limitations</em>",id:"limitations",level:3},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",annotation:"annotation",blockquote:"blockquote",br:"br",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mo:"mo",mrow:"mrow",msub:"msub",mtext:"mtext",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"The Comprehensiveness Score (CS) is one of the eight metrics in the Software Engineering Excellence dimension of LoCoBench. It evaluates how complete, thorough, and requirement-aligned an LLM\u2019s solution is when performing complex software engineering tasks. The authors define CS as:"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["\u201cComprehensiveness Score (CS): Derived from software completeness metrics in quality assurance literature (Kan, 2002), assessing solution coverage, documentation quality, and requirement fulfillment.\u201d",(0,t.jsx)(n.br,{}),"\n","(Qiu et al., 2025)\r\nThis makes CS a high-level semantic metric that captures not only whether a solution works, but whether it fully satisfies all instructions, produces complete code artifacts, and includes adequate explanation or documentation. Unlike correctness-oriented metrics that focus on execution, CS evaluates holistic completeness."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"formula",children:"Formula"}),"\n",(0,t.jsx)(n.p,{children:"The LoCoBench paper does not provide a specific numerical formula for Comprehensiveness Score. Instead, the metric is conceptual and grounded in classical software completeness theory. The authors clearly present CS as a qualitative measure:"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["\u201cDerived from software completeness metrics in quality assurance literature (Kan, 2002).\u201d",(0,t.jsx)(n.br,{}),"\n","(Qiu et al., 2025)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Because CS evaluates aspects like coverage, documentation, and requirement fulfillment, it functions as a holistic completeness metric rather than a strict mathematical one."}),"\n",(0,t.jsx)(n.p,{children:"A conceptual representation of CS\u2014consistent with its role in LoCoBench\u2019s normalization framework\u2014is:"}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"C"}),(0,t.jsx)(n.mi,{children:"S"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsx)(n.mi,{children:"f"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mtext,{children:"Coverage"}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mtext,{children:"\u2005\u200a"}),(0,t.jsx)(n.mtext,{children:"Documentation"}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mtext,{children:"\u2005\u200a"}),(0,t.jsx)(n.mtext,{children:"RequirementFulfillment"}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"CS = f(\\text{Coverage},\\; \\text{Documentation},\\; \\text{RequirementFulfillment})"})]})})}),"\n",(0,t.jsx)(n.p,{children:"The final contribution of CS to the LoCoBench Score (LCBS) is produced through LoCoBench\u2019s shared normalization function:"}),"\n",(0,t.jsx)(n.span,{className:"katex",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"N"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(n.msub,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"})]}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsxs)(n.mfrac,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsxs)(n.msub,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"})]}),(0,t.jsx)(n.mo,{children:"\u2212"}),(0,t.jsx)(n.mi,{children:"min"}),(0,t.jsx)(n.mo,{children:"\u2061"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(n.msub,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"})]}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"max"}),(0,t.jsx)(n.mo,{children:"\u2061"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(n.msub,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"})]}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"}),(0,t.jsx)(n.mo,{children:"\u2212"}),(0,t.jsx)(n.mi,{children:"min"}),(0,t.jsx)(n.mo,{children:"\u2061"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(n.msub,{children:[(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"i"})]}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]})]})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"N(m_i) = \\frac{m_i - \\min(m_i)}{\\max(m_i) - \\min(m_i)}"})]})})}),"\n",(0,t.jsx)(n.p,{children:"and then aggregated within the Software Engineering Excellence dimension."}),"\n",(0,t.jsx)(n.h2,{id:"variants",children:"Variants"}),"\n",(0,t.jsx)(n.p,{children:"Although the paper does not define explicit variants of CS, its description implies several operational sub-dimensions aligned with completeness metrics in software quality assurance:"}),"\n",(0,t.jsx)(n.h3,{id:"1-coverage-completeness",children:(0,t.jsx)(n.em,{children:"1. Coverage Completeness"})}),"\n",(0,t.jsx)(n.p,{children:"Evaluates whether the generated solution addresses all required components, such as:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"implementing all requested features"}),"\n",(0,t.jsx)(n.li,{children:"completing all specified files"}),"\n",(0,t.jsx)(n.li,{children:"handling expected workflows or interactions"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-documentation-completeness",children:(0,t.jsx)(n.em,{children:"2. Documentation Completeness"})}),"\n",(0,t.jsx)(n.p,{children:"Assesses whether the output includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"comments"}),"\n",(0,t.jsx)(n.li,{children:"explanations"}),"\n",(0,t.jsx)(n.li,{children:"design rationales"}),"\n",(0,t.jsx)(n.li,{children:"API documentation or docstrings"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is uncommon among code benchmarks and makes CS uniquely holistic."}),"\n",(0,t.jsx)(n.h3,{id:"3-requirement-fulfillment",children:(0,t.jsx)(n.em,{children:"3. Requirement Fulfillment"})}),"\n",(0,t.jsx)(n.p,{children:"Measures compliance with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"task instructions"}),"\n",(0,t.jsx)(n.li,{children:"architectural constraints"}),"\n",(0,t.jsx)(n.li,{children:"specified behaviors or design conventions"}),"\n",(0,t.jsx)(n.li,{children:"boundary conditions or edge-case requirements"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These dimensions are not separately scored but represent the internal structure of comprehensiveness under the LoCoBench framework."}),"\n",(0,t.jsx)(n.h2,{id:"applications-in-software-engineering",children:"Applications in Software Engineering"}),"\n",(0,t.jsx)(n.h3,{id:"1-evaluating-completeness-in-long-context-tasks",children:(0,t.jsx)(n.em,{children:"1. Evaluating completeness in long-context tasks"})}),"\n",(0,t.jsx)(n.p,{children:"Large context windows increase the likelihood of missing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"cross-file requirements"}),"\n",(0,t.jsx)(n.li,{children:"secondary features"}),"\n",(0,t.jsx)(n.li,{children:"edge-case handling"}),"\n",(0,t.jsx)(n.li,{children:"documentation requirements"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"CS directly measures how well models maintain completeness across sprawling codebases."}),"\n",(0,t.jsx)(n.h3,{id:"2-capturing-semantic-correctness-not-visible-to-tests",children:(0,t.jsx)(n.em,{children:"2. Capturing semantic correctness not visible to tests"})}),"\n",(0,t.jsx)(n.p,{children:"Some essential requirements may not be covered by unit or integration tests. CS evaluates whether the model addressed the full intent of the specification, even for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"design rationales"}),"\n",(0,t.jsx)(n.li,{children:"structural consistency"}),"\n",(0,t.jsx)(n.li,{children:"alignment with project requirements"}),"\n",(0,t.jsx)(n.li,{children:"comments or documentation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-complementing-correctness-and-design-metrics",children:(0,t.jsx)(n.em,{children:"3. Complementing correctness and design metrics"})}),"\n",(0,t.jsx)(n.p,{children:"CS sits between correctness (UTP, ITP, CCS) and design excellence (ACS, CRS, STS). It ensures that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"designs are thorough"}),"\n",(0,t.jsx)(n.li,{children:"implementations are complete"}),"\n",(0,t.jsx)(n.li,{children:"solutions provide full project deliverables"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This makes it important for evaluating LLMs in tasks that resemble real-world engineering workflows."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"interpretation",children:"Interpretation"}),"\n",(0,t.jsx)(n.h3,{id:"what-a-high-cs-means",children:(0,t.jsx)(n.em,{children:"What a high CS means"})}),"\n",(0,t.jsx)(n.p,{children:"A high Comprehensiveness Score indicates that the model:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"covered all required functionality"}),"\n",(0,t.jsx)(n.li,{children:"produced all necessary files and components"}),"\n",(0,t.jsx)(n.li,{children:"included meaningful documentation"}),"\n",(0,t.jsx)(n.li,{children:"met all specified requirements"}),"\n",(0,t.jsx)(n.li,{children:"handled edge cases or secondary details"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"strengths",children:(0,t.jsx)(n.em,{children:"Strengths"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Captures completeness, which is a core engineering quality"}),"\n",(0,t.jsx)(n.li,{children:"Reflects real-world evaluation of developer work"}),"\n",(0,t.jsx)(n.li,{children:"Goes beyond execution to evaluate project deliverables"}),"\n",(0,t.jsx)(n.li,{children:"Complements both code quality and correctness metrics"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"limitations",children:(0,t.jsx)(n.em,{children:"Limitations"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"No explicit formula is given in the paper"}),"\n",(0,t.jsx)(n.li,{children:"Likely dependent on LLM-as-a-judge scoring"}),"\n",(0,t.jsx)(n.li,{children:"Some subjectivity may arise in evaluating documentation completeness"}),"\n",(0,t.jsx)(n.li,{children:"Completeness may vary by task type and complexity"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Nevertheless, CS fills an important gap by evaluating thoroughness and requirement fulfillment\u2014critical qualities in long-context software development."}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.p,{children:["Qiu, J., Liu, Z., Liu, Z., Murthy, R., Zhang, J., Chen, H., Wang, S., Zhu, M., Yang, L., Tan, J., Cen, Z., Qian, C., Heinecke, S., Yao, W., Savarese, S., Xiong, C., & Wang, H. (2025). LoCoBench: A benchmark for long-context large language models in complex software engineering.\r\n",(0,t.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2509.09614",children:"https://doi.org/10.48550/arXiv.2509.09614"})]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>c});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);