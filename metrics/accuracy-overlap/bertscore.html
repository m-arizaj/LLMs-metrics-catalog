<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-metrics/accuracy-overlap/bertscore" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">BERTScore | LLM Metrics Catalog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://m-arizaj.github.io/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BERTScore | LLM Metrics Catalog"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="canonical" href="https://m-arizaj.github.io/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore"><link data-rh="true" rel="alternate" href="https://m-arizaj.github.io/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore" hreflang="en"><link data-rh="true" rel="alternate" href="https://m-arizaj.github.io/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"BERTScore","item":"https://m-arizaj.github.io/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore"}]}</script><link rel="stylesheet" href="/LLMs-metrics-catalog/assets/css/styles.422c2d42.css">
<script src="/LLMs-metrics-catalog/assets/js/runtime~main.1ecd9bae.js" defer="defer"></script>
<script src="/LLMs-metrics-catalog/assets/js/main.638bd90d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><a class="navbar__brand" href="/LLMs-metrics-catalog/"></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/LLMs-metrics-catalog/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/bleu"><span title="Core Accuracy &amp; Overlap Metrics" class="categoryLinkLabel_W154">Core Accuracy &amp; Overlap Metrics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/bleu"><span title="BLEU" class="linkLabel_WmDU">BLEU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/accuracy"><span title="Accuracy" class="linkLabel_WmDU">Accuracy</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/pass-at-k"><span title="Pass@k" class="linkLabel_WmDU">Pass@k</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/f1-score"><span title="F1-Score" class="linkLabel_WmDU">F1-Score</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/exact-match"><span title="Exact Match" class="linkLabel_WmDU">Exact Match</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/rouge"><span title="ROUGE" class="linkLabel_WmDU">ROUGE</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/recall"><span title="Recall" class="linkLabel_WmDU">Recall</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/precision"><span title="Precision" class="linkLabel_WmDU">Precision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/meteor"><span title="Meteor" class="linkLabel_WmDU">Meteor</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/edit-distance"><span title="Edit Distance" class="linkLabel_WmDU">Edit Distance</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/mrr"><span title="Mean Reciprocal Rank" class="linkLabel_WmDU">Mean Reciprocal Rank</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/swe"><span title="SWE-Judge Score" class="linkLabel_WmDU">SWE-Judge Score</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/bertscore"><span title="BERTScore" class="linkLabel_WmDU">BERTScore</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/coverage"><span title="Coverage" class="linkLabel_WmDU">Coverage</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/chrf"><span title="chrF" class="linkLabel_WmDU">chrF</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/codescore"><span title="CodeScore" class="linkLabel_WmDU">CodeScore</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/moverscore"><span title="MoverScore" class="linkLabel_WmDU">MoverScore</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/LLMs-metrics-catalog/metrics/statistical/ice-score"><span title="Statistical &amp; Correlation Metrics" class="categoryLinkLabel_W154">Statistical &amp; Correlation Metrics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/ice-score"><span title="ICE-Score" class="linkLabel_WmDU">ICE-Score</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/kendall"><span title="Kendall’s τ" class="linkLabel_WmDU">Kendall’s τ</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/spearman"><span title="Spearman’s ρ" class="linkLabel_WmDU">Spearman’s ρ</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/pearson"><span title="Pearson’s r" class="linkLabel_WmDU">Pearson’s r</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/cohen"><span title="Cohen&#x27;s Score" class="linkLabel_WmDU">Cohen&#x27;s Score</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/smape"><span title="Smape" class="linkLabel_WmDU">Smape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/rate"><span title="Rate Metrics" class="linkLabel_WmDU">Rate Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/statistical/average"><span title="Average Metrics" class="linkLabel_WmDU">Average Metrics</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/LLMs-metrics-catalog/metrics/code-structural/ast-metrics"><span title="Code Quality &amp; Structural Metrics" class="categoryLinkLabel_W154">Code Quality &amp; Structural Metrics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/code-structural/ast-metrics"><span title="AST Metrics" class="linkLabel_WmDU">AST Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/code-structural/edition-metrics"><span title="Edition Metrics" class="linkLabel_WmDU">Edition Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/code-structural/bugs"><span title="Bug Metrics" class="linkLabel_WmDU">Bug Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/code-structural/codejudge"><span title="CodeJudge" class="linkLabel_WmDU">CodeJudge</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/LLMs-metrics-catalog/metrics/functional-test/error"><span title="Functional &amp; Test-based Evaluation" class="categoryLinkLabel_W154">Functional &amp; Test-based Evaluation</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/functional-test/error"><span title="Error Metrics" class="linkLabel_WmDU">Error Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/functional-test/percentage"><span title="Percentage Metrics" class="linkLabel_WmDU">Percentage Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/functional-test/number"><span title="Number Metrics" class="linkLabel_WmDU">Number Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/LLMs-metrics-catalog/metrics/functional-test/test"><span title="Test Metrics" class="linkLabel_WmDU">Test Metrics</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/human"><span title="Human &amp; Subjective Evaluation" class="categoryLinkLabel_W154">Human &amp; Subjective Evaluation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/generative/amber-score"><span title="Generative &amp; Distribution Metrics" class="categoryLinkLabel_W154">Generative &amp; Distribution Metrics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/logical/cog-metric"><span title="Logical Reasoning &amp; Verification" class="categoryLinkLabel_W154">Logical Reasoning &amp; Verification</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/robustness/ct-score"><span title="Robustness, Security &amp; Reliability" class="categoryLinkLabel_W154">Robustness, Security &amp; Reliability</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/efficiency/time"><span title="Efficiency &amp; Resource Usage" class="categoryLinkLabel_W154">Efficiency &amp; Resource Usage</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/architectural/composite"><span title="Architectural &amp; System-level Metrics" class="categoryLinkLabel_W154">Architectural &amp; System-level Metrics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/creativity/dialogue-similarities"><span title="Creativity, Diversity &amp; Novelty" class="categoryLinkLabel_W154">Creativity, Diversity &amp; Novelty</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/ranking/elo-score"><span title="Ranking, Reward &amp; Optimization" class="categoryLinkLabel_W154">Ranking, Reward &amp; Optimization</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/LLMs-metrics-catalog/metrics/semantic/hallucination"><span title="Semantic, Coherence &amp; Hallucination" class="categoryLinkLabel_W154">Semantic, Coherence &amp; Hallucination</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/LLMs-metrics-catalog/references"><span title="References" class="linkLabel_WmDU">References</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/LLMs-metrics-catalog/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Core Accuracy &amp; Overlap Metrics</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">BERTScore</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>BERTScore</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>BERTScore is an embedding-based evaluation metric that measures the similarity between a generated text and a reference by leveraging contextual embeddings from pretrained models like BERT.<br>
<!-- -->Instead of relying on surface-level token overlap (as BLEU or ROUGE do), BERTScore computes semantic similarity between words or code tokens in their contextual vector space.</p>
<p>In Software Engineering, BERTScore and its specialized variant CodeBERTScore are widely used for evaluating code generation, summarization, and translation tasks. These metrics capture semantic correctness, contextual similarity, and embedding-based alignment between generated and reference code.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="formula">Formula<a href="#formula" class="hash-link" aria-label="Direct link to Formula" title="Direct link to Formula" translate="no">​</a></h2>
<p>BERTScore is computed using cosine similarity between contextual embeddings of candidate and reference tokens:</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>BERTScore</mtext><mrow><mi>P</mi><mo separator="true">,</mo><mi>R</mi><mo separator="true">,</mo><msub><mi>F</mi><mn>1</mn></msub></mrow></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>P</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>e</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>Y</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><msub><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>e</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>e</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>F</mi><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mi>P</mi><mi>R</mi></mrow><mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\text{BERTScore}_{P,R,F_1} = 
\begin{cases}
P = \frac{1}{|X|} \sum_{x \in X} \max_{y \in Y} \cos(e(x), e(y)) \\
R = \frac{1}{|Y|} \sum_{y \in Y} \max_{x \in X} \cos(e(x), e(y)) \\
F_1 = \frac{2PR}{P + R}
\end{cases}</annotation></semantics></math></span>
<p>Where:</p>
<ul>
<li>( X ) and ( Y ) represent the sets of tokens in the candidate and reference texts, respectively.</li>
<li>( e(x) ) and ( e(y) ) are contextual embeddings (e.g., from BERT or CodeBERT).</li>
<li>Cosine similarity measures semantic closeness between embeddings.</li>
</ul>
<p>The metric produces Precision (P), Recall (R), and F1 variants, depending on whether the focus is on generated-token coverage, reference coverage, or their harmonic mean.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="variants">Variants<a href="#variants" class="hash-link" aria-label="Direct link to Variants" title="Direct link to Variants" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bertscore-standard">BERTScore (Standard)<a href="#bertscore-standard" class="hash-link" aria-label="Direct link to BERTScore (Standard)" title="Direct link to BERTScore (Standard)" translate="no">​</a></h3>
<p>The base version measures token-level embedding similarity using pretrained BERT models. It is often applied in natural language processing (NLP) and natural language generation (NLG) tasks, where meaning preservation is more important than exact lexical matching.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="codebertscore">CodeBERTScore<a href="#codebertscore" class="hash-link" aria-label="Direct link to CodeBERTScore" title="Direct link to CodeBERTScore" translate="no">​</a></h3>
<p>An adaptation for programming languages proposed by <em>Zhou et al. (2023)</em>.<br>
<!-- -->It employs CodeBERT embeddings trained jointly on natural and programming languages, making it better suited for evaluating code generation, translation, and documentation synthesis tasks. Unlike BLEU or ROUGE, CodeBERTScore captures the semantic consistency and syntactic integrity of the generated code.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="codebertscore-f1-f3">CodeBERTScore (F1, F3)<a href="#codebertscore-f1-f3" class="hash-link" aria-label="Direct link to CodeBERTScore (F1, F3)" title="Direct link to CodeBERTScore (F1, F3)" translate="no">​</a></h3>
<p>These weighted variants balance or prioritize recall:</p>
<ul>
<li><em>F1</em> provides a harmonic balance between precision and recall.</li>
<li><em>F3</em> emphasizes recall, rewarding outputs that capture more of the reference semantics even if syntactic precision is lower.<br>
<!-- -->Such adjustments are particularly relevant in tasks where <em>functional correctness</em> is the main concern (e.g., HumanEval or CoNaLa benchmarks).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="recall-weighted-and-llm-based-extensions">Recall-weighted and LLM-based Extensions<a href="#recall-weighted-and-llm-based-extensions" class="hash-link" aria-label="Direct link to Recall-weighted and LLM-based Extensions" title="Direct link to Recall-weighted and LLM-based Extensions" translate="no">​</a></h3>
<p>Recent studies in LLM evaluation have extended the metric to recall-weighted embedding similarity and LLM-based embedding matching, where embeddings are obtained from large models (e.g., GPT or CodeLlama encoders).<br>
<!-- -->These approaches aim to align metric behavior more closely with human judgments of functionality and logical soundness.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications-in-software-engineering">Applications in Software Engineering<a href="#applications-in-software-engineering" class="hash-link" aria-label="Direct link to Applications in Software Engineering" title="Direct link to Applications in Software Engineering" translate="no">​</a></h2>
<p>BERTScore and its extensions have been widely applied across SE-related evaluations:</p>
<ul>
<li>In LLM evaluation tasks such as GEM, GLGE, and CLUE, BERTScore is used as an automatic semantic evaluation metric for text-based reasoning or natural language understanding.</li>
<li>Within software engineering benchmarks such as CoNaLa, Card2Code, APR-Assess, and Summary-Assess, it quantifies embedding-based similarity between generated and reference code.</li>
<li>In LLM-based functional correctness studies, BERTScore complements metrics like Exact Match by providing LLM-based embedding match (EM) scores that reflect semantic equivalence.</li>
<li>CodeBERTScore, specifically, is used to assess the semantic and structural similarity of generated code in tasks like code translation, generation, and summarization.</li>
<li>Variants such as F1 and F3 have been evaluated on HumanEval and CoNaLa, demonstrating improved sensitivity to code semantics and recall-weighted meaning preservation.</li>
</ul>
<p>Overall, BERTScore offers a flexible framework for evaluating embedding-based similarity, while CodeBERTScore extends this capability to cross-lingual code understanding and LLM evaluation scenarios.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="interpretation">Interpretation<a href="#interpretation" class="hash-link" aria-label="Direct link to Interpretation" title="Direct link to Interpretation" translate="no">​</a></h2>
<p>In software engineering, BERTScore has become an essential bridge between surface-level metrics and semantic understanding.<br>
<!-- -->Its primary advantages include:</p>
<ul>
<li><em>Contextual comprehension:</em> Goes beyond literal matching by analyzing token meaning in context, crucial for tasks like code summarization or translation.</li>
<li><em>Tolerance to variation:</em> Robust to stylistic or lexical changes that don’t affect functionality (e.g., variable renaming or reordering).</li>
<li><em>High correlation with human evaluation:</em> Especially for code generation tasks when paired with CodeBERT or LLM-based embeddings.</li>
</ul>
<p>However, BERTScore also has limitations:</p>
<ul>
<li>It can inflate scores for semantically incorrect but lexically similar code.</li>
<li>It depends on the pretraining corpus of the chosen embedding model (e.g., BERT, CodeBERT, or GPT-based encoders).</li>
<li>It is computationally heavier than token-based alternatives.</li>
</ul>
<p>In practice, BERTScore and CodeBERTScore are most effective when combined with execution-based metrics (like Pass@k or Test Pass Rate), providing a more holistic view of both semantic and functional model performance.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li>
<p><em>Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., &amp; Artzi, Y. (2019).</em> BERTScore: Evaluating Text Generation with BERT.<br>
<a href="https://arxiv.org/abs/1904.09675" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1904.09675</a></p>
</li>
<li>
<p><em>Hanna, M., &amp; Bojar, O. (2021).</em> A Fine-Grained Analysis of BERTScore.<br>
<!-- -->In Proceedings of the Sixth Conference on Machine Translation (WMT 2021).<br>
<a href="https://aclanthology.org/2021.wmt-1.59.pdf" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.wmt-1.59.pdf</a></p>
</li>
<li>
<p><em>Zhou, S., Alon, U., Agarwal, S., &amp; Neubig, G. (2023).</em> CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (Also listed in the references database)
<a href="https://aclanthology.org/2023.emnlp-main.859.pdf" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2023.emnlp-main.859.pdf</a></p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="additional-references-in-dataset">Additional References in Dataset<a href="#additional-references-in-dataset" class="hash-link" aria-label="Direct link to Additional References in Dataset" title="Direct link to Additional References in Dataset" translate="no">​</a></h3>
<ul>
<li>1, 2, 11, 12, 24, 25, 26, 37, 40</li>
</ul></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/swe"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">SWE-Judge Score</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/LLMs-metrics-catalog/metrics/accuracy-overlap/coverage"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Coverage</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#formula" class="table-of-contents__link toc-highlight">Formula</a></li><li><a href="#variants" class="table-of-contents__link toc-highlight">Variants</a><ul><li><a href="#bertscore-standard" class="table-of-contents__link toc-highlight">BERTScore (Standard)</a></li><li><a href="#codebertscore" class="table-of-contents__link toc-highlight">CodeBERTScore</a></li><li><a href="#codebertscore-f1-f3" class="table-of-contents__link toc-highlight">CodeBERTScore (F1, F3)</a></li><li><a href="#recall-weighted-and-llm-based-extensions" class="table-of-contents__link toc-highlight">Recall-weighted and LLM-based Extensions</a></li></ul></li><li><a href="#applications-in-software-engineering" class="table-of-contents__link toc-highlight">Applications in Software Engineering</a></li><li><a href="#interpretation" class="table-of-contents__link toc-highlight">Interpretation</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a><ul><li><a href="#additional-references-in-dataset" class="table-of-contents__link toc-highlight">Additional References in Dataset</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>